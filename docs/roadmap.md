# AlphaTrade Gap Analysis and Roadmap

## Gap Analysis Report

This section compares the current emp_proving_ground_v1 codebase against the envisioned AlphaTrade architecture. The AlphaTrade blueprint organizes the system into five major components – Perception, Adaptation, Reflection, Risk/Execution, and Governance – each corresponding to a stage in the intelligent trading loop. Below, we identify for each component what is already implemented, what is partially in place, what is missing, and any misalignments between the blueprint and the current implementation.

### Perception (Sensory Ingest & Belief Formation)

**Implemented:** The code establishes a layered perception pipeline that ingests sensory data (market signals) into an internal belief state. A BeliefState model is in place to represent posterior beliefs and regime context, including Hebbian-style updates to capture recent patterns[1]. A sensory cortex framework exists with a “real_sensory_organ” module that fuses multiple signal types (WHAT/WHEN/WHY/HOW/ANOMALY) into unified sensory payloads[2]. The belief/regime scaffolding is functional: it buffers sensor inputs, emits finite-state regime indicators, and enforces positive semi-definite (PSD) covariance updates as a stability check[3]. Core architecture for perception reflects the encyclopedia’s layered design (core → sensory → thinking…)[4].
**Partially Implemented:** Many sensory inputs are still mock or synthetic. The real-time data ingest is not fully wired – currently relying on placeholders rather than live market feeds[5][4]. Key sensory organs now execute deterministic logic – HOW clamps telemetry, WHAT/WHEN/WHY emit lineage and quality metadata, and ANOMALY wraps a z-score based `BasicAnomalyDetector` with regression tests – yet they still operate on synthetic frames until live ingest is available. Lineage and telemetry for sensory signals are now exercised by guardrail suites (e.g. sensor lineage/quality tests for the primary dimensions and anomaly detector), but the system still feeds on sample or generated data until a real data backbone is delivered.【src/sensory/what/what_sensor.py:131】【src/sensory/anomaly/basic_detector.py:1】【tests/sensory/test_primary_dimension_sensors.py:1】【tests/sensory/test_basic_anomaly_detector.py:1】 Drift detection is present (the Drift Sentry monitors statistical drift in beliefs) but depends on synthetic data and has not been exercised on real streaming input[2].
An integration test now exercises the Timescale→Kafka→RealSensoryOrgan path with synthetic EURUSD slices, proving the operational backbone can stream telemetry into the cortex even though live feeds are still pending.【tests/integration/test_operational_data_backbone.py:124】
**Missing:** A production-grade data backbone for perception is absent – there are no live connections to TimescaleDB, Redis, or Kafka yet[5]. The system needs actual market data ingest (e.g. price feeds, order book updates) feeding the sensory organ. Robust anomaly detection (beyond the new basic z-score detector) and explanatory diagnostics (the WHY organ) still need live data calibration and richer models.[2] Also missing is the continuous calibration of sensors: e.g. adjusting to new instrument data, handling missing data, etc. Without these, the perception layer cannot fully reflect the blueprint’s vision of a rich sensory cortex driving the loop.
**Misalignments:** The AlphaTrade vision calls for a rich, real-time sensory cortex with multiple specialized detectors and trustworthy signals; currently, the code’s perception layer is conceptually aligned but functionally limited to scaffolding[2]. In particular, the blueprint’s emphasis on executable organs and fused signals is only partially realized – the code structure is there, but actual data quality, validation, and anomaly logic are lacking. This means the spirit of Perception (delivering reliable, validated market facts to downstream components) isn’t yet met in practice. The design itself is sound (no major structural misalignment), but the implementation is incomplete, leaving a gap to close before Perception can fully anchor the loop.

### Adaptation (Policy Routing & Fast-Weight Learning)

**Implemented:** The code implements an Understanding Router and a Policy Router that together realize the Adaptation stage of AlphaTrade. The UnderstandingRouter ingests belief snapshots and routes them to a chosen strategy/tactic, functioning as the decision-making “brain” that selects an action or intent[6][7]. Crucially, fast-weight adaptation is integrated: the router supports Hebbian fast-weight updates – short-term adjustments to strategy preferences based on recent evidence[8]. This means the system can amplify or dampen certain tactics dynamically (“neurons that fire together wire together”), consistent with the BDH-inspired fast-weight principle. The PolicyRouter in the thinking layer tracks strategy metadata (objectives, tags) and manages experiment lifecycles, allowing new tactics to be registered and providing reflection data on their performance[9][10]. The overall Adaptation loop (Perception → Adaptation → Reflection) is coded in the AlphaTradeLoopOrchestrator, which ties router decisions to drift checks and governance gating[11][6]. In summary, the adaptive decision-making framework – including configurable strategy routing and fast-weight toggles – is present and anchoring the system’s logic.
**Partially Implemented:** While the routing and fast-weight mechanics exist, the broader Evolution Engine (adaptive intelligence that generates or mutates strategies) remains mostly scaffolding. The alignment plan envisioned an “institutional genome catalogue” and pipeline for evolving new strategies[12], but in code this is limited. The current adaptation relies on predefined strategies and simple heuristic adjustments. There are placeholders for evolutionary pipelines and integration with a strategy catalogue, but these are not fully fleshed out (many “adaptive population” features are toggled off or produce no-ops)[13]. Fast-weight experiments are implemented behind a feature flag that requires governance approval[12], indicating that adaptive learning is not yet default. Additionally, the fast-weight updates currently apply a simple decay and boost to strategy weights; more complex behaviors (e.g. long-term learning or meta-learning) are not yet realized. Sparse positive activation – a key BDH principle where neurons have high-dimensional positive activations – is not explicitly enforced, although the groundwork (non-negative weight updates via Hebbian logic) is laid. In short, the adaptation layer has the basic fast-weight loop but is not yet the full intelligent, self-evolving system described in the blueprint.
**Missing:** The “Evolution Engine” is largely missing in practice. The system has no mechanism to create entirely new strategies or significantly alter algorithms based on performance – there is no genetic algorithm or gradient descent updating the strategy set. Also absent is the integration with a strategy catalogue or repository: e.g. selecting from a library of tactics or logging new variants into that library for future use[14]. Advanced adaptation features like long-term memory of successful patterns, or automatic hyperparameter tuning of strategies over time, are not implemented. Moreover, the blueprint’s notion of adaptation includes mutating against real data feeds[14] – since real feeds aren’t hooked up yet (Perception gap), the adaptation cannot truly learn from live market behavior. Sparse positive activations (ensuring that only a small fraction of strategy “neurons” activate at a time, and that activations are non-negative) are not enforced by any module – this could be a design adjustment needed to align with BDH theory. In essence, the system does not yet learn new trading tactics or significantly improve existing ones on its own; it can only tweak weightings of pre-programmed tactics.
**Misalignments:** Conceptually, the code aligns with the blueprint’s Adaptation loop – it has a router with fast-weight updates and the idea of evolving strategy preferences. However, there is a gap between the aspirational adaptive behavior (a rich evolution of strategies) and the current simplistic implementation. The BDH-inspired elements (fast weights) are present, but others (sparse activations, graph-based reasoning dynamics) are not explicitly present beyond basic data structures. The blueprint expects Adaptation to be highly dynamic and data-driven, whereas the current state is rule-based and limited. No fundamental architecture changes are needed (the scaffolding is in place), but significant development is required to realize the blueprint’s vision of an autonomous, learning “evolution engine” driving this component[13].

### Reflection (Decision Diaries & Learning Feedback)

**Implemented:** The system includes a Reflection stage that records decisions and generates insights from them. A Decision Diary mechanism is implemented – every AlphaTrade loop iteration produces a DecisionDiaryEntry that logs the context, chosen strategy, outcomes, and reasoning notes[15][16]. These diaries are persisted via a DecisionDiaryStore and serve as an auditable trail of “why was this decision made.” The code also provides a PolicyReflectionBuilder which compiles recent decision records into Reflection Artifacts[17][18]. These artifacts summarize emerging tactics, their performance, first/last seen times, and any gating (e.g. if a strategy was forced to paper trading)[10]. In effect, the system can produce a reflection digest for reviewers or automated analysis, so that over a window of time one can see which strategies are gaining or losing favor and why. There’s also a graph diagnostics tool that visualizes the understanding loop (sensory → belief → router → policy) as a DAG, helping reflect on the decision pipeline structure[19]. The presence of these features means the Reflection component – capturing experience and providing data for learning – is acknowledged in the codebase and partially functional. The blueprint’s intent for an auditable reasoning loop is met at least to the extent that every decision is transparently recorded with metadata[20].
**Partially Implemented:** The diaries and reflection summaries exist, but using them for learning feedback is limited. Currently, reflection artifacts are primarily for human governance review (e.g. seeing evidence for strategy promotions) rather than automatically adjusting the system. The fast-weight adaptation loop does not yet incorporate long-term feedback from the diaries – e.g. there is no mechanism like “if a strategy consistently underperforms as seen in diaries, automatically downweight or remove it.” Instead, such adjustments would still be manual (via governance CLI). Some aspects of reflection that the blueprint envisions, like “sigma stability checkpoints” (monitoring the stability of belief updates over time) and other health metrics, are only partly realized via tests (ensuring covariance matrices remain PSD, etc.)[21]. The new `StrategyPerformanceTracker` aggregates ROI, win/loss, and drift loop metrics for each strategy under guardrail coverage, but the resulting reports are still consumed manually – nothing reweights strategies automatically based on those insights.【src/operations/strategy_performance_tracker.py:1】【tests/operations/test_strategy_performance_tracker.py:1】 Interpretability is another partial area: the blueprint highlights interpretability of state and reasoning; the code provides raw data (diaries, graph dumps) but no higher-level analysis like highlighting which “concept synapses” were most active (a nod to BDH’s interpretability). Exported understanding graphs now include fast-weight utilisation, sparsity, and dominant-strategy summaries to highlight overused or idle adapters, yet these metrics come from synthetic decision windows rather than live feedback streams.【src/understanding/diagnostics.py†L621-L958】【tests/understanding/test_understanding_diagnostics.py†L15-L39】 The building blocks for reflection are there, but the feedback loop is not closed – insights are gathered but not yet used to automatically tune the system.
**Missing:** Automated post-analysis and learning from the decision records are missing. For example, there’s no module performing trend analysis on the diary entries (e.g. detecting that “Strategy A only works in regime X” or “Strategy B’s performance is deteriorating”). The blueprint suggests a system that can reflect and self-correct; currently, any such reflection-driven changes must be done by a developer or analyst examining the logs. Also missing are formal acceptance tests (validation hooks) for the reflection outputs – while there are some tests (ensuring the diary CLI and reflection builder run[19]), the criteria for a “good” reflection (e.g. it correctly identifies new emerging strategies or anomalies in decisions) are not automated. Graph health metrics now emit utilisation, sparsity, and dominance summaries but still rely on curated synthetic scenarios; the system is not yet feeding real historical windows into the diagnostics or alerting when thresholds are breached, so reviewers must manually interpret the numbers.【src/understanding/diagnostics.py†L621-L958】【tests/tools/test_understanding_graph_cli.py†L8-L19】 In summary, Reflection currently records but does not learn; the system lacks an implementation of “reflection as a teacher” to the adaptation process.
**Misalignments:** The architecture for Reflection is in line with the blueprint – the idea of diaries and reflection artifacts matches the vision of an introspective system. The misalignment lies in purpose and depth: the blueprint (inspired by BDH and similar cognitive architectures) implies that reflection should influence the system’s behavior (closing the loop with adaptation), whereas in the current code reflection is passive (for audit/compliance). Another subtle misalignment is in the metric-driven reflection: the blueprint’s emphasis on metrics like “graph sparsity” or activation patterns for interpretability isn’t yet reflected in the implementation (the code doesn’t ensure activations are sparse positive or measure concept-level activity). Addressing these gaps will bring Reflection from a compliance log towards a true learning mechanism.

### Risk/Execution (Risk Management and Trade Execution)

**Implemented:** The codebase contains foundational elements of risk management and execution control. A DriftSentryGate is implemented to act as a risk gate on execution – it evaluates each proposed trade against drift metrics and confidence thresholds, potentially flipping a force_paper flag to prevent live execution if conditions look anomalous[23]. Risk policies (like leverage and exposure limits) are defined in configurations and there are tests ensuring warnings trigger before limits are breached[24]. An execution release router exists to decide how orders are routed (e.g. to paper trading vs live, based on the drift gate’s decision and policy stage)[23]. The system also includes a portfolio monitor for tracking positions and P&L, albeit in a basic form, and an execution readiness journal that logs whether services (like data feeds or brokers) are up before trading[25]. These show that the scaffolding for execution – the order lifecycle and risk checks – mirrors the encyclopedia’s prescribed order of operations[26]. Additionally, compliance telemetry hooks are present (audit logs, incident response stubs) indicating the system’s awareness of compliance needs. In short, the codebase has risk check structures and toggles to ensure that trades are gated by risk evaluations and that execution can be toggled between simulation and real mode.
**Partially Implemented:** Actual trade execution is still running on “paper.” The trading and execution modules operate on simulated orders and mock broker interfaces – there is no live broker integration or order routing to markets yet[4]. Risk enforcement is described as “hollow” in assessments[4]: while limits exist on paper, the system doesn’t yet connect to real capital constraints (e.g. it’s not hooked to an account to truly prevent an order). Recent hardening ensures the risk manager now returns `0.0` when no risk budget or minimum lot is available so orchestration stops before oversizing depleted accounts, but those checks still protect only simulated balances.【src/risk/risk_manager_impl.py:220】【tests/risk/test_risk_manager_impl_additional.py:27】 Some risk checks (like those for position sizing or portfolio diversification) may not be fully implemented beyond placeholders. The async task supervision for execution (running order placements in background tasks, ensuring none hang or crash) is only partially migrated – the runtime builder and task supervisor are in progress, meaning execution tasks might still be launched unsupervised in some paths[26]. Compliance monitoring (e.g. checking regulatory rules or logging trade data for compliance) is minimal: there are audit log structures, but no active enforcement beyond risk limits. In summary, the mechanics to simulate trading are there and the safety switches exist, but real execution capabilities and robust risk responses are not yet complete.
A dedicated PaperBrokerExecutionAdapter now bridges release-aware execution into the FIX pilot, capturing deterministic risk context on every submission under regression coverage, and the runtime can optionally attach a REST paper-trading bridge via `PaperTradingApiAdapter` when extras are provided, with bootstrap cleanup hooks and adapter tests exercising real HTTP round-trips; the stack still stops at paper credentials rather than live exchanges.【src/trading/execution/paper_broker_adapter.py:1】【src/trading/integration/paper_trading_api.py:1】【src/runtime/predator_app.py:2095】【tests/trading/test_paper_trading_api_adapter.py:1】【tests/runtime/test_bootstrap_paper_broker.py:1】
**Missing:** Key pieces missing include production integration for execution – for example, connections to brokerage APIs or trading exchanges are not implemented, so the system cannot place a real order. Also missing is risk-based position sizing: the blueprint expects that given a strategy signal, the system calculates an optimal position size under risk limits, but currently there is “no risk sizing” at all[4]. The institutional risk and compliance layer is incomplete – features like pre-trade compliance checks, post-trade reconciliation, or regulatory audit trail generation are absent. Ops readiness items like stress tests or kill-switches for the trading engine need to be added to match the blueprint’s focus on safety (some incident response hooks exist, but likely not end-to-end). Additionally, the blueprint calls for expanding broker coverage after internal gates are solid[27], implying that multi-broker or multi-exchange handling is on the roadmap but currently missing. Finally, while the system can force trades to paper mode, a robust policy enforcement that only allows fully approved strategies to execute live is not yet guaranteed (it depends on humans using the CLI to promote stages). In essence, AlphaTrade cannot yet execute a real trade in production with full confidence – the pipeline from decision to execution is incomplete.
**Misalignments:** The major misalignment is that the current system is still a simulation framework, whereas the AlphaTrade blueprint assumes a trajectory toward a real trading platform with enforceable risk controls. The architecture is on the right track, but practically, capital is not at risk because the system isn’t ready to handle real capital[28]. For instance, the blueprint emphasizes deterministic risk APIs and policy breach telemetry[29] – the code has placeholders for these, but until actual execution is attempted, it’s unclear how effective they are. There’s also a structural note: risk management in the code has been refactored (old risk modules deprecated in favor of a canonical core risk module)[4][30], which aligns with the blueprint’s intent to have a single source of truth for risk. However, until the execution layer is fully functional, the risk management can’t be truly battle-tested. In summary, the design largely aligns with the envisioned risk/execution loop (no major redesign needed), but there is a significant gap in implementation maturity.

### Governance (Policy Governance & Promotion Process)

**Implemented:** A robust Governance layer is present to oversee which strategies can trade and under what conditions. The system includes a Policy Ledger (in src/governance) that records each strategy (“policy”) and its approval stage – e.g. experimental, paper-trade, or live[31][32]. The ledger persists promotion history, approvals, threshold overrides, and links to decision diary evidence for each promotion[33]. On top of this, a suite of governance CLI tools is implemented: for example, rebuild_policy to regenerate risk config from the ledger, promote_policy to approve a strategy’s next stage with proper sign-offs, and alpha_trade_graduation to batch-promote strategies that meet all criteria[34][35]. These tools ensure that any strategy going from backtest to paper or paper to live is traceable and auditable. The AlphaTrade loop orchestrator itself uses the ledger: it queries the LedgerReleaseManager to fetch the current release stage and risk thresholds for the chosen policy, and uses that to enforce stage-appropriate gates (for example, ensuring a strategy in “paper” stage cannot execute real trades, by activating the force_paper flag)[36][37]. Recent hardening introduced filesystem locks and atomic writes around ledger persistence together with multi-approver enforcement, promotion logging, and runbook-linked summaries in the CLI, keeping governance state consistent even when multiple operators promote simultaneously.【src/governance/policy_ledger.py:260】【tools/governance/promote_policy.py:122】【tools/governance/_promotion_helpers.py:13】【tests/tools/test_promote_policy_cli.py:1】 Overall, the governance processes and data structures described in the blueprint (promotion gates, evidence-backed approvals, deterministic governance metadata on each decision) are present and functioning in the code.
**Partially Implemented:** The governance features exist, but their integration into day-to-day operations is partial. Enforcement of governance policies relies on the developers/operators running the CLI tools and monitoring outputs – there isn’t a live UI or automated daemon that, for example, halts an unapproved strategy (though the architecture would allow it via the ledger checks). Some governance checks are enforced in code (like requiring a decision diary entry ID when promoting a policy, to ensure evidence is cited[38]), but others may be more procedural (outside the code’s automatic handling). Recent hardening trims and normalises evidence identifiers before they persist to history, rejects whitespace-only submissions, and raises deterministic errors that test suites exercise, reducing the manual clean-up needed when operators promote tactics.【src/governance/policy_ledger.py:127】【tests/governance/test_policy_ledger.py:147】【tests/governance/test_policy_ledger_locking.py:51】 The governance telemetry (dashboards showing how many strategies at each stage, any pending approvals, etc.) is not fully developed – observability panels exist for the understanding loop and drift, but governance info might only be in logs or JSON manifests. Additionally, while the ledger captures threshold overrides per strategy, the system still needs human input to decide those overrides; there’s no AI deciding “this strategy should have tighter risk limits” – governance in that sense is manual. The blueprint’s notion of deterministic promotion gates is implemented at a basic level (stages in ledger), but dynamic policy enforcement (like auto-downgrading a strategy if it triggers too many alerts) is not implemented. In summary, governance is structurally in place but not yet a “hands-off” autonomous module – it provides tools for humans to govern the system.
**Missing:** A few things are missing to fully realize the governance vision. Real-time governance monitoring – e.g. a continuously running process or dashboard that flags when a strategy is eligible for promotion or needs demotion – is not present. Integration with enterprise governance (approvals via UI, or embedding in a larger workflow system) is beyond the current scope. Also, policy documentation and rationales might need to be auto-generated for each promotion (currently, one must read the diaries and ledger entries manually). The blueprint’s focus on compliance telemetry suggests that every governance action should be visible and testable; while the ledger provides data, the system lacks a user-friendly presentation of compliance status (e.g. “All strategies trading live have passed X criteria”). Another missing piece is Governance of configuration: ensuring any config changes go through similar review (the current ledger is strategy-focused). Finally, as a future feature, one could imagine machine-supported governance (ML suggestions for promotions or flagging anomalies in strategy performance for review) – needless to say, this is not yet implemented. Essentially, the governance is policy-driven but not yet intelligent or fully automated.
**Misalignments:** The code’s governance approach aligns well with the blueprint’s intent: it provides structured, auditable control over system behavior. There is no major misalignment in design; rather, the misalignment is in maturity – the blueprint likely envisions a seamless promotion pipeline with clear metrics at each gate, whereas currently it’s a set of powerful but developer-operated tools. One area to watch is whether all blueprint governance rules are enforced: for example, the blueprint implies that understanding loop outputs should feed governance decisions (ensuring “AlphaTrade parity work can ship without capital risk” by using live-shadow mode until ready[28]). The current system does enforce a “live-shadow” (paper) mode by default for new strategies via drift gating and ledger stage, which is correct. However, if there are any blueprint policies not coded (such as time-based graduation criteria or multi-approval requirements), those would be gaps. In summary, governance in code is largely faithful to the plan, with the remaining work being operational integration and perhaps adding intelligence to assist human governors.
**Summary of Gaps:** In all, the emp_proving_ground_v1 codebase has established the core architecture (the five components exist and interact as intended[4]), but most components are only partially realized. Many subsystems still rely on scaffolding or mocks, and several advanced features from the AlphaTrade vision (live data ingestion, adaptive evolution of strategies, automated reflective learning, real trade execution, and fully automated governance oversight) are incomplete or absent. There are few fundamental design misalignments – the gaps are mostly feature-completeness and integration gaps rather than structural flaws. This is a strong position to be in: the blueprint is validated by the current design, and the task ahead is to close the implementation gaps with focused development in each area[26][39].

## 90-Day Roadmap (Phased Execution Plan)

Below is a refreshed 90-day roadmap to guide the next phase of AlphaTrade development. This plan is organized into three phases, each roughly one month, aligning with: Phase I – Understanding Loop & Data Backbone, Phase II – Governance & Risk Hardening, and Phase III – Full Integration & “Paper-Ready” System. Each phase is broken down into key milestones with checklist deliverables and measurable acceptance criteria (Definitions of Done). We also include a “Start Now” section for immediate next steps (first 48 hours) to build momentum. Throughout, we incorporate BDH-inspired primitives – specifically fast-weights (already in use), sparse positive activations, and graph health metrics – to ensure the system stays aligned with cutting-edge architectural principles. All new development will adhere to the preferred project directory structure (layered by core/sensory/thinking/trading/governance domains) and maintain strict code contracts (typed interfaces, data model schemas, and regression tests) for clarity and reliability[4][40].

### Start Now (Next 48 Hours) – Jumpstart and Quick Wins


- [x] Provision Development Data Services – Setup local instances of TimescaleDB, Redis, and Kafka to begin replacing mock data feeds[5]. DoD: Services running in docker or dev environment; connection parameters added to config; basic connectivity tests pass (e.g. can write/read a test record to Timescale). Completed via the new `docker-compose` services, `env_templates/dev_data_services.env`, and connectivity guardrails in `tests/operations/test_dev_data_services.py`.

- [ ] Review and Cleanup Namespace Drift – Identify any mismatches between module names and usage (e.g. “intelligence” vs “understanding” in tests) and clean up deprecated references[4]. DoD: All references to removed or moved modules (e.g. get_risk_manager shim) are eliminated or properly routed to new modules; pytest and mypy run clean with no module-not-found or deprecation warnings.

- [ ] Enable Real Data Ingest in a Slice – Pick one data source (e.g. daily price candles for one asset) and connect it end-to-end through the system. This involves writing a small ingestion script using the new Timescale service and feeding the data into the real_sensory_organ. DoD: A new integration test or notebook that ingests real market data into the BeliefState (via the sensory organ) and produces a belief snapshot; ensure the belief update (Hebbian step) runs without errors on this data.
  - Progress: Operational backbone integration test drives Timescale ingest plans, bridges Kafka telemetry onto the event bus, and feeds RealSensoryOrgan snapshots for EURUSD slices, satisfying the slice rehearsal with synthetic data while real feeds stay outstanding.【tests/integration/test_operational_data_backbone.py:124】【src/data_integration/real_data_integration.py:1】

- [x] Quick Win: Implement Basic Anomaly Detector – Fill in a simple implementation for the ANOMALY organ stub. For example, use a z-score or Bollinger band method on recent prices to flag anomalies. DoD: real_sensory_organ no longer raises NotImplemented for anomaly detection; anomaly flags show up in belief snapshots (e.g. boolean “anomaly” field); a unit test feeds a out-of-range value and the anomaly flag triggers.
  - Completion: `BasicAnomalyDetector` now computes rolling z-scores with window/min-sample validation, the anomaly sensor consumes it for both sequence and frame inputs, and regression tests assert anomaly flags, detector metadata, and lineage payloads across the sensory stack.【src/sensory/anomaly/basic_detector.py:1】【src/sensory/anomaly/anomaly_sensor.py:38】【tests/sensory/test_basic_anomaly_detector.py:1】【tests/sensory/test_how_anomaly_sensors.py:170】

- [x] Draft “Definition of Done” Templates – For each upcoming phase’s major feature, write a one-paragraph definition of done. This will guide development and testing (e.g. DoD for “executable HOW organ” or “risk policy enforcement”). DoD: DoD statements written in the project docs (or as comments in alignment briefs) for at least the top 5 upcoming deliverables, including measurable criteria. Alignment briefs for the evolution engine, data backbone, and sensory cortex now host the new templates landed in commit 96cd9e3 so teams have measurable success criteria ahead of implementation.
**Quick Wins vs. Deferrals:** The tasks above focus on low-hanging fruit that de-risk the project (e.g. setting up infrastructure, trivial feature implementations). More speculative or complex features (like sophisticated ML-based anomaly detection or full BDH graph algorithms) are acknowledged in the roadmap but deferred until core functionality is in place. For example, implementing a complete BDH spiking neural network is out of scope for 90 days (strategic deferral), whereas computing simple graph metrics and ensuring weight sparsity are achievable and included.

## Phase I (Days 0–30): Understanding Loop & Data Backbone

**Goal:** Establish a fully functional Understanding Loop v2 running on real data, backed by a solid data pipeline. This phase focuses on closing the Perception gap – replacing mocks with live data ingest – and strengthening the Adaptation loop with initial evolutionary features. We will also ensure the foundation (data and fast-weight logic) is robust with tests and documentation.

- [ ] Operational Data Backbone Online – Deploy real data ingestion and caching services for live-shadow mode. Replace the remaining mock data pathways with actual TimescaleDB (for historical/time-series data), a streaming source via Kafka (for live ticks), and Redis for caching hot data[5]. DoD: The system can ingest data from Timescale (e.g. recent market history) and stream updates via Kafka into the sensory layer. All SQL queries are parameterized (no raw SQL injection risks)[41], and background ingest tasks are supervised (monitored via the runtime supervisor). Regression tests cover a full ingest cycle (store, cache, retrieve) proving that no mocks are needed for core data flows.
  - Progress: `config/system/dev_data_backbone.yaml`, the Timescale init script, and the new dev-services test suite validate the docker-compose stack end-to-end while docs/development/setup.md walks operators through running the services locally; live ingestion into sensory remains outstanding.
  - Progress: New end-to-end coverage simulates Timescale writes, Kafka bridges, cache metrics, and sensory fusion during shutdown, anchoring the operational backbone DoD even though services still run in mocked local mode.【tests/integration/test_operational_data_backbone.py:124】【src/data_foundation/streaming/kafka_stream.py:1】
**Acceptance tests:** Simulate a minute of live data (via Kafka) and verify the UnderstandingRouter receives appropriate belief updates. Validate that if Timescale is turned off, the failover mechanisms log warnings but the system continues (using cached data)[42].

- [ ] Executable Sensory Organs (WHAT/WHEN/HOW/WHY/ANOMALY) – Implement the remaining sensory organ logic. Each organ module should process raw data into a feature: e.g. HOW = volatility or trend detection, WHY = news or causal tag (could be stubbed with “N/A” for now), WHEN = timing features (market session or event proximity), WHAT = primary observable (price/volume movement), ANOMALY = outlier detection. DoD: No organ function remains a stub; each produces a defined output in the sensory payload. These outputs feed into the BeliefState construction. The organs publish lineage and quality metadata (timestamp, source, confidence) for each signal[43]. Added tests for each organ: e.g. feed a known pattern into HOW organ, verify it identifies trend up vs down; feed a known anomaly, verify anomaly organ flags it.
  - Progress: WHAT/WHEN/WHY sensors now attach `quality` metadata with timestamps, source identifiers, and confidence values, and the lineage payloads are validated by the new `test_primary_dimension_sensors` suite while the anomaly stack exercises the shared detector path under guardrail coverage.【src/sensory/what/what_sensor.py:131】【src/sensory/when/when_sensor.py:147】【src/sensory/why/why_sensor.py:126】【tests/sensory/test_primary_dimension_sensors.py:1】
**Acceptance tests:** Run the entire sensory organ on a historical dataset and inspect a sample BeliefState. Confirm that all five signal types are present and plausible (e.g. “anomaly” flag raised during a known market shock). Additionally, verify drift detection reacts to the HOW signal (e.g. high volatility triggers drift alert).

- [ ] Belief & Regime Integration with Real Data – Ensure the BeliefState and Regime detectors work with the live data feeds. As real data can be noisy, calibrate the Hebbian update (fast-weight decay) parameters and regime thresholds. Possibly introduce a “calm/normal/storm” regime classification using volatility measures. DoD: BeliefState updates remain stable (covariance matrix stays PSD and bounded) under real feed[21]. Regime FSM (finite-state machine) transitions appropriately with real volatility – e.g., in a quiet period, system is in “calm” regime, and switches to “storm” when volatility spikes, publishing an event on the bus. Include guardrail tests for extreme scenarios: feeding constant data (should remain calm) vs. highly erratic data (should trigger storm regime and maybe anomalies).
**Acceptance tests:** After ingesting a month of historical data, the distribution of regime states should make sense (e.g. some % calm, some % normal, few % storm, without oscillating every tick). A test replays a known volatile period (market crash) and asserts that the regime state enters “storm” and drift alerts fire, aligning with expectations.

- [ ] Fast-Weight Adaptation Tuning – Leverage BDH principles to refine fast-weight updates. Introduce configuration to enforce sparse positive activations in adaptation: for instance, apply ReLU or thresholding so that fast-weight adjustments never produce negative connection strengths, and perhaps prune (set to zero) the smallest weights periodically to maintain sparsity[44][45]. DoD: A new fast_weights.py module (or extension of the router) implements constrained Hebbian updates: only additive (excitatory) weight changes or segregated inhibitory signals if needed. A metric for “% of strategy weights active” is tracked per decision – aiming for only a small fraction of strategies to get significant weight boosts at any time (sparsity). Unit tests verify that with fast-weights enabled, all weights in the router’s internal matrices remain ≥ 0 (non-negative) and that toggling the fast-weight feature flag reverts to baseline behavior[12][46].
**Acceptance tests:** Run a simulation of 100 decisions with fast-weights on; ensure that at most (say) 20% of strategies got weight updates in that period (sparsity threshold) and none of the weight adjustments were negative. Also verify that disabling fast-weights yields identical decisions for a given input sequence (proving determinism when the feature is off[47]).

- [ ] Seed the Evolution Engine (Basic) – Implement a simple strategy mutation or catalog integration. As a first step toward the Evolution Engine, allow the system to tweak an existing strategy’s parameter or choose from a library of strategies when performance lags. For example, introduce a configuration where after N iterations, if a strategy’s win-rate is below X, an alternate strategy from the catalogue is introduced for trial (or a parameter is perturbed). DoD: A rudimentary strategy catalogue is created (even if just a list of two strategies for now). The PolicyRouter or a new EvolutionManager monitors performance stats from diaries and can swap or spawn a strategy in the routing table under certain conditions[13]. Document this behavior and ensure it’s feature-flagged (only active in paper-trade mode). At least one test simulates a consistently losing strategy and confirms the system either adjusts its weight down or replaces it with a variant.
**Acceptance tests:** Use the decision diary data to verify evolutionary behavior: e.g. after 50 iterations with poor outcomes for Strategy A, the logs/diary should show Strategy B (new or mutated) being tried. Confirm that governance rules still apply (the new strategy should start in an experimental stage with force_paper true until promoted).

- [x] Documentation & Example Notebook – Document the Understanding Loop and provide a usage example. Update docs/ with a description of the Phase I features: data flow from ingest to decision, how fast-weights work, how to interpret the decision diary. Create a simple Jupyter notebook (or markdown in docs/examples) demonstrating a small end-to-end run: ingest sample data, run a loop iteration, inspect outputs (belief state, chosen strategy, diary entry). DoD: Documentation pages updated (include the new config options, flags, and any new modules). The example notebook runs without errors and produces a clear visualization or printout of one loop cycle on real data. This serves as both a validation and a training artifact for new contributors.
  - Completion: Understanding Loop Phase I guide and the runnable notebook now walk through ingest→belief→decision flow, fast-weight adapters, and diary exports with troubleshooting notes and demo outputs for the EURUSD slice.【docs/UNDERSTANDING_LOOP_GUIDE.md†L1-L67】【docs/examples/understanding_loop_demo.ipynb†L1-L420】
**Acceptance tests:** (Manual) A reviewer can follow the docs to set up data ingest and reproduce the notebook’s results, confirming the system behaves as documented.

## Phase II (Days 31–60): Governance, Risk & Policy Hardening

**Goal:** Solidify the Governance and Risk management aspects so that the system can safely handle increasingly autonomous decisions. In this phase, we focus on enforcing risk limits deterministically, integrating governance checks throughout the loop, and improving overall system resilience (task supervision, compliance logging). By the end of Phase II, AlphaTrade should be robust enough to run in a closed-loop paper trading environment with minimal human intervention, while ensuring no strategy can bypass defined policies. Governance processes will be streamlined and fully tested, and the groundwork laid for eventual live trading under strict control.

- [ ] Task Supervision & Runtime Builder Completion – Finalize the asynchronous task management. Ensure that all background tasks (data ingest, signal processing, trade execution) are launched via the unified runtime supervisor. Remove any remaining asyncio.create_task calls that aren’t tracked[26]. DoD: The bootstrap_runtime code uses a supervised task registry for every service (ingest loops, drift monitoring, etc.). If a task fails or hangs, the supervisor logs it and optionally restarts it. Add a torture test: deliberately cause an ingest task to fail and verify the system logs the failure and continues running other tasks (no global crash). The runtime builder configuration should now treat data backbone, understanding loop, and drift monitors as first-class citizens (no manual steps to enable them).
  - Progress: Timescale ingest schedulers, liquidity prober probes, and FIX broker loops now require or lazily bind a `TaskSupervisor`, and regression coverage proves a failing ingest runner no longer cancels sibling tasks, closing the last unsupervised runtime surfaces.【src/data_foundation/ingest/scheduler.py:99】【src/trading/execution/liquidity_prober.py:83】【src/trading/integration/fix_broker_interface.py:126】【tests/runtime/test_task_supervisor.py:63】
**Acceptance tests:** Run the full system for an hour in a test (accelerated or with dummy fast-forward) – all tasks should remain running. Introduce a fault (e.g. make the Kafka consumer throw an exception) and verify via logs or events that the supervisor caught it and attempted recovery.

- [x] Deterministic Risk Enforcement – Enforce all risk limits and ensure any breach is caught before trade execution. Expand the risk checks beyond drift: e.g., incorporate position sizing rules, max drawdown limits, and exposure limits from the Policy Ledger into the execution gating. DoD: The trading execution module (or a new RiskManager component) will intercept proposed trades and validate: (a) position size does not exceed a percentage of portfolio based on confidence, (b) leverage constraints are respected, (c) if adding this position would breach max sector or asset exposure, it’s blocked. Any violation should result in the trade being either converted to paper or cancelled, and an event recorded. These checks must run in O(1) time per trade (simple arithmetic) for determinism. Add tests simulating a trade that violates each rule – confirm the system sets decision.decision outcome to “REJECTED” or similar, and that the diary notes the reason (e.g. “Blocked by risk policy: exposure limit”).
  - Completion: RiskGateway now enforces confidence-notional, leverage, and sector caps derived from `RiskConfig`, rejects breaches with reason-coded telemetry, and ships guardrail tests for each violation path alongside refreshed limit snapshots for diaries and dashboards.【src/trading/risk/risk_gateway.py:336】【tests/current/test_risk_gateway_validation.py:452】【tests/current/test_risk_gateway_validation.py:479】【tests/current/test_risk_gateway_validation.py:528】
**Acceptance tests:** Using a controlled scenario, attempt to execute a trade that would take the account over a 5:1 leverage – verify it is forced to paper (if in dev mode) or not executed at all, with a log entry. Also confirm normal trades (well within limits) pass through. This ensures no silent breach of risk happens[4].

- [ ] Governance Checkpoints in Loop – Integrate governance stage checks at every decision point. Currently, the ledger stage is consulted in the loop orchestrator to resolve thresholds[37]; expand this so that at the moment of decision routing, the system double-checks the strategy’s allowed actions. DoD: If a strategy is in “Shadow” (paper) stage, the orchestrator (or release router) ensures force_paper=True for that strategy regardless of drift (even if drift is normal). If a strategy is unapproved (not in ledger or in “Concept” stage), the system should not execute it live under any circumstance. Essentially, tie the strategy’s release stage directly to execution mode: Concept/Experiment -> can only simulate; Paper -> can execute on paper; Live -> allowed to send real orders (provided other checks pass)[32]. Tests: try to trick the system by manually calling an execution with a concept-stage strategy – it should refuse or convert to paper. Also verify that when a strategy is promoted to Live (in the ledger), the next run no longer forces paper (assuming drift is fine).
**Acceptance tests:** Simulate the promotion process: Start with a new strategy (stage=experiment) – it should only paper trade (check diary/log). Update ledger to stage=live for that strategy (simulate governance approval), run again – now the system allows it to go live (perhaps by printing “Live trade allowed” in logs). This demonstrates closed-loop governance enforcement.

- [x] Policy Promotion Workflow & Tests – Fully test and document the governance CLI tools and their effects. During this phase, as strategies evolve, use the promote_policy.py and alpha_trade_graduation.py tools in dry-runs to ensure they behave as expected. Possibly improve them: e.g. add a prompt or config for multi-approver requirement or automatic evidence attachment. DoD: The promotion CLI and graduation CLI have 100% scenario coverage – tests for promoting with valid evidence (should succeed) and without evidence (should fail)[38] are in place. If any gaps are found (e.g. no test for threshold override parsing), add them. Ensure the CLI outputs are clear (they produce a markdown or summary of what was changed). Definition of Done for governance could include “All promotions produce a log entry in a governance log file and update the ledger JSON.” Documentation is updated to instruct how a user promotes a strategy from paper to live.
  - Completion: `promote_policy.py` now enforces distinct-approver thresholds for limited live, streams JSONL logs, and writes Markdown summaries via shared helpers, with the runbook updated and CLI tests covering evidence validation, logging, and waiver flows end to end.【tools/governance/promote_policy.py:122】【tools/governance/_promotion_helpers.py:13】【docs/operations/runbooks/policy_promotion_governance.md:15】【tests/tools/test_promote_policy_cli.py:1】
**Acceptance tests:** Perform an end-to-end promotion in a staging environment: mark a strategy as ready (perhaps by having diaries showing X successful paper trades), run the promotion CLI with that strategy and dummy approval inputs – confirm the ledger file updates the stage, and the next loop run recognizes the new stage. This test should mimic a real review board decision to ensure the tools support the process.

- [x] Compliance & Observability Enhancements – Integrate risk and governance events into observability dashboards. Expand the existing observability dashboard (which currently has panels for drift, regime, etc.) to include compliance status – for example, a panel that shows any recent risk limit warnings or governance actions (like “Strategy Y demoted to paper due to breach”). DoD: The observability_dashboard module now consumes events from risk enforcement and governance: each loop iteration’s result (AlphaTradeLoopResult) includes a summary of any risk interventions and this is reflected on the dashboard[48]. Additionally, define Service Level Objectives (SLOs) for risk/compliance (e.g. “0 trades executed that violate policy” as a target) and surface these on the dashboard. Set up Prometheus gauges or log counters for: number of policy breaches caught, number of governance promotions done, etc.[49][50]. Write a small test to simulate a breach and ensure the dashboard data structure contains an entry for it. Completed in 2fc70b5 by introducing compliance/governance Prometheus gauges, wiring the loop’s compliance events into a dedicated dashboard panel, and adding regression coverage for risk breach scenarios.
**Acceptance tests:** Trigger a known compliance event (e.g. push a fake trade that violates a rule) and refresh the observability data – confirm that a WARN flag or specific entry is present in the output (could be as simple as checking a JSON from the dashboard component). Also ensure normal operation (no breaches) results in a green status for compliance. This keeps operators informed in real time.

- [x] Graph Diagnostics & Health Metrics – Augment the understanding loop graph diagnostics with “health” metrics. Since Phase I introduced sparse activations, now compute metrics like graph sparsity, degree distribution, and synapse utilization from the fast-weight adaptation graph. DoD: The graph_diagnostics CLI or a new graph_health.py tool calculates: (a) percentage of active fast-weight connections (non-zero weights) in the understanding graph, (b) distribution of strategy activation counts (how many times each strategy was chosen in a window), and (c) any nodes with disproportionate influence (e.g. a strategy that is always chosen or never chosen). These metrics should be output alongside the graph structure (e.g. as a section in the Markdown report)[51]. Tests: construct a scenario with a known activation pattern (maybe stub the router to activate the same strategy every time) and verify the metrics catch this (e.g. one node with 100% activation).
  - Completion: Diagnostics builder now synthesises decision windows, computes fast-weight utilisation, sparsity, and dominance summaries, and surfaces them through the CLI JSON/Markdown exports under regression coverage.【src/understanding/diagnostics.py†L621-L958】【tests/understanding/test_understanding_diagnostics.py†L15-L39】【tests/tools/test_understanding_graph_cli.py†L8-L19】
**Acceptance tests:** After running the system for some period, generate the diagnostics report. It should show, for example, “Graph sparsity: 85% of possible fast-weight connections are zero” and “No single strategy exceeds 30% usage” (or if it does, highlight it). Reviewers can use this to decide if the adaptation is balanced or needs tweaking. These graph health metrics ensure the adaptation mechanism stays interpretable and controlled, aligning with BDH’s emphasis on modularity and sparse activation[52][53].

- [ ] Policy & Code Audit (Phase II Completion) – Conduct a mini-audit focusing on governance and risk code. Before moving to Phase III, spend ~2 days reviewing the code for any weaknesses in enforcement logic, race conditions in async tasks, or gaps in test coverage identified in prior phases. DoD: A short report (added to docs or as an issue ticket) listing any findings and quick fixes. Address critical fixes immediately (e.g. if a race condition is found in risk gating, fix it now). Ensure test coverage for governance/risk has improved (target, say, 85%+ for those modules).
**Acceptance tests:** All Phase II features should have passing tests. CI pipeline shows improved coverage numbers for src/governance and src/trading modules. Any remaining TODO or FIXME notes in those modules are resolved or ticketed for later with a plan.

## Phase III (Days 61–90): Full Integration & Paper Trading Readiness

**Goal:** Tie everything together and demonstrate the AlphaTrade system operating as a cohesive whole. In Phase III, we focus on end-to-end runs, system optimization, and documentation/preparation for a “paper trading launch.” This includes finalizing any features needed for a controlled public demo or an internal pilot running on a paper trading account. We also incorporate the remaining BDH-inspired ideas: making sure activations remain interpretable and that the system’s behavior can be explained (key for writing a paper or report on AlphaTrade). By the end of this phase, the system should be ready for either a formal write-up (if academic bent) and/or a closed beta with paper trading, with confidence in its stability and clarity.

- [ ] End-to-End Paper Trade Simulation – Integrate with a paper trading API and simulate real trading. Connect the execution module to a broker’s paper trading endpoint (e.g. Interactive Brokers paper account or a sandbox exchange API). DoD: The system can place paper trades in real-time based on its decisions. This requires implementing a minimal BrokerAdapter class to translate AlphaTrade’s internal order (symbol, quantity, etc.) into API calls. All trades should go through this adapter when stage=Live but using paper credentials. Conduct a full-day simulation on a live market (in paper mode) with no manual intervention. The system should ingest live data, make decisions, and “execute” them via the paper API, while logging outcomes to the decision diary. Any error (API failure, etc.) should be caught by the supervisor with the system continuing (failover to no-trade rather than crash).
  - Progress: PaperBrokerExecutionAdapter now installs as the trading manager’s live engine and, when paper-trading extras are supplied, bootstrap attaches the REST-based `PaperTradingApiAdapter`; integration tests cover adapter lifecycle, runtime cleanup callbacks, and HTTP order placement while full-day simulations remain outstanding.【src/runtime/predator_app.py:2095】【src/trading/integration/paper_trading_api.py:1】【tests/runtime/test_bootstrap_paper_broker.py:1】【tests/trading/test_paper_trading_api_adapter.py:1】
**Acceptance tests:** (Integration test) Hook up to a testnet or simulated exchange environment for a short run – e.g. let the system trade a single asset for 1 hour. Verify that at least one trade is attempted and logged in the diary. Check that all risk and governance checks still applied (e.g. if a trade was blocked by risk, it did not reach the API). This proves the system is “paper-ready” in that it can run continuously with real data and trading actions, but without risking real money.

- [ ] Performance Tuning & Throttle – Optimize system performance and add throttling to avoid overtrading. Analyze any bottlenecks revealed in the end-to-end run (Phase II/III tests). If the adaptation or ingest loop is slow, optimize critical sections (vectorize computations, use asyncio properly, etc.). Also, implement a configurable Trade Throttle – e.g. limit the system to at most N trades per minute or require a minimum time between trades, as a safety governance measure. DoD: No backlog buildup in event loop (system processes data in real-time without lag). CPU and memory usage are within acceptable bounds on a test machine (document baseline resource usage). The throttle mechanism is in place: if the system is in a rapid oscillation, it will log “Throttled: too many trades in short time” and skip or delay some decisions. Tests for throttle: simulate a scenario where the strategy would trade on every tick; with throttle set to, say, 1 trade/minute, ensure it only executes the first trade and defers others, and diary notes the throttle activation[54][48].
**Acceptance tests:** Run a high-frequency data replay (e.g. tick data) faster than real-time – confirm that the system doesn’t fall behind processing (throughput is sufficient). Also, check logs for any “throttled” messages when appropriate. This will give confidence that when connected to live markets, AlphaTrade won’t overload itself or the broker with excessive orders.

- [x] Evaluation and KPIs Collection – Define and collect Key Performance Indicators for the understanding loop and trading performance. Even though still in paper mode, decide on metrics such as: win rate of strategy decisions, average trade return, maximum drawdown in the paper account, accuracy of regime detection, and false positive/negative rate of drift alerts. DoD: Implement a StrategyPerformanceTracker (if not already) that computes ROI, P&L, and other stats per strategy[22]. At end of each day (or test run), output a summary (could integrate with the observability dashboard or as a separate report). Metrics on the cognitive loop itself (e.g. “Did the fast-weight adaptation improve decision quality?”) are gathered by comparing periods with it on vs off. Possibly use the existing benchmark harness to log latency and variance impact of fast-weights[55]. Tests: artificially manipulate outcomes to see if tracker correctly computes metrics (e.g. feed 10 trades with known P&L and verify ROI calc).
  - Completion: `StrategyPerformanceTracker` now calculates per-strategy KPIs, ROI snapshots, loop metrics, and Markdown summaries with coverage validating trades, regime/ drift stats, and report surfaces so operational dashboards can rely on a single aggregation surface.【src/operations/strategy_performance_tracker.py:1】【tests/operations/test_strategy_performance_tracker.py:1】
**Acceptance tests:** After a multi-day paper trading simulation, produce the performance report. It should list, for example: “Strategy A: 30 trades, 60% win rate, +2.3% ROI; Strategy B: 15 trades, 40% win rate, -1.0% ROI; Overall P&L: +1.5%”. It should also include system metrics like uptime, number of drift alerts, etc. These KPIs will inform whether the system is ready to consider live trading or needs adjustments (and also provide material for a potential paper).

- [ ] Documentation: AlphaTrade Whitepaper Draft – Compile the architecture, methods, and results into a draft paper or detailed technical document. By now, we have a wealth of information (design, features, metrics, and possibly unique findings like the effect of fast-weights). DoD: A comprehensive document (could be docs/AlphaTrade_Whitepaper.md) is written, including: the high-level architecture diagram of Perception→Adaptation→Reflection→Execution→Governance loop, an explanation of the BDH-inspired features (fast weights, positive sparse activations, graph metrics) and why they matter, and preliminary results from the paper-trading simulation (e.g. charts of performance, illustrations of decision graphs). This serves as the “Paper readiness” proof – demonstrating we can communicate the system’s value to stakeholders or as an academic/industry case study.
**Acceptance tests:** (Editorial review) Ensure the document covers all important points without contradiction. Stakeholders should be able to read it and understand how AlphaTrade works and how it was validated. In the context of development, treat the completion and approval of this document as a milestone indicating the system is stable and well-understood.

- [ ] Final Dry Run & Sign-off – Run a final dry-run of the entire system under realistic conditions for several days and fix any last bugs. This is effectively a user acceptance test (UAT) for the platform. DoD: The system runs continuously (in paper mode) for, say, 3 days straight with no crashes or major errors. All logs remain at info/debug (no uncaught exceptions). Any minor issues found are documented and either fixed or added to a backlog for post-90-day work. At the end of this run, gather all evidence (logs, performance metrics, diary samples) and have a wrap-up review meeting.
**Acceptance criteria:** The review board (or project owner) signs off that the 90-day objectives have been met: data backbone is live, understanding loop is autonomous and interpretable, governance and risk are enforced, and the system is ready for either live pilot or publication. If any objective is not met, we clearly itemize why and possibly schedule that as priority in the next roadmap.

## Directory Layout & Code Practices

Throughout the roadmap execution, maintain a clean project structure and code quality:
Organize new code by domain layer for clarity. For example, perception-related additions go under src/sensory or src/understanding (ensuring sensory organs and belief logic stay together), adaptation and learning components under src/thinking (e.g. adaptation/fast_weights.py, evolution/ subpackage), execution and risk under src/trading or src/risk, and governance tools under src/governance. This preserves the core → sensory → thinking → trading → orchestration layering[4] and helps new contributors map features to architecture. If certain cross-cutting concerns (like operations/incident_response.py or operations/event_bus_failover.py) exist, document their role clearly rather than moving them, to avoid confusion.
Continue using dataclasses and Pydantic models for config and state where appropriate to enforce schema (e.g. a Pydantic model for a strategy config ensures required fields are present). All public interfaces of modules should be well-defined – consider adding .pyi stub files for any dynamically generated interfaces, following the project’s stub guidelines (model minimal API surface, avoid Any types)[40][56]. This acts as an explicit contract for each component.
Each new major feature should come with corresponding tests (unit and integration) and documentation. Adhere to the existing guardrail testing concept: critical paths (like ingest, risk enforcement, fast-weight toggling) should have tests marked with guardrail so they run in CI gatekeeping[57]. Likewise, update the pytest markers/manifest to include new domains if needed so nothing slips through CI.
Maintain and improve the CI coverage: aim to raise coverage from ~76% into the 80s or 90s by adding tests as you implement features[58]. Particularly, ensure new code in previously untested areas (sensory organs, evolution, broker adapter) has thorough test cases. Leverage the existing style of golden files and replay tests for deterministic behavior (e.g. record a golden belief snapshot JSON from Phase I and use it to test backward compatibility after later changes).
Keep documentation up-to-date with code. The roadmap’s context briefs and sprint briefs should be revised when assumptions change. For example, once real data ingest is done, the alignment brief for the data backbone can be marked as achieved or updated with remaining gaps. This ensures the “concept promises” in the docs match reality, maintaining alignment between narrative and implementation[59].
By following this roadmap, in 90 days we will have transformed the AlphaTrade prototype from a scaffold into a paper-trading-ready, well-governed AI trading loop. We will have demonstrated the system’s ability to perceive real market data, adapt using fast weights and evolutionary tweaks, reflect on its decisions with interpretable metrics, execute trades under strict risk controls, and govern itself through policy stages. Quick wins are captured early to build momentum, and risky, research-heavy tasks are deferred or made optional behind feature flags. Each phase concludes with concrete acceptance tests and definitions of done, giving a clear target for completion. With this structured approach, the team and stakeholders can track progress and gain confidence as AlphaTrade moves toward both practical deployment and the potential to contribute novel insights (via the BDH-inspired design) to the algorithmic trading community.

## Automation updates — 2025-10-08T11:32:42Z

### Last 4 commits
- a008e94 feat(runtime): add 5 files (2025-10-08)
- 96cd9e3 docs(docs): tune 3 files (2025-10-08)
- 758515d docs(config): add 7 files (2025-10-08)
- 2fc70b5 refactor(operational): tune 4 files (2025-10-08)
