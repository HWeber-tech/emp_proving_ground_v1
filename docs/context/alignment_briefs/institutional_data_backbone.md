# Alignment brief – Institutional data backbone

**Why this brief exists:** The concept blueprint promises a professional-grade data backbone backed by TimescaleDB, Redis, Kafka, and Spark once the system graduates beyond the 1 GB bootstrap footprint. This document anchors the delivery plan to that narrative so discovery, tickets, and reviews share the same context.

## Concept promise

- The technology stack specifies TimescaleDB for time-series persistence, Redis for caching, and Kafka/Spark for streaming and batch workloads in the professional tiers.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L376-L994】
- Tier 1+ infrastructure elevates the storage layer to a TimescaleDB cluster with Redis caching, while Tier 2 expands into enterprise-grade clusters.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L569-L988】
- The roadmap gap table reiterates that institutional readiness hinges on a tiered backbone capable of switching between bootstrap and professional services without downtime.【F:docs/roadmap.md†L22-L66】

## Reality snapshot (September 2025)

- Tier‑0 ingest relies on a Yahoo helper that writes into DuckDB or CSV files, with no TimescaleDB wiring or orchestration layer.【F:src/data_foundation/ingest/yahoo_ingest.py†L1-L105】
- The runtime now exposes a `data_backbone_mode` toggle that defaults to bootstrap ingest; institutional ingest executes when operators switch the mode, while Redis/Kafka remain stubs.【F:src/runtime/runtime_builder.py†L700-L898】
- `SystemConfig` keeps the default backbone mode on bootstrap unless Redis/Kafka credentials are detected, so staging builds continue to run DuckDB/in-memory helpers until the institutional services are provisioned.【F:src/governance/system_config.py†L1-L231】
- Redis cache plumbing now materialises production clients when `data_backbone_mode` is institutional, injecting them into the runtime trading stack while bootstrap tiers continue to use the in-memory fallback.【F:src/data_foundation/cache/redis_cache.py†L1-L212】【F:src/runtime/predator_app.py†L1-L420】
- `evaluate_cache_health` records Redis namespace coverage, hit/miss ratios, and eviction pressure into a `telemetry.cache.health` event and runtime summary block so operators can verify cache readiness alongside ingest health and professional readiness snapshots.【F:src/operations/cache_health.py†L1-L191】【F:src/runtime/runtime_builder.py†L608-L741】【F:src/runtime/predator_app.py†L200-L360】
- `RuntimeApplication` and the builder in `src/runtime/runtime_builder.py` now separate
  ingestion and trading workloads, publish structured plan metadata, and register
  managed shutdown hooks so institutional ingest can run alongside live trading with
  explicit telemetry.【F:src/runtime/runtime_builder.py†L76-L898】【F:tests/runtime/test_runtime_builder.py†L1-L118】
- `TimescaleQueryCache` now serialises reader results into Redis-managed namespaces and the professional runtime wires the Timescale connectors through the shared cache, with pytest coverage validating cache hits, TTL expiry, and runtime integration.【F:src/data_foundation/cache/timescale_query_cache.py†L1-L190】【F:src/data_foundation/fabric/timescale_connector.py†L1-L160】【F:src/runtime/predator_app.py†L320-L520】【F:tests/data_foundation/test_timescale_cache.py†L1-L160】
- `TimescaleMacroEventService` enriches market-data connectors with macro bias, confidence, and recent event metadata so sensors receive institutional macro context without bespoke queries.【F:src/data_foundation/services/macro_events.py†L1-L226】【F:src/data_foundation/fabric/timescale_connector.py†L1-L164】【F:src/runtime/predator_app.py†L420-L470】【F:tests/data_foundation/test_timescale_connectors.py†L1-L150】
- Kafka streaming scaffolding now exposes connection settings, sanitised event payloads, orchestrator hooks, and runtime wiring that instantiates ingest publishers whenever Kafka credentials and topics are supplied. `KafkaIngestEventConsumer` mirrors the configured topics back onto the runtime event bus so institutional runs inherit Kafka-sourced telemetry without custom polling, and `KafkaIngestHealthPublisher` continues to mirror `telemetry.ingest.health` payloads for downstream monitoring stacks.【F:src/data_foundation/streaming/kafka_stream.py†L1-L924】【F:src/data_foundation/ingest/timescale_pipeline.py†L1-L227】【F:src/runtime/predator_app.py†L500-L540】【F:tests/data_foundation/test_kafka_stream.py†L1-L332】
  The consumer can also emit `telemetry.kafka.lag` snapshots via `capture_consumer_lag`, surfacing per-partition offsets, totals, and metadata so operators can monitor Kafka lag without bespoke scripts.【F:src/data_foundation/streaming/kafka_stream.py†L259-L415】【F:src/data_foundation/streaming/kafka_stream.py†L1700-L1919】【F:tests/data_foundation/test_kafka_stream.py†L453-L564】
- `evaluate_kafka_readiness` now blends connection settings, topic provisioning summaries, publisher availability, and lag telemetry into `telemetry.kafka.readiness`, with the runtime builder publishing and recording the snapshot so professional summaries surface streaming posture alongside ingest/cross-region readiness. Pytest covers the evaluator plus runtime storage hooks.【F:src/operations/kafka_readiness.py†L1-L213】【F:src/runtime/runtime_builder.py†L600-L930】【F:src/runtime/predator_app.py†L200-L400】【F:tests/operations/test_kafka_readiness.py†L1-L97】【F:tests/runtime/test_runtime_builder.py†L665-L782】【F:tests/runtime/test_professional_app_timescale.py†L1116-L1160】
- `KafkaTopicProvisioner` now provisions ingest topics ahead of runtime startup when `KAFKA_INGEST_AUTO_CREATE_TOPICS` (or `KAFKA_AUTO_CREATE_TOPICS`) is enabled, and `build_institutional_ingest_config` surfaces the resolved topic list plus provisioning toggle in its metadata so operators can audit planned topics alongside Timescale plans.【F:src/data_foundation/streaming/kafka_stream.py†L226-L470】【F:src/runtime/runtime_builder.py†L752-L806】【F:src/data_foundation/ingest/configuration.py†L1-L210】【F:tests/data_foundation/test_kafka_stream.py†L60-L360】【F:tests/data_foundation/test_timescale_config.py†L1-L160】
- `build_institutional_ingest_config` converts `SystemConfig` extras into orchestrator plans, logging metadata for auditability and falling back to DuckDB ingest when the institutional plan is incomplete; pytest coverage documents the supported knobs for operators.【F:src/data_foundation/ingest/configuration.py†L1-L208】【F:tests/data_foundation/test_timescale_config.py†L1-L147】
- `TimescaleIngestScheduler` runs the orchestrator plan on configurable intervals whenever extras enable the schedule, and the professional runtime registers the scheduler as a managed background task so institutional ingest keeps Timescale data current without manual commands.【F:src/data_foundation/ingest/scheduler.py†L1-L137】【F:src/data_foundation/ingest/configuration.py†L1-L210】【F:src/runtime/runtime_builder.py†L845-L869】【F:tests/data_foundation/test_ingest_scheduler.py†L1-L78】
- `evaluate_configuration_audit` inspects successive `SystemConfig` snapshots, highlights tier/run-mode/credential changes, publishes `telemetry.runtime.configuration`, and records markdown-backed summaries on the professional runtime while Timescale persistence stores a durable audit log for operators and compliance reviewers.【F:src/operations/configuration_audit.py†L1-L308】【F:src/runtime/runtime_builder.py†L1-L260】【F:src/runtime/predator_app.py†L1-L400】【F:src/data_foundation/persist/timescale.py†L1-L2150】【F:tests/operations/test_configuration_audit.py†L1-L94】【F:tests/runtime/test_professional_app_timescale.py†L1-L200】【F:tests/data_foundation/test_timescale_configuration_journal.py†L1-L47】
- `evaluate_data_retention` grades Timescale retention windows across daily, intraday, and macro tables, publishes `telemetry.data_backbone.retention`, and surfaces the markdown snapshot inside `ProfessionalPredatorApp.summary()` so operators can confirm historical coverage alongside other backbone feeds.【F:src/operations/retention.py†L1-L192】【F:src/runtime/runtime_builder.py†L1060-L1210】【F:src/runtime/predator_app.py†L150-L360】【F:tests/operations/test_data_retention.py†L1-L118】【F:tests/runtime/test_professional_app_timescale.py†L640-L700】
- CI health snapshot now documents ingest metrics, health, and the operational SLO feed published on `telemetry.operational.slos`, giving drills a canonical source for freshness/completeness status and alert routing metadata.【F:docs/status/ci_health.md†L1-L120】【F:src/operations/slo.py†L1-L206】

## Gap map

| Concept excerpt | Observable gap | Impact |
| --- | --- | --- |
| TimescaleDB + Redis + Kafka/Spark form the professional data foundation.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L376-L994】 | Only DuckDB/CSV helpers exist; Redis/Kafka hooks default to in-memory stubs.【F:src/data_foundation/ingest/yahoo_ingest.py†L1-L105】【F:src/trading/monitoring/portfolio_monitor.py†L64-L137】 | Professional tiers lack persistence guarantees, cache invalidation, and streaming semantics.
| Tier switching should promote resiliency and operate without downtime.【F:docs/roadmap.md†L22-L66】 | Runtime toggles between bootstrap and Timescale ingest via `data_backbone_mode`, but Redis/Kafka switches and validation harnesses remain missing.【F:src/runtime/runtime_builder.py†L700-L898】【F:src/governance/system_config.py†L1-L231】 | Operators still lack an end-to-end institutional failover path until caches/streaming toggles and validation land.
| Data foundation layer requires validation, anomaly detection, and quality metrics.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L893-L925】 | Quality checks depend on ad-hoc scripts; roadmap lacks acceptance criteria for telemetry coverage yet.【F:docs/status/ci_health.md†L1-L56】 | Missing guardrails risk data drift, stale feeds, and undetected ingestion failures.

## Delivery plan

### Now (30-day outlook)

1. **TimescaleDB prototype vertical**
   - Design schemas for daily bars, intraday trades, and macro series with migration scripts aligned to the Tier 1 spec.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L569-L988】
   - Build a migration + connection package that reuses the Yahoo ingest output as seed data, exercising connection pooling and error handling.【F:docs/roadmap.md†L53-L66】
   - Instrument pytest/notebook smoke tests that confirm round-trip persistence and expose freshness/latency metrics for CI snapshots.【F:docs/roadmap.md†L53-L66】【F:docs/status/ci_health.md†L1-L56】
   - **Status:** `TimescaleConnectionSettings`, `TimescaleMigrator`, and `TimescaleIngestor` now land in `src/data_foundation/persist/timescale.py`, with `ingest_yahoo_daily_bars` wiring bootstrap data into the new hypertables and tests covering schema creation + idempotent replays.【F:src/data_foundation/persist/timescale.py†L1-L410】【F:src/data_foundation/ingest/timescale_pipeline.py†L1-L227】【F:tests/data_foundation/test_timescale_ingest.py†L1-L220】
     `TimescaleReader` provides sensor- and risk-friendly read APIs backed by pandas dataframes, and additional smoke tests prove date-window and macro calendar queries execute through the institutional tables.【F:src/data_foundation/persist/timescale_reader.py†L1-L247】【F:tests/data_foundation/test_timescale_ingest.py†L270-L385】
     `SystemConfig` now surfaces a `data_backbone_mode` toggle and the runtime entrypoint routes ingest through DuckDB or Timescale based on that mode, preparing the orchestrator for Redis/Kafka validation in upcoming sprints. The orchestrator now emits ingest metadata to pluggable publishers so Kafka pipelines inherit structured events immediately, with `build_institutional_ingest_config` exposing the configuration knobs and validation paths for operators.【F:src/governance/system_config.py†L1-L231】【F:src/data_foundation/ingest/timescale_pipeline.py†L87-L227】【F:src/data_foundation/ingest/configuration.py†L1-L208】【F:src/runtime/runtime_builder.py†L700-L898】
    `evaluate_ingest_health` converts orchestrator results into pass/warn/fail health reports and the runtime publishes the aggregated payload on `telemetry.ingest.health`, giving CI and operators a consistent ingest freshness signal, while `summarise_ingest_metrics` produces complementary `telemetry.ingest.metrics` snapshots with per-dimension row counts, freshness, and symbol coverage for dashboards. `KafkaIngestMetricsPublisher` now mirrors the same metrics payloads to Kafka so institutional telemetry stacks ingest identical totals regardless of transport.【F:src/data_foundation/ingest/health.py†L1-L197】【F:src/data_foundation/ingest/metrics.py†L1-L88】【F:src/data_foundation/streaming/kafka_stream.py†L1022-L1135】【F:src/runtime/runtime_builder.py†L577-L593】
    `evaluate_ingest_quality` grades coverage and freshness into a quality score that the runtime publishes on `telemetry.ingest.quality`, and the new `KafkaIngestQualityPublisher` mirrors the payloads so dashboards and Kafka consumers see the same density telemetry alongside health and metrics.【F:src/data_foundation/ingest/quality.py†L1-L208】【F:src/runtime/runtime_builder.py†L649-L706】【F:src/data_foundation/streaming/kafka_stream.py†L1126-L1287】【F:tests/data_foundation/test_ingest_quality.py†L1-L60】
    `build_ingest_observability_snapshot` fuses metrics, health checks, recovery recommendations, and failover decisions into a single `telemetry.ingest.observability` payload so runtimes, dashboards, and CI reports can consume one authoritative ingest summary. Tests cover the snapshot contract to keep the observability surface stable as recovery/failover evolve.【F:src/data_foundation/ingest/observability.py†L1-L211】【F:tests/data_foundation/test_ingest_observability.py†L1-L110】【F:src/runtime/runtime_builder.py†L624-L635】
    `evaluate_data_backbone_readiness` fuses ingest health, quality, recovery, backup posture, Redis/Kafka wiring, and scheduler state into `telemetry.data_backbone.readiness`, and the runtime records the latest snapshot for operator summaries while pytest covers the contract and runtime wiring.【F:src/operations/data_backbone.py†L1-L320】【F:src/runtime/runtime_builder.py†L600-L900】【F:src/runtime/predator_app.py†L200-L430】【F:tests/operations/test_data_backbone.py†L1-L220】【F:tests/runtime/test_professional_app_timescale.py†L395-L460】
    `evaluate_data_backbone_validation` now validates Timescale connection settings, Redis/Kafka expectations, and scheduler telemetry, emitting `telemetry.data_backbone.validation` alongside markdown summaries so operators can confirm institutional toggles before ingest executes.【F:src/operations/data_backbone.py†L120-L320】【F:src/runtime/runtime_builder.py†L650-L780】【F:src/runtime/predator_app.py†L150-L370】【F:tests/operations/test_data_backbone.py†L120-L220】【F:tests/runtime/test_runtime_builder.py†L180-L280】【F:tests/runtime/test_professional_app_timescale.py†L395-L460】 The runtime halts Timescale ingest when validation fails, records the degraded readiness snapshot, and falls back to the DuckDB pipeline so institutional runs never proceed with an invalid backbone configuration.【F:src/runtime/runtime_builder.py†L700-L780】【F:tests/data_foundation/test_ingest_journal.py†L300-L374】
    `tools/telemetry/export_data_backbone_snapshots.py` packages the readiness, validation, retention, ingest trend/scheduler, and Kafka posture blocks from `ProfessionalPredatorApp.summary()` into a JSON payload so dashboards and runbooks ingest the backbone state without parsing Markdown; pytest locks the CLI contract and missing-section guardrails.【F:tools/telemetry/export_data_backbone_snapshots.py†L1-L147】【F:tests/tools/test_data_backbone_export.py†L1-L74】
    `evaluate_professional_readiness` layers that backbone snapshot with backup posture, ingest SLOs, failover outcomes, and recovery recommendations to produce `telemetry.operational.readiness`, giving the runtime and dashboards a single professional readiness view alongside ingest-specific feeds.【F:src/operations/professional_readiness.py†L1-L190】【F:src/runtime/runtime_builder.py†L360-L873】【F:src/runtime/predator_app.py†L1-L470】【F:tests/operations/test_professional_readiness.py†L1-L120】【F:tests/runtime/test_professional_app_timescale.py†L1-L320】
    `TimescaleIngestScheduler` keeps the prototype plan running on a configurable cadence, drawing interval/jitter/max-failure knobs from extras and wiring the scheduler into the runtime lifecycle so ingest freshness tests stay relevant between manual runs.【F:src/data_foundation/ingest/scheduler.py†L1-L137】【F:src/data_foundation/ingest/configuration.py†L1-L210】【F:src/runtime/runtime_builder.py†L845-L869】【F:tests/data_foundation/test_ingest_scheduler.py†L1-L78】【F:tests/data_foundation/test_timescale_config.py†L120-L170】
    `decide_ingest_failover` analyses the same health report, publishes a `telemetry.ingest.failover` decision, and automatically replays the DuckDB bootstrap ingest when required slices fail so operators inherit a documented rollback path for institutional runs.【F:src/data_foundation/ingest/failover.py†L1-L120】【F:src/runtime/runtime_builder.py†L618-L650】【F:tests/data_foundation/test_ingest_failover.py†L1-L120】
    `evaluate_backup_readiness` evaluates backup posture, publishes `telemetry.operational.backups`, and records the latest snapshot inside the runtime summary so operators see backup and restore drift alongside ingest telemetry.【F:src/operations/backup.py†L1-L206】【F:src/runtime/runtime_builder.py†L300-L360】【F:src/runtime/predator_app.py†L260-L380】【F:tests/operations/test_backup.py†L1-L80】
    `TimescaleIngestJournal` records every ingest attempt (status, rows, freshness, symbols, plan metadata) into a `telemetry.ingest_runs` table so operators and dashboards can audit the institutional pipeline directly from Timescale; `_record_ingest_journal` wires the runtime to persist each run after health evaluation and pytest covers the journal round-trip plus runtime integration. `ProfessionalPredatorApp.summary()` now exposes these journal entries and their aggregated statuses so runtime consumers can inspect ingest health without writing SQL.【F:src/data_foundation/persist/timescale.py†L1-L720】【F:src/runtime/runtime_builder.py†L347-L437】【F:src/runtime/predator_app.py†L220-L317】【F:tests/data_foundation/test_ingest_journal.py†L1-L200】【F:tests/runtime/test_professional_app_timescale.py†L120-L196】
    `TimescaleExecutionJournal` extends the persistence layer with a `telemetry.execution_snapshots` table and the professional runtime now records every execution readiness snapshot, surfacing recent and latest entries in `summary()` so institutional operators inherit an auditable execution history alongside live telemetry.【F:src/data_foundation/persist/timescale.py†L900-L1290】【F:src/runtime/predator_app.py†L200-L760】【F:tests/data_foundation/test_timescale_execution_journal.py†L1-L91】【F:tests/runtime/test_professional_app_timescale.py†L400-L460】
    `plan_ingest_recovery` inspects degraded ingest health reports, rebuilds targeted plans for missing symbols with extended lookbacks, and replays them before failover triggers. Successful recoveries merge into the aggregated metrics and journal metadata so operators can see the automated retries alongside the original run outcomes.【F:src/data_foundation/ingest/recovery.py†L1-L152】【F:src/runtime/runtime_builder.py†L484-L570】【F:tests/data_foundation/test_ingest_recovery.py†L1-L95】【F:tests/data_foundation/test_ingest_journal.py†L120-L197】
    `execute_failover_drill` now simulates Timescale outages using the most recent ingest results, confirms the failover policy, publishes `telemetry.ingest.failover_drill`, and records the Markdown snapshot on the professional runtime so operators can rehearse the DuckDB fallback without touching production extras.【F:src/operations/failover_drill.py†L1-L213】【F:src/runtime/runtime_builder.py†L1015-L1072】【F:src/runtime/predator_app.py†L145-L180】【F:tests/operations/test_failover_drill.py†L1-L88】【F:tests/runtime/test_professional_app_timescale.py†L610-L650】
    `evaluate_cross_region_failover` compares primary ingest telemetry with replica journal history, scores scheduler readiness, and publishes `telemetry.ingest.cross_region_failover` so operators can rehearse cross-region cutovers from runtime summaries before enabling automated failover.【F:src/operations/cross_region_failover.py†L1-L238】【F:src/runtime/runtime_builder.py†L900-L1165】【F:src/runtime/predator_app.py†L180-L340】【F:tests/operations/test_cross_region_failover.py†L1-L130】【F:tests/runtime/test_runtime_builder.py†L250-L360】
    `execute_spark_export_plan` serialises Timescale query results into Spark-friendly datasets, writes per-job manifests, and the runtime publishes `telemetry.ingest.spark_exports` while surfacing the snapshot in professional summaries so operators can confirm batch exports alongside ingest health.【F:src/data_foundation/batch/spark_export.py†L1-L233】【F:src/runtime/runtime_builder.py†L657-L866】【F:src/runtime/predator_app.py†L1-L520】【F:tests/data_foundation/test_spark_export.py†L1-L129】【F:tests/data_foundation/test_ingest_journal.py†L360-L438】【F:tests/runtime/test_professional_app_timescale.py†L1-L620】
    `evaluate_ingest_trends` mines the ingest journal for recent history, publishes `telemetry.ingest.trends`, and records the latest snapshot on the professional runtime so operators see row-count drops or freshness regressions building over time alongside the live ingest feeds.【F:src/operations/ingest_trends.py†L1-L240】【F:src/runtime/runtime_builder.py†L900-L1090】【F:src/runtime/predator_app.py†L680-L735】【F:tests/operations/test_ingest_trends.py†L1-L90】【F:tests/runtime/test_professional_app_timescale.py†L240-L330】
    `evaluate_data_backbone_readiness` folds Spark export results into the backbone readiness snapshot so institutional operators see batch export coverage alongside ingest health and cache/streaming context.【F:src/operations/data_backbone.py†L1-L360】【F:src/runtime/runtime_builder.py†L657-L1160】【F:tests/operations/test_data_backbone.py†L1-L150】

2. **Context pack publishing**
   - Export this brief into issue templates and PR checklists so contributors cite the concept excerpt and acceptance tests by default.【F:docs/roadmap.md†L53-L66】
   - Link schema drafts, migration notebooks, and failure drills from the brief to keep discovery artefacts centralized.
   - **Status:** Roadmap-aligned issue forms and an updated pull-request template now live under `.github/`, prompting authors to quote the concept blueprint, reality gap, validation hooks, and telemetry updates for every change.【F:.github/ISSUE_TEMPLATE/roadmap_execution.yml†L1-L98】【F:.github/pull_request_template.md†L1-L78】

3. **Roadmap guardrails**
   - Extend `tools.roadmap.snapshot` so the modernization status check imports ingest metrics, quality, observability, trend, and validation helpers, surfacing regressions when those modules disappear or rename.【F:docs/roadmap.md†L70-L90】
   - **Status:** The snapshot CLI now requires those modules, causing the readiness dashboard to fail fast if ingest telemetry surfaces regress, keeping this brief, the roadmap, and automation in lockstep.【F:tools/roadmap/snapshot.py†L125-L151】

### Next (60-day outlook)

3. **Redis hot-path cache**
   - Replace `InMemoryRedis` fallbacks with injectable Redis clients and establish caching policies for tiered symbols, including eviction strategies, TTLs, and telemetry.【F:src/trading/monitoring/portfolio_monitor.py†L64-L210】【F:docs/roadmap.md†L68-L80】
   - Provide integration tests that prove cache hits/misses, persistence fallback, and reconnection logic.
- **Status:** `RedisCachePolicy` and `ManagedRedisCache` now enforce TTL/eviction rules, emit hit/miss metrics, and wrap both fakeredis-backed institutional clients and bootstrap fallbacks, while the portfolio monitor publishes cache telemetry through the event bus. `TimescaleQueryCache` extends the policy to Timescale readers so institutional connectors reuse cached slices, with pytest verifying cache hits, TTL expirations, and runtime wiring. The new [Redis cache outage runbook](../../operations/runbooks/redis_cache_outage.md) documents the recovery steps that tie into the telemetry surface.【F:src/data_foundation/cache/redis_cache.py†L1-L263】【F:src/data_foundation/cache/timescale_query_cache.py†L1-L190】【F:src/data_foundation/fabric/timescale_connector.py†L1-L160】【F:src/runtime/predator_app.py†L320-L520】【F:src/trading/monitoring/portfolio_monitor.py†L60-L250】【F:tests/data_foundation/test_redis_cache.py†L1-L120】【F:tests/data_foundation/test_timescale_cache.py†L1-L160】【F:tests/runtime/test_predator_app_redis.py†L1-L110】【F:tests/current/test_portfolio_monitor_runtime.py†L1-L120】【F:docs/operations/runbooks/redis_cache_outage.md†L1-L60】

4. **Kafka event streaming**
   - Stand up a Kafka topic for intraday updates, connect the ingest loop, and pipe events into the runtime bus so sensors/risk modules can subscribe.【F:docs/roadmap.md†L68-L80】
   - Document replay and backfill procedures that tie into Spark batch jobs once available.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L376-L994】
- **Status:** `KafkaConnectionSettings`, `KafkaIngestEventPublisher`, and the runtime helper now turn Kafka extras into live publishers that stream ingest metadata per Timescale slice, while `KafkaIngestEventConsumer` replays those topics onto the runtime bus so Tier‑1 deployments see Kafka updates immediately. Offset commits are configurable via `KAFKA_INGEST_CONSUMER_COMMIT_ON_PUBLISH` / `KAFKA_INGEST_CONSUMER_COMMIT_ASYNC`, letting operators disable auto-commit and rely on the bridge for acknowledgements. Pytest covers topic parsing, publisher wiring, consumer fan-out, offset commits, and the runtime bridge setup. `backfill_ingest_dimension_to_kafka` documents the replay/backfill payload, and `KafkaTopicProvisioner` now auto-creates ingest topics when `KAFKA_INGEST_AUTO_CREATE_TOPICS` is enabled with metadata surfaced via `build_institutional_ingest_config`. The [Kafka ingest offset recovery runbook](../../operations/runbooks/kafka_ingest_offset_recovery.md) captures the operational procedure for advancing stuck offsets and validating lag telemetry.【F:src/data_foundation/streaming/kafka_stream.py†L1-L1756】【F:src/data_foundation/ingest/telemetry.py†L1-L168】【F:src/runtime/predator_app.py†L500-L540】【F:src/runtime/runtime_builder.py†L752-L806】【F:src/data_foundation/ingest/configuration.py†L1-L210】【F:tests/data_foundation/test_kafka_stream.py†L320-L660】【F:tests/runtime/test_professional_app_timescale.py†L1-L123】【F:tests/data_foundation/test_timescale_config.py†L1-L160】【F:docs/operations/runbooks/kafka_ingest_offset_recovery.md†L1-L66】
  `capture_consumer_lag` now feeds optional `telemetry.kafka.lag` events from the same bridge, providing lag totals and partition offsets for dashboards with pytest coverage documenting the cadence guardrails.【F:src/data_foundation/streaming/kafka_stream.py†L259-L415】【F:src/data_foundation/streaming/kafka_stream.py†L1700-L1919】【F:tests/data_foundation/test_kafka_stream.py†L453-L564】

5. **Operational telemetry**
   - Extend ingest telemetry to include SLO grading, alert routing, and runtime publications tied to the context pack so drills inherit actionable outputs.【F:docs/roadmap.md†L112-L130】
- **Status:** `evaluate_ingest_slos` now merges ingest metrics and health reports into an operational snapshot, the runtime publishes the markdown plus a `telemetry.operational.slos` event, and extras can override alert routes via `OPERATIONS_ALERT_ROUTES`; pytest coverage locks the contract.【F:src/operations/slo.py†L1-L206】【F:src/runtime/runtime_builder.py†L624-L669】【F:src/data_foundation/ingest/configuration.py†L1-L244】【F:tests/operations/test_slo.py†L1-L70】【F:tests/data_foundation/test_timescale_config.py†L1-L220】
  `evaluate_system_validation` now parses the concept-aligned validation report, publishes `telemetry.operational.system_validation`, and the professional runtime records the markdown snapshot so operators can confirm architecture checks alongside ingest, security, and readiness feeds.【F:src/operations/system_validation.py†L1-L230】【F:src/runtime/runtime_builder.py†L1905-L1950】【F:src/runtime/predator_app.py†L120-L360】【F:tests/operations/test_system_validation.py†L1-L120】【F:tests/runtime/test_professional_app_timescale.py†L400-L455】【F:tests/runtime/test_runtime_builder.py†L200-L360】

### Later (90-day+ considerations)

6. **Spark analytics & failover drills**
   - Execute Spark jobs that stress-test retention windows, continuous aggregates, and failure recovery scenarios to satisfy the enterprise tier claims.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L376-L994】【F:docs/roadmap.md†L68-L80】
   - Capture runbooks for switchover between DuckDB bootstrap mode and Timescale-backed institutional mode with rollback checkpoints.【F:docs/roadmap.md†L22-L66】
   - **Status:** `execute_spark_stress_drill` provides an optional resilience drill that replays Spark export plans across configurable cycles, enforces warn/fail duration thresholds, publishes `telemetry.ingest.spark_stress`, and surfaces markdown-backed summaries inside the professional runtime summary so operators can validate batch durability alongside ingest telemetry.【F:src/operations/spark_stress.py†L1-L166】【F:src/runtime/runtime_builder.py†L774-L1256】【F:src/runtime/predator_app.py†L150-L860】【F:tests/operations/test_spark_stress.py†L1-L87】【F:tests/runtime/test_professional_app_timescale.py†L740-L813】

## Acceptance criteria & validation hooks

- **Schema + ingest smoke tests** – Pytest suite (or notebook) asserts Timescale inserts, idempotent replays, and freshness SLAs; metrics flow into the CI health snapshot.【F:docs/roadmap.md†L53-L66】【F:docs/status/ci_health.md†L1-L56】
- **Feature-flagged runtime toggle** – CLI or config flag flips the runtime between tier-0 DuckDB and tier-1 Timescale/Redis without manual code edits, validated by integration tests.【F:src/runtime/runtime_builder.py†L700-L898】【F:docs/roadmap.md†L22-L66】
- **Runtime CLI & restart drills** – The `emp-runtime` CLI wraps the runtime builder with `summary`, `run`, `ingest-once`, and `restart` commands, wiring signal-aware shutdowns/timeouts so operators can rehearse ingestion-only cycles or multi-run restarts under pytest coverage.【F:src/runtime/cli.py†L1-L258】【F:tests/runtime/test_runtime_cli.py†L1-L88】
- **Observability** – Alerting plan extends to ingest latency, Kafka lag, and Redis hit ratios, with drill cadence documented alongside existing CI alerts.【F:docs/status/ci_health.md†L1-L87】【F:docs/roadmap.md†L68-L80】

### Appendix – Redis cache defaults

- Default bootstrap policy keeps 30-minute TTLs and a 256-key window to ease local testing, while institutional policy tightens TTL to 15 minutes across 1 024 hot keys under the `emp:cache` namespace.【F:src/data_foundation/cache/redis_cache.py†L120-L208】
- Cache telemetry publishes `hits`, `misses`, `evictions`, `expirations`, and `invalidations` via the `telemetry.cache` topic so CI and operators can monitor cache health without shell access.【F:src/trading/monitoring/portfolio_monitor.py†L90-L180】【F:tests/current/test_portfolio_monitor_runtime.py†L1-L120】

## Dependencies & collaboration points

- Runtime refactor work (Initiative 5) will supply the dependency-injected builder needed to swap data services cleanly.【F:docs/technical_debt_assessment.md†L36-L76】【F:docs/roadmap.md†L180-L260】
- Compliance and risk briefs should align on audit storage, retention policies, and data lineage once Timescale schemas are defined.【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L893-L925】【F:docs/roadmap.md†L68-L80】
- Formatter/regression initiatives must keep ingest modules stable while migrations land; coordinate freeze windows per the rollout plan.【F:docs/technical_debt_assessment.md†L16-L33】

## Open questions

1. Which managed TimescaleDB tier (self-hosted vs. cloud service) best matches the €12 000 professional budget while satisfying replication needs?【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L569-L600】
2. How will we seed historical macro/economic series into Timescale without breaching provider rate limits or licensing constraints?【F:docs/EMP_ENCYCLOPEDIA_v2.3_CANONICAL.md†L1010-L1034】
3. What rollback procedure should operators follow if Kafka/Spark infrastructure is unavailable but Timescale must remain authoritative?【F:docs/roadmap.md†L22-L80】

Keep this brief updated as prototypes land, tests expand, and operators rehearse the institutional tier workflows.
