#!/usr/bin/env python3
# EMP Proving Ground - Unified v2.0
# Single-file implementation for collaborative audit and development.

import sys
import numpy as np
import pandas as pd
import random
import copy
import time
import logging
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any, Callable
from dataclasses import dataclass, field
from enum import Enum

import scipy.stats as stats
from scipy.signal import find_peaks
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.exceptions import NotFittedError
from decimal import Decimal, getcontext, ROUND_HALF_UP
from pydantic import BaseModel, Field, validator
import warnings

# Configure global decimal precision for financial calculations
getcontext().prec = 12
getcontext().rounding = ROUND_HALF_UP

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# ==============================================================================
# ### Component: Data Pipeline ###
# (Content from storage.py, clean.py, ingest.py)
# ==============================================================================

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s:%(lineno)d - %(message)s",
    stream=sys.stdout,
)
logger = logging.getLogger(__name__)

# ==============================================================================
# ### Part 0: Foundational Principles & Dependencies ###
# ==============================================================================

class RiskConfig(BaseModel):
    """Configuration for risk management"""
    max_risk_per_trade_pct: Decimal = Field(default=Decimal('0.02'), description="Maximum risk per trade as percentage of equity")
    max_leverage: Decimal = Field(default=Decimal('10.0'), description="Maximum allowed leverage")
    max_total_exposure_pct: Decimal = Field(default=Decimal('0.5'), description="Maximum total exposure as percentage of equity")
    max_drawdown_pct: Decimal = Field(default=Decimal('0.25'), description="Maximum drawdown before stopping")
    min_position_size: int = Field(default=1000, description="Minimum position size in units")
    max_position_size: int = Field(default=1000000, description="Maximum position size in units")
    mandatory_stop_loss: bool = Field(default=True, description="Whether stop loss is mandatory")
    research_mode: bool = Field(default=False, description="Research mode disables some safety checks")
    
    @validator('max_risk_per_trade_pct', 'max_leverage', 'max_total_exposure_pct', 'max_drawdown_pct')
    def validate_percentages(cls, v):
        if v <= 0 or v > 1:
            raise ValueError(f"Percentage must be between 0 and 1, got {v}")
        return v

@dataclass
class Instrument:
    """Instrument metadata for financial calculations"""
    symbol: str
    pip_decimal_places: int
    contract_size: Decimal
    long_swap_rate: Decimal
    short_swap_rate: Decimal
    margin_currency: str
    swap_time: str = "22:00"  # Default swap time UTC
    
    def __post_init__(self):
        if self.pip_decimal_places < 0:
            raise ValueError("pip_decimal_places must be non-negative")
        if self.contract_size <= 0:
            raise ValueError("contract_size must be positive")

class InstrumentProvider:
    """Provides instrument metadata"""
    
    def __init__(self, instruments_file: str = "configs/instruments.json"):
        self.instruments_file = Path(instruments_file)
        self.instruments: Dict[str, Instrument] = {}
        self._load_instruments()
    
    def _load_instruments(self):
        """Load instrument definitions from JSON file"""
        if not self.instruments_file.exists():
            # Create default instruments
            self._create_default_instruments()
        else:
            with open(self.instruments_file, 'r') as f:
                data = json.load(f)
                for symbol, config in data.items():
                    self.instruments[symbol] = Instrument(
                        symbol=symbol,
                        pip_decimal_places=config['pip_decimal_places'],
                        contract_size=Decimal(str(config['contract_size'])),
                        long_swap_rate=Decimal(str(config['long_swap_rate'])),
                        short_swap_rate=Decimal(str(config['short_swap_rate'])),
                        margin_currency=config['margin_currency'],
                        swap_time=config.get('swap_time', '22:00')
                    )
    
    def _create_default_instruments(self):
        """Create default instrument definitions"""
        default_instruments = {
            'EUR_USD': {
                'pip_decimal_places': 4,
                'contract_size': '100000',
                'long_swap_rate': '-0.0001',
                'short_swap_rate': '0.0001',
                'margin_currency': 'USD',
                'swap_time': '22:00'
            },
            'GBP_USD': {
                'pip_decimal_places': 4,
                'contract_size': '100000',
                'long_swap_rate': '-0.0002',
                'short_swap_rate': '0.0002',
                'margin_currency': 'USD',
                'swap_time': '22:00'
            },
            'USD_JPY': {
                'pip_decimal_places': 2,
                'contract_size': '100000',
                'long_swap_rate': '-0.0001',
                'short_swap_rate': '0.0001',
                'margin_currency': 'USD',
                'swap_time': '22:00'
            }
        }
        
        # Save default instruments
        self.instruments_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.instruments_file, 'w') as f:
            json.dump(default_instruments, f, indent=2)
        
        # Load them
        self._load_instruments()
    
    def get_instrument(self, symbol: str) -> Optional[Instrument]:
        """Get instrument by symbol"""
        return self.instruments.get(symbol)
    
    def list_instruments(self) -> List[str]:
        """List all available instruments"""
        return list(self.instruments.keys())

class CurrencyConverter:
    """Handles currency conversions for pip value calculations"""
    
    def __init__(self, rates_file: str = "configs/exchange_rates.json"):
        self.rates_file = Path(rates_file)
        self.rates: Dict[str, Decimal] = {}
        self._load_rates()
    
    def _load_rates(self):
        """Load exchange rates from JSON file"""
        if not self.rates_file.exists():
            # Create default rates
            self._create_default_rates()
        else:
            with open(self.rates_file, 'r') as f:
                data = json.load(f)
                self.rates = {k: Decimal(str(v)) for k, v in data.items()}
    
    def _create_default_rates(self):
        """Create default exchange rates"""
        default_rates = {
            'EUR_USD': '1.1000',
            'GBP_USD': '1.2500',
            'USD_JPY': '110.00',
            'USD_EUR': '0.9091',
            'USD_GBP': '0.8000',
            'JPY_USD': '0.0091'
        }
        
        # Save default rates
        self.rates_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.rates_file, 'w') as f:
            json.dump(default_rates, f, indent=2)
        
        # Load them
        self._load_rates()
    
    def get_rate(self, from_currency: str, to_currency: str) -> Decimal:
        """Get exchange rate between currencies"""
        if from_currency == to_currency:
            return Decimal('1.0')
        
        # Try direct rate
        pair = f"{from_currency}_{to_currency}"
        if pair in self.rates:
            return self.rates[pair]
        
        # Try inverse rate
        inverse_pair = f"{to_currency}_{from_currency}"
        if inverse_pair in self.rates:
            return Decimal('1.0') / self.rates[inverse_pair]
        
        # If not found, return 1.0 (assume same currency)
        logger.warning(f"Exchange rate not found for {from_currency} to {to_currency}, using 1.0")
        return Decimal('1.0')
    
    def calculate_pip_value(self, instrument: Instrument, account_currency: str) -> Decimal:
        """Calculate pip value in account currency"""
        # Get the quote currency from the symbol
        base_currency, quote_currency = instrument.symbol.split('_')
        
        # Calculate pip value in quote currency
        pip_value_quote = Decimal('0.0001') if instrument.pip_decimal_places == 4 else Decimal('0.01')
        pip_value_quote *= instrument.contract_size
        
        # Convert to account currency
        if quote_currency != account_currency:
            rate = self.get_rate(quote_currency, account_currency)
            pip_value_account = pip_value_quote * rate
        else:
            pip_value_account = pip_value_quote
        
        return pip_value_account


# ==============================================================================
# ### Component: Data Pipeline ###
# (Content from storage.py, clean.py, ingest.py)
# ==============================================================================

class TickDataStorage:
    """
    Manages storage and retrieval of historical tick data.
    
    - Stores data in Parquet format for efficiency.
    - Implements caching for frequently accessed data.
    - Provides OHLCV aggregation on the fly.
    """
    
    def __init__(self, data_dir: str = "data"):
        """
        Initialize tick data storage
        
        Args:
            data_dir: Directory to store tick data
        """
        self.data_dir = Path(data_dir)
        self.processed_dir = self.data_dir / "processed"
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        # In-memory cache for tick data
        self.cache: Dict[str, pd.DataFrame] = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "cached_datasets": 0
        }
        
        logger.info(f"Initialized tick data storage: {data_dir}")

    def load_tick_data(self, symbol: str, start_time: datetime, 
                       end_time: datetime) -> pd.DataFrame:
        """
        Load tick data for a given symbol and time range
        
        Args:
            symbol: Trading symbol (e.g., EURUSD)
            start_time: Start of time range
            end_time: End of time range
        
        Returns:
            DataFrame with tick data
        """
        
        cache_key = f"{symbol}_{start_time.isoformat()}_{end_time.isoformat()}"
        
        if cache_key in self.cache:
            self.cache_stats["hits"] += 1
            logger.debug(f"Cache hit for {cache_key}")
            return self.cache[cache_key].copy()
        
        self.cache_stats["misses"] += 1
        
        # Load data from Parquet files (simplified for demo)
        # In real implementation, would load multiple files and filter
        file_path = self.processed_dir / symbol / f"{symbol}_{start_time.year}_{start_time.month:02d}.parquet"
        
        if not file_path.exists():
            logger.warning(f"No data file found for {symbol} {start_time.year}")
            return pd.DataFrame()
        
        df = pd.read_parquet(file_path)
        
        # Ensure df is a DataFrame
        if not isinstance(df, pd.DataFrame):
            df = pd.DataFrame(df)
        
        # Filter by time range
        df = df[(df["timestamp"] >= start_time) & (df["timestamp"] <= end_time)]
        
        # Add to cache
        df_to_cache = df if isinstance(df, pd.DataFrame) else pd.DataFrame(df)
        self._add_to_cache(cache_key, df_to_cache)
        
        logger.info(f"Loaded {len(df):,} ticks for {symbol} ({start_time.date()} to {end_time.date()})")
        return df.copy() if isinstance(df, pd.DataFrame) else pd.DataFrame(df)

    def get_ohlcv(self, symbol: str, start_time: datetime, 
                  end_time: datetime, freq: str = "M1") -> pd.DataFrame:
        """
        Get OHLCV data for a given symbol and time range
        
        Args:
            symbol: Trading symbol
            start_time: Start of time range
            end_time: End of time range
            freq: Timeframe (e.g., M1, M5, H1)
        
        Returns:
            DataFrame with OHLCV data
        """
        
        tick_data = self.load_tick_data(symbol, start_time, end_time)
        
        if tick_data.empty:
            return pd.DataFrame()
        
        return self._ticks_to_ohlcv(tick_data, freq)

    def _ticks_to_ohlcv(self, tick_data: pd.DataFrame, freq: str) -> pd.DataFrame:
        """
        Convert tick data to OHLCV format
        
        Args:
            tick_data: DataFrame with tick data
            freq: Timeframe string (e.g., M1, H1)
        
        Returns:
            DataFrame with OHLCV data
        """
        
        if tick_data.empty:
            return pd.DataFrame()
        
        # Ensure timestamp is index
        tick_data = tick_data.set_index("timestamp")
        
        # Resample to OHLCV
        ohlcv = tick_data.groupby("symbol").resample(freq).agg({
            "bid": "ohlc",
            "ask": "ohlc",
            "bid_volume": "sum",
            "ask_volume": "sum"
        })
        
        if ohlcv.empty:
            return pd.DataFrame()
        
        # Flatten multi-index columns
        ohlcv.columns = [f"{col[0]}_{col[1]}" for col in ohlcv.columns]
        
        # Calculate OHLC from bid/ask
        ohlcv["open"] = (ohlcv["bid_open"] + ohlcv["ask_open"]) / 2
        ohlcv["high"] = ohlcv["ask_high"]
        ohlcv["low"] = ohlcv["bid_low"]
        ohlcv["close"] = (ohlcv["bid_close"] + ohlcv["ask_close"]) / 2
        ohlcv["volume"] = ohlcv["bid_volume"] + ohlcv["ask_volume"]
        
        # Drop intermediate columns
        ohlcv = ohlcv[["open", "high", "low", "close", "volume"]]
        
        # Ensure ohlcv is a DataFrame before calling reset_index
        if isinstance(ohlcv, pd.DataFrame):
            return ohlcv.reset_index()
        else:
            return pd.DataFrame(ohlcv).reset_index()

    def _add_to_cache(self, key: str, df: pd.DataFrame):
        """
        Add data to cache with size management (simplified)
        """
        
        # Simple cache size limit
        if len(self.cache) > 20:  # Max 20 datasets in cache
            # Remove oldest item
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            self.cache_stats["cached_datasets"] -= 1
        
        self.cache[key] = df
        self.cache_stats["cached_datasets"] += 1
        logger.debug(f"Added {key} to cache ({len(df)} ticks)")

    def get_cache_stats(self) -> Dict:
        """Get cache performance statistics"""
        return self.cache_stats


class TickDataCleaner:
    """
    Cleans and validates raw tick data.
    
    - Removes duplicates and outliers.
    - Handles missing data.
    - Validates data integrity.
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize data cleaner
        
        Args:
            config: Configuration dictionary
        """
        
        default_config = {
            "max_spread_bps": 50.0,  # Max spread in basis points
            "max_price_deviation_std": 10.0, # Max deviation from rolling mean
            "rolling_window_size": 100
        }
        
        self.config = {**default_config, **(config or {})}
        logger.info("Initialized tick data cleaner")

    def clean(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """
        Clean and validate tick data
        
        Args:
            df: Raw tick data DataFrame
            symbol: Trading symbol
        
        Returns:
            Cleaned tick data DataFrame
        """
        
        if df.empty:
            return df
        
        initial_rows = len(df)
        
        # 1. Remove duplicates
        df = df.drop_duplicates(subset=["timestamp"])
        
        # 2. Sort by timestamp
        df = df.sort_values("timestamp").reset_index(drop=True)
        
        # 3. Handle missing values (forward fill)
        df = df.ffill()
        
        # 4. Validate spread
        df["spread_bps"] = ((df["ask"] - df["bid"]) / df["bid"]) * 10000
        df_filtered = df[df["spread_bps"] <= self.config["max_spread_bps"]]
        
        # 5. Remove price outliers
        df_filtered["mid_price"] = (df_filtered["bid"] + df_filtered["ask"]) / 2
        
        # Ensure df_filtered is a DataFrame before calling rolling
        if isinstance(df_filtered, pd.DataFrame):
            rolling_mean = df_filtered["mid_price"].rolling(window=self.config["rolling_window_size"]).mean()
            rolling_std = df_filtered["mid_price"].rolling(window=self.config["rolling_window_size"]).std()
            
            lower_bound = rolling_mean - self.config["max_price_deviation_std"] * rolling_std
            upper_bound = rolling_mean + self.config["max_price_deviation_std"] * rolling_std
            
            df_filtered = df_filtered[(df_filtered["mid_price"] >= lower_bound) & (df_filtered["mid_price"] <= upper_bound)]
            
            # Final cleanup
            df_filtered = df_filtered.drop(columns=["spread_bps", "mid_price"])
            df_filtered = df_filtered.dropna()
        else:
            # Convert to DataFrame if it's not already
            df_filtered = pd.DataFrame(df_filtered)
        
        return df_filtered if isinstance(df_filtered, pd.DataFrame) else pd.DataFrame(df_filtered)
        
        # Final cleanup
        df = df.drop(columns=["spread_bps", "mid_price"])
        df = df.dropna()
        
        final_rows = len(df)
        removed_rows = initial_rows - final_rows
        
        logger.info(f"Cleaned {symbol} data: {initial_rows} -> {final_rows} rows ({removed_rows} removed)")
        
        return df


class DukascopyIngestor:
    """
    Ingests historical tick data from Dukascopy using real data-sourcing.
    
    Implements real-world data pipeline with regime-aware processing.
    """
    
    def __init__(self, storage: TickDataStorage, cleaner: TickDataCleaner):
        """
        Initialize Dukascopy ingestor
        
        Args:
            storage: TickDataStorage instance
            cleaner: TickDataCleaner instance
        """
        self.storage = storage
        self.cleaner = cleaner
        logger.info("Initialized Dukascopy ingestor (real data pipeline)")

    def ingest_year(self, symbol: str, year: int) -> bool:
        """
        Ingest a full year of data for a symbol using real data pipeline
        
        Args:
            symbol: Trading symbol
            year: Year to ingest
        
        Returns:
            True if successful, False otherwise
        """
        
        logger.info(f"Ingesting {symbol} data for {year} using real data pipeline...")
        
        try:
            # Create partitioned directory structure
            year_dir = self.storage.processed_dir / symbol / str(year)
            year_dir.mkdir(parents=True, exist_ok=True)
            
            # Download and process monthly data
            monthly_data = []
            for month in range(1, 13):
                start_date = datetime(year, month, 1)
                end_date = (start_date + timedelta(days=31)).replace(day=1) - timedelta(days=1)
                
                # Download real data for the month
                raw_ticks = self._download_real_data(symbol, start_date, end_date)
                if raw_ticks is not None and not raw_ticks.empty:
                    monthly_data.append(raw_ticks)
                    
                    # Save monthly partition
                    month_file = year_dir / f"{month:02d}.parquet"
                    raw_ticks.to_parquet(month_file, compression="snappy", index=False)
                    logger.debug(f"Saved {len(raw_ticks):,} ticks to {month_file}")
            
            if not monthly_data:
                logger.error(f"No data downloaded for {symbol} {year}")
                return False
            
            # Combine all monthly data
            logger.info("Combining monthly data...")
            combined_data = pd.concat(monthly_data, ignore_index=True)
            combined_data = combined_data.sort_values("timestamp").reset_index(drop=True)
            
            # Clean and validate data
            logger.info("Cleaning and validating data...")
            cleaned_data = self.cleaner.clean(combined_data, symbol)
            
            # Save to parquet with partitioned structure
            output_file = year_dir / "full_year.parquet"
            logger.info(f"Saving to {output_file}")
            cleaned_data.to_parquet(
                output_file,
                compression="snappy",
                index=False
            )
            
            logger.info(f"Successfully ingested {len(cleaned_data):,} ticks for {symbol} {year}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to ingest {symbol} {year}: {e}")
            return False

    def _download_real_data(self, symbol: str, start_time: datetime, end_time: datetime) -> Optional[pd.DataFrame]:
        """
        Download real historical tick data from Dukascopy
        
        Args:
            symbol: Trading symbol
            start_time: Start of time range
            end_time: End of time range
        
        Returns:
            DataFrame with real tick data or None if failed
        """
        
        try:
            # Import duka library for real data download
            try:
                # Note: duka library needs to be installed separately
                # pip install duka
                import duka  # type: ignore
                from duka import download_ticks  # type: ignore
            except ImportError:
                logger.warning("Duka library not available, using fallback data generation")
                return self._generate_fallback_data(symbol, start_time, end_time)
            
            # Download ticks using duka
            logger.info(f"Downloading {symbol} data from {start_time.date()} to {end_time.date()}")
            
            # Create temporary directory for download
            import tempfile
            import os
            
            with tempfile.TemporaryDirectory() as temp_dir:
                # Download ticks to temporary directory
                download_ticks(symbol, start_time, end_time, temp_dir)
                
                # Read downloaded data
                tick_files = []
                for file in os.listdir(temp_dir):
                    if file.endswith('.csv'):
                        file_path = os.path.join(temp_dir, file)
                        df = pd.read_csv(file_path)
                        tick_files.append(df)
                
                if not tick_files:
                    logger.warning(f"No tick files found for {symbol}")
                    return self._generate_fallback_data(symbol, start_time, end_time)
                
                # Combine all tick files
                combined_df = pd.concat(tick_files, ignore_index=True)
                
                # Standardize column names
                column_mapping = {
                    'timestamp': 'timestamp',
                    'bid': 'bid',
                    'ask': 'ask',
                    'bid_volume': 'bid_volume',
                    'ask_volume': 'ask_volume'
                }
                
                # Rename columns if needed
                for old_col, new_col in column_mapping.items():
                    if old_col in combined_df.columns and new_col not in combined_df.columns:
                        combined_df = combined_df.rename(columns={old_col: new_col})
                
                # Add symbol column if not present
                if 'symbol' not in combined_df.columns:
                    combined_df['symbol'] = symbol
                
                # Convert timestamp to datetime
                if 'timestamp' in combined_df.columns:
                    combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])
                
                # Filter by time range
                combined_df = combined_df[
                    (combined_df['timestamp'] >= start_time) & 
                    (combined_df['timestamp'] <= end_time)
                ]
                
                logger.info(f"Downloaded {len(combined_df):,} real ticks for {symbol}")
                return combined_df if isinstance(combined_df, pd.DataFrame) else None
                
        except Exception as e:
            logger.error(f"Real data download failed for {symbol}: {e}")
            logger.info("Falling back to realistic synthetic data")
            return self._generate_fallback_data(symbol, start_time, end_time)

    def _generate_fallback_data(self, symbol: str, start_time: datetime, end_time: datetime) -> pd.DataFrame:
        """
        Generate realistic fallback data when real data is unavailable
        
        Args:
            symbol: Trading symbol
            start_time: Start of time range
            end_time: End of time range
        
        Returns:
            DataFrame with realistic synthetic tick data
        """
        
        # Calculate realistic number of ticks (1 tick per second during trading hours)
        trading_hours_per_day = 24  # Forex trades 24/5
        days_in_period = (end_time - start_time).days
        total_trading_seconds = trading_hours_per_day * 3600 * days_in_period
        num_ticks = min(total_trading_seconds, 1000000)  # Cap at 1M ticks
        
        # Generate timestamps
        timestamps = pd.to_datetime(np.linspace(start_time.timestamp(), end_time.timestamp(), num_ticks), unit="s")
        
        # Set realistic base prices and volatility based on symbol
        current_year = start_time.year
        if symbol == "EURUSD":
            base_price = 1.08 + (current_year - 2020) * 0.02  # Realistic price evolution
            volatility = 0.0001
        elif symbol == "GBPUSD":
            base_price = 1.25 + (current_year - 2020) * 0.01
            volatility = 0.00012
        elif symbol == "XAUUSD":
            base_price = 2300.0 + (current_year - 2020) * 50
            volatility = 0.5
        else:
            base_price = 1.0
            volatility = 0.0001
        
        # Generate realistic price walk with mean reversion
        price_changes = np.random.normal(0, volatility, num_ticks)
        
        # Add realistic market patterns
        # Trend component
        trend = np.linspace(0, np.random.normal(0, volatility * 100), num_ticks)
        # Mean reversion component
        mean_reversion = np.zeros(num_ticks)
        for i in range(1, num_ticks):
            mean_reversion[i] = -0.1 * (price_changes[i-1] - np.mean(price_changes[:i]))
        
        # Combine components
        price_changes = price_changes + trend + mean_reversion
        prices = base_price + np.cumsum(price_changes)
        
        # Generate realistic spreads
        if symbol in ["EURUSD", "GBPUSD"]:
            spread_range = (0.00015, 0.0003)  # 1.5-3 pips
        elif symbol == "XAUUSD":
            spread_range = (0.3, 0.8)  # 30-80 cents
        else:
            spread_range = (0.00015, 0.0003)
        
        # Vary spreads based on market conditions
        spreads = np.random.uniform(spread_range[0], spread_range[1], num_ticks)
        # Wider spreads during low liquidity hours - simplified approach
        # Use random variation instead of time-based spread adjustment
        spread_multiplier = np.random.uniform(1.0, 1.5, num_ticks)
        spreads *= spread_multiplier
        
        bids = prices - spreads / 2
        asks = prices + spreads / 2
        
        # Generate realistic volumes with intraday patterns
        base_volume = 100
        # Higher volume during major session overlaps - simplified approach
        volume_multiplier = np.ones(num_ticks)
        # Randomly increase volume for some periods to simulate session overlaps
        overlap_periods = np.random.choice([True, False], num_ticks, p=[0.3, 0.7])
        volume_multiplier[overlap_periods] = 2.0
        
        bid_volumes = base_volume * volume_multiplier * np.random.uniform(0.5, 1.5, num_ticks)
        ask_volumes = base_volume * volume_multiplier * np.random.uniform(0.5, 1.5, num_ticks)
        
        # Create DataFrame
        data = pd.DataFrame({
            "timestamp": timestamps,
            "symbol": symbol,
            "bid": bids,
            "ask": asks,
            "bid_volume": bid_volumes,
            "ask_volume": ask_volumes
        })
        
        logger.info(f"Generated {len(data):,} realistic fallback ticks for {symbol}")
        return data


class MarketRegimeIdentifier:
    """
    Identifies market regimes by analyzing historical data patterns.
    
    Scans the entire downloaded historical dataset to identify three distinct regimes:
    1. Trending: Strong directional movement with clear trends
    2. Ranging: Sideways consolidation with frequent reversals  
    3. Volatile: High volatility crisis periods with extreme moves
    """
    
    def __init__(self, data_storage: TickDataStorage):
        """
        Initialize market regime identifier
        
        Args:
            data_storage: TickDataStorage instance for data access
        """
        self.data_storage = data_storage
        self.regime_config_file = Path("regimes.json")
        logger.info("Initialized market regime identifier")

    def identify_regimes(self, symbol: str, start_year: int, end_year: int) -> Dict[str, Dict]:
        """
        Identify market regimes by analyzing historical data
        
        Args:
            symbol: Trading symbol to analyze
            start_year: Start year for analysis
            end_year: End year for analysis
        
        Returns:
            Dictionary with regime configurations
        """
        
        logger.info(f"Identifying market regimes for {symbol} ({start_year}-{end_year})...")
        
        # Load and analyze historical data
        all_data = []
        for year in range(start_year, end_year + 1):
            try:
                # Try to load from partitioned structure first
                year_dir = self.data_storage.processed_dir / symbol / str(year)
                if (year_dir / "full_year.parquet").exists():
                    year_data = pd.read_parquet(year_dir / "full_year.parquet")
                else:
                    # Fallback to old structure
                    year_file = self.data_storage.processed_dir / f"{symbol}_{year}.parquet"
                    if year_file.exists():
                        year_data = pd.read_parquet(year_file)
                    else:
                        logger.warning(f"No data found for {symbol} {year}")
                        continue
                
                all_data.append(year_data)
                logger.debug(f"Loaded {len(year_data):,} ticks for {year}")
                
            except Exception as e:
                logger.error(f"Failed to load data for {symbol} {year}: {e}")
                continue
        
        if not all_data:
            logger.error(f"No data available for regime identification")
            return {}
        
        # Combine all data
        combined_data = pd.concat(all_data, ignore_index=True)
        combined_data = combined_data.sort_values("timestamp").reset_index(drop=True)
        
        logger.info(f"Analyzing {len(combined_data):,} total ticks for regime identification")
        
        # Convert to OHLCV for regime analysis
        ohlcv_data = self._convert_to_ohlcv(combined_data)
        
        # Identify regimes
        regimes = self._analyze_regimes(ohlcv_data, symbol)
        
        # Save regime configuration
        self._save_regime_config(regimes)
        
        logger.info(f"Identified {len(regimes)} market regimes")
        return regimes

    def _convert_to_ohlcv(self, tick_data: pd.DataFrame) -> pd.DataFrame:
        """
        Convert tick data to OHLCV for regime analysis
        
        Args:
            tick_data: Raw tick data
        
        Returns:
            OHLCV data with technical indicators
        """
        
        if tick_data.empty:
            return pd.DataFrame()
        
        # Calculate mid price if not available
        if "mid_price" not in tick_data.columns:
            tick_data["mid_price"] = (tick_data["bid"] + tick_data["ask"]) / 2
        
        # Resample to daily OHLCV for regime analysis
        tick_data = tick_data.set_index("timestamp")
        ohlcv = tick_data["mid_price"].resample("D").ohlc()
        # Add volume column safely using assign method
        volume_data = tick_data["bid_volume"].resample("D").sum() + tick_data["ask_volume"].resample("D").sum()
        ohlcv = ohlcv.assign(volume=volume_data)
        
        # Add technical indicators
        if isinstance(ohlcv, pd.DataFrame):
            ohlcv = self._add_regime_indicators(ohlcv)
        else:
            # Convert to DataFrame if needed
            ohlcv = pd.DataFrame(ohlcv)
            ohlcv = self._add_regime_indicators(ohlcv)
        
        return ohlcv.reset_index()

    def _add_regime_indicators(self, ohlcv: pd.DataFrame) -> pd.DataFrame:
        """
        Add technical indicators for regime analysis
        
        Args:
            ohlcv: OHLCV data
        
        Returns:
            OHLCV data with indicators
        """
        
        if len(ohlcv) < 50:
            return ohlcv
        
        # Moving averages
        ohlcv["sma_20"] = ohlcv["close"].rolling(window=20).mean()
        ohlcv["sma_50"] = ohlcv["close"].rolling(window=50).mean()
        ohlcv["sma_200"] = ohlcv["close"].rolling(window=200).mean()
        
        # Volatility indicators
        ohlcv["atr"] = self._calculate_atr(ohlcv)
        ohlcv["volatility"] = ohlcv["close"].rolling(window=20).std()
        
        # Trend indicators
        ohlcv["trend_strength"] = abs(ohlcv["sma_20"] - ohlcv["sma_50"]) / ohlcv["sma_50"]
        
        # Mean reversion indicators
        ohlcv["price_position"] = (ohlcv["close"] - ohlcv["low"]) / (ohlcv["high"] - ohlcv["low"])
        ohlcv["mean_reversion_score"] = abs(ohlcv["price_position"] - 0.5) * 2
        
        # Volume indicators
        ohlcv["volume_sma"] = ohlcv["volume"].rolling(window=20).mean()
        ohlcv["volume_ratio"] = ohlcv["volume"] / ohlcv["volume_sma"]
        
        return ohlcv

    def _calculate_atr(self, ohlcv: pd.DataFrame, period: int = 14) -> pd.Series:
        """
        Calculate Average True Range
        
        Args:
            ohlcv: OHLCV data
            period: ATR period
        
        Returns:
            ATR series
        """
        
        if len(ohlcv) < period + 1:
            return pd.Series(index=ohlcv.index)
        
        high = ohlcv["high"]
        low = ohlcv["low"]
        close = ohlcv["close"]
        
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        
        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = true_range.rolling(window=period).mean()
        
        # Ensure we return a Series
        if isinstance(atr, pd.Series):
            return atr
        else:
            return pd.Series(atr, index=ohlcv.index)

    def _analyze_regimes(self, ohlcv: pd.DataFrame, symbol: str) -> Dict[str, Dict]:
        """
        Analyze OHLCV data to identify market regimes
        
        Args:
            ohlcv: OHLCV data with indicators
            symbol: Trading symbol
        
        Returns:
            Dictionary with regime configurations
        """
        
        if ohlcv.empty or len(ohlcv) < 100:
            logger.warning("Insufficient data for regime analysis")
            return {}
        
        # Remove NaN values
        ohlcv = ohlcv.dropna()
        
        # Calculate regime scores for each period
        regime_scores = []
        
        for i in range(50, len(ohlcv)):  # Start after enough data for indicators
            window = ohlcv.iloc[i-50:i+1]
            
            # Calculate regime characteristics
            trending_score = self._calculate_trending_score(window)
            ranging_score = self._calculate_ranging_score(window)
            volatile_score = self._calculate_volatile_score(window)
            
            regime_scores.append({
                "timestamp": ohlcv.iloc[i]["timestamp"],
                "trending": trending_score,
                "ranging": ranging_score,
                "volatile": volatile_score,
                "dominant_regime": max(["trending", "ranging", "volatile"], 
                                     key=lambda x: locals()[f"{x}_score"])
            })
        
        # Identify regime periods
        regimes = self._identify_regime_periods(regime_scores, ohlcv)
        
        return regimes

    def _calculate_trending_score(self, window: pd.DataFrame) -> float:
        """
        Calculate trending regime score
        
        Args:
            window: Data window for analysis
        
        Returns:
            Trending score (0-1)
        """
        
        if len(window) < 20:
            return 0.0
        
        # Trend strength
        trend_strength = window["trend_strength"].mean()
        
        # Directional consistency
        price_changes = window["close"].diff().dropna()
        positive_changes = (price_changes > 0).sum()
        negative_changes = (price_changes < 0).sum()
        direction_consistency = max(positive_changes, negative_changes) / len(price_changes)
        
        # Moving average alignment
        sma_alignment = 0.0
        if "sma_20" in window.columns and "sma_50" in window.columns:
            aligned_days = ((window["sma_20"] > window["sma_50"]) | (window["sma_20"] < window["sma_50"])).sum()
            sma_alignment = aligned_days / len(window)
        
        # Combine scores
        trending_score = (trend_strength * 0.4 + 
                         direction_consistency * 0.4 + 
                         sma_alignment * 0.2)
        
        return min(trending_score, 1.0)

    def _calculate_ranging_score(self, window: pd.DataFrame) -> float:
        """
        Calculate ranging regime score
        
        Args:
            window: Data window for analysis
        
        Returns:
            Ranging score (0-1)
        """
        
        if len(window) < 20:
            return 0.0
        
        # Low trend strength
        trend_strength = window["trend_strength"].mean()
        low_trend_score = 1.0 - min(trend_strength * 10, 1.0)
        
        # High mean reversion
        mean_reversion = window["mean_reversion_score"].mean()
        
        # Price range consistency
        price_range = (window["high"].max() - window["low"].min()) / window["close"].mean()
        range_consistency = 1.0 - min(price_range * 100, 1.0)
        
        # Volume consistency
        volume_consistency = 1.0 - window["volume_ratio"].std()
        volume_consistency = max(0.0, volume_consistency)
        
        # Combine scores
        ranging_score = (low_trend_score * 0.3 + 
                        mean_reversion * 0.3 + 
                        range_consistency * 0.2 + 
                        volume_consistency * 0.2)
        
        return min(ranging_score, 1.0)

    def _calculate_volatile_score(self, window: pd.DataFrame) -> float:
        """
        Calculate volatile regime score
        
        Args:
            window: Data window for analysis
        
        Returns:
            Volatile score (0-1)
        """
        
        if len(window) < 20:
            return 0.0
        
        # High volatility
        volatility = window["volatility"].mean()
        volatility_score = min(volatility * 1000, 1.0)
        
        # High ATR
        atr_score = 0.0
        if "atr" in window.columns:
            atr = window["atr"].mean()
            atr_score = min(atr * 100, 1.0)
        
        # Large price swings
        price_swings = window["high"] - window["low"]
        swing_score = min(price_swings.mean() * 100, 1.0)
        
        # Volume spikes
        volume_spikes = (window["volume"] > window["volume_sma"] * 1.5).sum()
        volume_score = volume_spikes / len(window)
        
        # Combine scores
        volatile_score = (volatility_score * 0.3 + 
                         atr_score * 0.3 + 
                         swing_score * 0.2 + 
                         volume_score * 0.2)
        
        return min(volatile_score, 1.0)

    def _identify_regime_periods(self, regime_scores: List[Dict], ohlcv: pd.DataFrame) -> Dict[str, Dict]:
        """
        Identify distinct regime periods from regime scores
        
        Args:
            regime_scores: List of regime scores
            ohlcv: Original OHLCV data
        
        Returns:
            Dictionary with regime configurations
        """
        
        if not regime_scores:
            return {}
        
        # Find periods with dominant regimes
        regime_periods = []
        current_regime = regime_scores[0]["dominant_regime"]
        start_idx = 0
        
        for i, score in enumerate(regime_scores):
            if score["dominant_regime"] != current_regime:
                # Regime change detected
                if i - start_idx >= 30:  # Minimum 30 days for a regime
                    regime_periods.append({
                        "regime": current_regime,
                        "start_idx": start_idx,
                        "end_idx": i - 1,
                        "start_time": regime_scores[start_idx]["timestamp"],
                        "end_time": regime_scores[i-1]["timestamp"],
                        "avg_score": np.mean([s[current_regime] for s in regime_scores[start_idx:i]])
                    })
                
                current_regime = score["dominant_regime"]
                start_idx = i
        
        # Add final period
        if len(regime_scores) - start_idx >= 30:
            regime_periods.append({
                "regime": current_regime,
                "start_idx": start_idx,
                "end_idx": len(regime_scores) - 1,
                "start_time": regime_scores[start_idx]["timestamp"],
                "end_time": regime_scores[-1]["timestamp"],
                "avg_score": np.mean([s[current_regime] for s in regime_scores[start_idx:]])
            })
        
        # Select best periods for each regime type
        regimes = {}
        
        for regime_type in ["trending", "ranging", "volatile"]:
            regime_periods_filtered = [p for p in regime_periods if p["regime"] == regime_type]
            
            if regime_periods_filtered:
                # Select period with highest average score
                best_period = max(regime_periods_filtered, key=lambda x: x["avg_score"])
                
                regimes[regime_type] = {
                    "name": f"{regime_type.capitalize()} Period",
                    "start_time": best_period["start_time"],
                    "end_time": best_period["end_time"],
                    "description": self._get_regime_description(regime_type),
                    "characteristics": self._get_regime_characteristics(regime_type),
                    "avg_score": best_period["avg_score"],
                    "duration_days": (best_period["end_time"] - best_period["start_time"]).days
                }
            else:
                # Create synthetic regime if none found
                logger.warning(f"No {regime_type} regime found, creating synthetic period")
                regimes[regime_type] = self._create_synthetic_regime(regime_type, ohlcv)
        
        return regimes

    def _get_regime_description(self, regime_type: str) -> str:
        """Get description for regime type"""
        
        descriptions = {
            "trending": "Strong directional movement with clear trends and momentum",
            "ranging": "Sideways consolidation with frequent reversals and mean reversion",
            "volatile": "High volatility crisis period with extreme moves and uncertainty"
        }
        
        return descriptions.get(regime_type, "Unknown regime type")

    def _get_regime_characteristics(self, regime_type: str) -> List[str]:
        """Get characteristics for regime type"""
        
        characteristics = {
            "trending": ["high_directionality", "low_reversals", "consistent_momentum", "trend_following"],
            "ranging": ["low_directionality", "high_reversals", "mean_reversion", "range_bound"],
            "volatile": ["high_volatility", "extreme_moves", "crisis_conditions", "uncertainty"]
        }
        
        return characteristics.get(regime_type, [])

    def _create_synthetic_regime(self, regime_type: str, ohlcv: pd.DataFrame) -> Dict:
        """Create synthetic regime when none is found in data"""
        
        # Use middle portion of data
        mid_point = len(ohlcv) // 2
        start_time = ohlcv.iloc[mid_point]["timestamp"]
        end_time = ohlcv.iloc[min(mid_point + 90, len(ohlcv) - 1)]["timestamp"]
        
        return {
            "name": f"Synthetic {regime_type.capitalize()} Period",
            "start_time": start_time,
            "end_time": end_time,
            "description": f"Synthetic {regime_type} regime (no natural period found)",
            "characteristics": self._get_regime_characteristics(regime_type),
            "avg_score": 0.5,
            "duration_days": 90
        }

    def _save_regime_config(self, regimes: Dict[str, Dict]):
        """
        Save regime configuration to JSON file
        
        Args:
            regimes: Regime configurations
        """
        
        try:
            # Convert datetime objects to strings for JSON serialization
            config = {}
            for regime_name, regime_data in regimes.items():
                config[regime_name] = {
                    "name": regime_data["name"],
                    "start_time": regime_data["start_time"].isoformat(),
                    "end_time": regime_data["end_time"].isoformat(),
                    "description": regime_data["description"],
                    "characteristics": regime_data["characteristics"],
                    "avg_score": regime_data["avg_score"],
                    "duration_days": regime_data["duration_days"]
                }
            
            with open(self.regime_config_file, "w") as f:
                json.dump(config, f, indent=2)
            
            logger.info(f"Regime configuration saved to {self.regime_config_file}")
            
        except Exception as e:
            logger.error(f"Failed to save regime configuration: {e}")

    def load_regime_config(self) -> Dict[str, Dict]:
        """
        Load regime configuration from JSON file
        
        Returns:
            Regime configurations
        """
        
        try:
            if not self.regime_config_file.exists():
                logger.warning("No regime configuration file found")
                return {}
            
            with open(self.regime_config_file, "r") as f:
                config = json.load(f)
            
            # Convert string timestamps back to datetime
            for regime_name, regime_data in config.items():
                regime_data["start_time"] = datetime.fromisoformat(regime_data["start_time"])
                regime_data["end_time"] = datetime.fromisoformat(regime_data["end_time"])
            
            logger.info(f"Loaded regime configuration from {self.regime_config_file}")
            return config
            
        except Exception as e:
            logger.error(f"Failed to load regime configuration: {e}")
            return {}


# ==============================================================================
# ### Component: Market Environment ###
# (Content from simulator.py, adversary.py, execution.py)
# ==============================================================================

# Enums for trading operations
class OrderSide(Enum):
    BUY = "buy"
    SELL = "sell"


class OrderType(Enum):
    MARKET = "market"
    LIMIT = "limit"
    STOP = "stop"


class OrderStatus(Enum):
    PENDING = "pending"
    FILLED = "filled"
    CANCELLED = "cancelled"
    REJECTED = "rejected"


@dataclass
class Order:
    """Represents a trading order"""
    order_id: str
    symbol: str
    side: OrderSide
    order_type: OrderType
    quantity: float
    price: Optional[float] = None
    timestamp: Optional[datetime] = None
    status: OrderStatus = OrderStatus.PENDING
    filled_quantity: float = 0.0
    filled_price: Optional[float] = None
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None


@dataclass
class Position:
    """Represents a trading position"""
    symbol: str
    quantity: float  # Positive for long, negative for short
    avg_price: float
    unrealized_pnl: float = 0.0
    realized_pnl: float = 0.0


# ==============================================================================
# ### Part 1: Risk Management Core (Enhanced) ###
# ==============================================================================

@dataclass
class TradeRecord:
    """Immutable record of a trade transaction for audit trail"""
    timestamp: datetime
    trade_type: str  # 'OPEN', 'ADD', 'REDUCE', 'CLOSE', 'REVERSE'
    quantity: int
    price: Decimal
    commission: Decimal
    slippage: Decimal
    swap_fee: Decimal = Decimal('0')
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class EnhancedPosition:
    """Enhanced position with v2.0 features"""
    symbol: str
    quantity: int  # Positive for long, negative for short
    avg_price: Decimal
    entry_timestamp: datetime
    last_swap_time: datetime
    unrealized_pnl: Decimal = Decimal('0')
    realized_pnl: Decimal = Decimal('0')
    max_adverse_excursion: Decimal = Decimal('0')
    max_favorable_excursion: Decimal = Decimal('0')
    trade_history: List[TradeRecord] = field(default_factory=list)
    
    def update(self, trade_quantity: int, trade_price: Decimal, 
               commission: Decimal, slippage: Decimal, 
               current_time: datetime, trade_type: str = "UNKNOWN") -> None:
        """Update position with new trade"""
        
        # Create trade record
        trade_record = TradeRecord(
            timestamp=current_time,
            trade_type=trade_type,
            quantity=trade_quantity,
            price=trade_price,
            commission=commission,
            slippage=slippage
        )
        self.trade_history.append(trade_record)
        
        # Calculate new position
        old_quantity = self.quantity
        old_avg_price = self.avg_price
        
        if trade_type in ["OPEN", "ADD"]:
            # Opening or adding to position
            if old_quantity == 0:
                # Opening new position
                self.quantity = trade_quantity
                self.avg_price = trade_price
                self.entry_timestamp = current_time
            else:
                # Adding to existing position
                total_quantity = old_quantity + trade_quantity
                self.avg_price = ((old_quantity * old_avg_price) + (trade_quantity * trade_price)) / total_quantity
                self.quantity = total_quantity
                
        elif trade_type in ["REDUCE", "CLOSE"]:
            # Reducing or closing position
            if abs(trade_quantity) > abs(old_quantity):
                raise ValueError(f"Cannot close more than current position: {trade_quantity} vs {old_quantity}")
            
            # Calculate realized PnL
            if trade_type == "CLOSE":
                # Full close
                pnl = (trade_price - old_avg_price) * old_quantity
                if old_quantity < 0:  # Short position
                    pnl = -pnl
                self.realized_pnl += pnl
                self.quantity = 0
            else:
                # Partial close
                pnl = (trade_price - old_avg_price) * trade_quantity
                if old_quantity < 0:  # Short position
                    pnl = -pnl
                self.realized_pnl += pnl
                self.quantity = old_quantity - trade_quantity
                
        elif trade_type == "REVERSE":
            # Reverse position (close old and open new)
            # First close existing position
            if old_quantity != 0:
                pnl = (trade_price - old_avg_price) * old_quantity
                if old_quantity < 0:  # Short position
                    pnl = -pnl
                self.realized_pnl += pnl
            
            # Then open new position
            self.quantity = trade_quantity
            self.avg_price = trade_price
            self.entry_timestamp = current_time
    
    def update_unrealized_pnl(self, current_market_price: Decimal) -> None:
        """Update unrealized PnL and track MAE/MFE"""
        if self.quantity == 0:
            self.unrealized_pnl = Decimal('0')
            return
        
        # Calculate unrealized PnL
        pnl = (current_market_price - self.avg_price) * self.quantity
        if self.quantity < 0:  # Short position
            pnl = -pnl
        
        self.unrealized_pnl = pnl
        
        # Update MAE/MFE
        if pnl < self.max_adverse_excursion:
            self.max_adverse_excursion = pnl
        if pnl > self.max_favorable_excursion:
            self.max_favorable_excursion = pnl
    
    def apply_swap_fee(self, current_time: datetime, instrument: Instrument) -> None:
        """Apply swap fee if past swap time"""
        if self.quantity == 0:
            return
        
        # Parse swap time
        swap_hour, swap_minute = map(int, instrument.swap_time.split(':'))
        swap_time = current_time.replace(hour=swap_hour, minute=swap_minute, second=0, microsecond=0)
        
        # Check if we're past swap time and it's a new day
        if (current_time >= swap_time and 
            current_time.date() > self.last_swap_time.date()):
            
            # Apply appropriate swap rate
            if self.quantity > 0:  # Long position
                swap_fee = instrument.long_swap_rate * abs(self.quantity)
            else:  # Short position
                swap_fee = instrument.short_swap_rate * abs(self.quantity)
            
            # Add to trade history
            swap_record = TradeRecord(
                timestamp=current_time,
                trade_type="SWAP",
                quantity=0,
                price=Decimal('0'),
                commission=Decimal('0'),
                slippage=Decimal('0'),
                swap_fee=swap_fee
            )
            self.trade_history.append(swap_record)
            
            # Update realized PnL
            self.realized_pnl -= swap_fee
            
            # Update last swap time
            self.last_swap_time = current_time

@dataclass
class ValidationResult:
    """Result of risk validation"""
    is_valid: bool
    reason: str
    risk_metadata: Optional[Dict[str, Any]] = None

class RiskManager:
    """Enhanced risk management core"""
    
    def __init__(self, config: RiskConfig, instrument_provider: InstrumentProvider):
        self.config = config
        self.instrument_provider = instrument_provider
        self.currency_converter = CurrencyConverter()
        
        logger.info(f"Initialized RiskManager with config: {config}")
    
    def calculate_position_size(self, account_equity: Decimal, stop_loss_pips: Decimal, 
                               instrument: Instrument, account_currency: str = "USD") -> int:
        """Calculate position size based on risk parameters"""
        
        # Validate inputs
        if account_equity <= 0:
            raise ValueError(f"Account equity must be positive, got {account_equity}")
        if stop_loss_pips <= 0:
            raise ValueError(f"Stop loss pips must be positive, got {stop_loss_pips}")
        
        # Calculate pip value
        pip_value = self.currency_converter.calculate_pip_value(instrument, account_currency)
        
        # Calculate risk amount
        risk_amount = account_equity * self.config.max_risk_per_trade_pct
        
        # Calculate stop loss value
        stop_loss_value = stop_loss_pips * pip_value
        
        # Calculate position size
        if stop_loss_value == 0:
            logger.warning("Stop loss value is zero, returning 0 position size")
            return 0
        
        size_in_lots = risk_amount / stop_loss_value
        size_in_units = int(size_in_lots * instrument.contract_size)
        
        # Apply size constraints
        size_in_units = max(self.config.min_position_size, 
                           min(self.config.max_position_size, size_in_units))
        
        logger.debug(f"Calculated position size: {size_in_units} units "
                    f"(risk: {risk_amount}, stop_loss: {stop_loss_value})")
        
        return size_in_units
    
    def validate_order(self, proposed_order: Order, account_state: Dict, 
                      open_positions: Dict[str, EnhancedPosition]) -> ValidationResult:
        """Validate order against risk rules"""
        
        # Check 1: Max Drawdown (Master circuit breaker)
        if "max_drawdown_pct" in account_state:
            if account_state["max_drawdown_pct"] > self.config.max_drawdown_pct:
                return ValidationResult(
                    is_valid=False,
                    reason=f"Max drawdown exceeded: {account_state['max_drawdown_pct']:.2%} > {self.config.max_drawdown_pct:.2%}"
                )
        
        # Check 2: Mandatory Stop Loss
        if self.config.mandatory_stop_loss and not self.config.research_mode:
            if not hasattr(proposed_order, 'stop_loss') or proposed_order.stop_loss is None:
                return ValidationResult(
                    is_valid=False,
                    reason="Stop loss is mandatory but not provided"
                )
        
        # Check 3: Max Leverage
        total_notional = sum(abs(pos.quantity) * pos.avg_price 
                           for pos in open_positions.values())
        if total_notional > 0:
            leverage = total_notional / account_state.get("equity", 1)
            if leverage > self.config.max_leverage:
                return ValidationResult(
                    is_valid=False,
                    reason=f"Max leverage exceeded: {leverage:.2f} > {self.config.max_leverage}"
                )
        
        # Check 4: Max Total Exposure
        total_exposure = sum(abs(pos.quantity) * pos.avg_price 
                           for pos in open_positions.values())
        exposure_pct = total_exposure / account_state.get("equity", 1)
        if exposure_pct > self.config.max_total_exposure_pct:
            return ValidationResult(
                is_valid=False,
                reason=f"Max total exposure exceeded: {exposure_pct:.2%} > {self.config.max_total_exposure_pct:.2%}"
            )
        
        # Check 5: Min/Max Position Size
        if proposed_order.quantity < self.config.min_position_size:
            return ValidationResult(
                is_valid=False,
                reason=f"Position size too small: {proposed_order.quantity} < {self.config.min_position_size}"
            )
        
        if proposed_order.quantity > self.config.max_position_size:
            return ValidationResult(
                is_valid=False,
                reason=f"Position size too large: {proposed_order.quantity} > {self.config.max_position_size}"
            )
        
        # All checks passed
        return ValidationResult(
            is_valid=True,
            reason="Order approved",
            risk_metadata={
                "leverage": leverage if total_notional > 0 else 0,
                "exposure_pct": exposure_pct,
                "total_notional": total_notional
            }
        )


@dataclass
class MarketState:
    """Current market state"""
    timestamp: datetime
    symbol: str
    bid: float
    ask: float
    bid_volume: float
    ask_volume: float
    spread_bps: float
    mid_price: float
    
    # Technical indicators (calculated on demand)
    atr: Optional[float] = None
    volatility: Optional[float] = None
    session: Optional[str] = None


class MarketSimulator:
    """
    Core market simulation engine that replays historical tick data
    and provides realistic execution environment for EMP organisms.
    """
    
    def __init__(self, data_storage: TickDataStorage, 
                 initial_balance: float = 100000.0,
                 leverage: float = 1.0):
        """
        Initialize market simulator
        
        Args:
            data_storage: TickDataStorage instance for data access
            initial_balance: Starting account balance
            leverage: Maximum leverage allowed
        """
        self.data_storage = data_storage
        self.initial_balance = initial_balance
        self.leverage = leverage
        
        # Simulation state
        self.current_tick_index = 0
        self.tick_data: Optional[pd.DataFrame] = None
        self.current_state: Optional[MarketState] = None
        
        # Account state
        self.balance = initial_balance
        self.equity = initial_balance
        self.margin_used = 0.0
        self.free_margin = initial_balance
        
        # Trading state
        self.orders: Dict[str, Order] = {}
        self.positions: Dict[str, Position] = {}
        self.trades: List[Dict] = []
        self.equity_curve: List[Tuple[datetime, float]] = []
        
        # Adversarial callbacks
        self.adversarial_callbacks: List[Callable] = []
        
        logger.info(f"Initialized market simulator with ${initial_balance:,.2f} balance")

    def load_data(self, symbol: str, start_time: datetime, end_time: datetime):
        """
        Load historical data for simulation
        
        Args:
            symbol: Trading symbol
            start_time: Start of simulation period
            end_time: End of simulation period
        """
        
        self.tick_data = self.data_storage.load_tick_data(symbol, start_time, end_time)
        
        if self.tick_data.empty:
            raise ValueError(f"No data available for {symbol} from {start_time} to {end_time}")
        
        # Prepare data for simulation
        self._prepare_data()
        
        # Reset simulation state
        self.current_tick_index = 0
        self.current_state = None
        
        logger.info(f"Loaded {len(self.tick_data):,} ticks for simulation")

    def _prepare_data(self):
        """Prepare loaded data for simulation"""
        
        if self.tick_data is None or self.tick_data.empty:
            return
        
        # Calculate spread in basis points
        self.tick_data["spread_bps"] = ((self.tick_data["ask"] - self.tick_data["bid"]) / 
                                       self.tick_data["bid"]) * 10000
        
        # Calculate mid price
        self.tick_data["mid_price"] = (self.tick_data["bid"] + self.tick_data["ask"]) / 2
        
        # Calculate rolling ATR (Average True Range)
        self._calculate_atr()
        
        # Determine trading sessions
        self._calculate_sessions()
        
        logger.info("Data preparation completed")

    def _calculate_atr(self, period: int = 14):
        """Calculate Average True Range"""
        
        if self.tick_data is None or len(self.tick_data) < period:
            if self.tick_data is not None:
                self.tick_data["atr"] = self.tick_data["spread_bps"] / 10000 * self.tick_data["mid_price"]
            return
        
        # Convert to OHLC for ATR calculation
        ohlc = self.data_storage._ticks_to_ohlcv(self.tick_data, "M1")
        
        if ohlc.empty:
            if self.tick_data is not None:
                self.tick_data["atr"] = self.tick_data["spread_bps"] / 10000 * self.tick_data["mid_price"]
            return
        
        # Calculate True Range
        ohlc["prev_close"] = ohlc["close"].shift(1)
        ohlc["tr1"] = ohlc["high"] - ohlc["low"]
        ohlc["tr2"] = abs(ohlc["high"] - ohlc["prev_close"])
        ohlc["tr3"] = abs(ohlc["low"] - ohlc["prev_close"])
        ohlc["true_range"] = ohlc[["tr1", "tr2", "tr3"]].max(axis=1)
        
        # Calculate ATR
        ohlc["atr"] = ohlc["true_range"].rolling(window=period).mean()
        
        # Map back to tick data (forward fill)
        if self.tick_data is not None:
            self.tick_data = self.tick_data.set_index("timestamp")
            ohlc = ohlc.set_index("timestamp")
            self.tick_data["atr"] = ohlc["atr"].reindex(self.tick_data.index, method="ffill")
            self.tick_data = self.tick_data.reset_index()

    def _calculate_sessions(self):
        """Determine trading sessions for each tick"""
        
        if self.tick_data is None:
            return
            
        def get_session(timestamp: datetime) -> str:
            hour = timestamp.hour
            
            # Trading sessions (UTC)
            if 0 <= hour < 7:
                return "ASIA"
            elif 7 <= hour < 15:
                return "LONDON"
            elif 15 <= hour < 22:
                return "NY"
            else:
                return "ASIA"
        
        self.tick_data["session"] = self.tick_data["timestamp"].apply(get_session)

    def step(self) -> Optional[MarketState]:
        """
        Advance simulation by one tick
        
        Returns:
            Current market state or None if simulation ended
        """
        
        if self.tick_data is None or self.current_tick_index >= len(self.tick_data):
            return None
        
        # Get current tick
        current_tick = self.tick_data.iloc[self.current_tick_index]
        
        # Create market state
        self.current_state = MarketState(
            timestamp=current_tick["timestamp"],
            symbol=current_tick["symbol"],
            bid=current_tick["bid"],
            ask=current_tick["ask"],
            bid_volume=current_tick["bid_volume"],
            ask_volume=current_tick["ask_volume"],
            spread_bps=current_tick["spread_bps"],
            mid_price=current_tick["mid_price"],
            atr=current_tick.get("atr"),
            session=current_tick.get("session")
        )
        
        # Apply adversarial modifications
        self._apply_adversarial_effects()
        
        # Process pending orders
        self._process_orders()
        
        # Update positions and account
        self._update_account()
        
        # Record equity curve
        self.equity_curve.append((self.current_state.timestamp, self.equity))
        
        # Advance to next tick
        self.current_tick_index += 1
        
        return self.current_state

    def _apply_adversarial_effects(self):
        """Apply adversarial modifications to current market state"""
        
        for callback in self.adversarial_callbacks:
            try:
                callback(self.current_state, self)
            except Exception as e:
                logger.error(f"Error in adversarial callback: {e}")

    def place_order(self, symbol: str, side: OrderSide, order_type: OrderType,
                   quantity: float, price: Optional[float] = None) -> str:
        """
        Place a trading order
        
        Args:
            symbol: Trading symbol
            side: Buy or sell
            order_type: Market, limit, or stop
            quantity: Order quantity
            price: Order price (for limit/stop orders)
        
        Returns:
            Order ID if successful, None if rejected
        """
        
        # Generate order ID
        order_id = f"order_{len(self.orders) + 1}"
        
        # Create order
        order = Order(
            order_id=order_id,
            symbol=symbol,
            side=side,
            order_type=order_type,
            quantity=quantity,
            price=price,
            timestamp=self.current_state.timestamp if self.current_state else datetime.now()
        )
        
        # Basic validation
        if not self._validate_order(order):
            order.status = OrderStatus.REJECTED
            logger.warning(f"Order rejected: {order_id}")
            return ""  # Return empty string instead of None
        
        # Add to pending orders
        self.orders[order_id] = order
        
        # Execute immediately if market order
        if order_type == OrderType.MARKET:
            self._execute_order(order_id)
        
        return order_id

    def _validate_order(self, order: Order) -> bool:
        """Validate order before placement"""
        
        # Check minimum quantity
        if order.quantity <= 0:
            return False
        
        # Check margin requirements (simplified)
        required_margin = order.quantity * 1000  # Simplified margin calculation
        if required_margin > self.free_margin:
            return False
        
        return True

    def _process_orders(self):
        """Process pending orders"""
        
        orders_to_remove = []
        
        for order_id, order in self.orders.items():
            if order.status != OrderStatus.PENDING:
                continue
            
            should_execute = False
            
            # Check execution conditions
            if order.order_type == OrderType.LIMIT:
                if (order.side == OrderSide.BUY and self.current_state and 
                    self.current_state.ask and order.price and self.current_state.ask <= order.price):
                    should_execute = True
                elif (order.side == OrderSide.SELL and self.current_state and 
                      self.current_state.bid and order.price and self.current_state.bid >= order.price):
                    should_execute = True
            
            elif order.order_type == OrderType.STOP:
                if (order.side == OrderSide.BUY and self.current_state and 
                    self.current_state.ask and order.price and self.current_state.ask >= order.price):
                    should_execute = True
                elif (order.side == OrderSide.SELL and self.current_state and 
                      self.current_state.bid and order.price and self.current_state.bid <= order.price):
                    should_execute = True
            
            if should_execute:
                self._execute_order(order_id)
                orders_to_remove.append(order_id)
        
        # Remove filled orders
        for order_id in orders_to_remove:
            if self.orders[order_id].status == "filled":
                del self.orders[order_id]

    def _execute_order(self, order_id: str):
        """Execute a specific order"""
        
        order = self.orders[order_id]
        
        if not self.current_state:
            order.status = OrderStatus.REJECTED
            return
        
        # Determine execution price
        if order.side == OrderSide.BUY:
            execution_price = self.current_state.ask
        else:
            execution_price = self.current_state.bid
        
        # Apply slippage and spread effects
        execution_price = self._apply_execution_effects(order, execution_price)
        
        # Update order
        order.filled_quantity = order.quantity
        order.filled_price = execution_price
        order.status = OrderStatus.FILLED
        
        # Update position
        self._update_position(order)
        
        # Record trade
        self._record_trade(order)
        
        logger.info(f"Order executed: {order_id} at {execution_price:.5f}")

    def _apply_execution_effects(self, order: Order, base_price: float) -> float:
        """Apply realistic execution effects (slippage, spread)"""
        
        # Simple slippage model
        slippage_bps = np.random.normal(0, 0.5)  # 0.5 bps average slippage
        slippage = base_price * slippage_bps / 10000
        
        # Apply slippage in unfavorable direction
        if order.side == OrderSide.BUY:
            execution_price = base_price + abs(slippage)
        else:
            execution_price = base_price - abs(slippage)
        
        return execution_price

    def _update_position(self, order: Order):
        """Update position based on executed order"""
        
        symbol = order.symbol
        
        if symbol not in self.positions:
            self.positions[symbol] = Position(
                symbol=symbol,
                quantity=0.0,
                avg_price=0.0
            )
        
        position = self.positions[symbol]
        
        # Calculate new position
        if order.side == OrderSide.BUY:
            new_quantity = position.quantity + order.filled_quantity
        else:
            new_quantity = position.quantity - order.filled_quantity
        
        # Update average price
        if new_quantity != 0 and order.filled_price is not None:
            total_cost = (position.quantity * position.avg_price + 
                         order.filled_quantity * order.filled_price)
            position.avg_price = total_cost / abs(new_quantity)
        
        position.quantity = new_quantity
        
        # Remove position if closed
        if abs(position.quantity) < 0.001:
            del self.positions[symbol]

    def _record_trade(self, order: Order):
        """Record completed trade"""
        
        trade = {
            "timestamp": order.timestamp,
            "symbol": order.symbol,
            "side": order.side.value,
            "quantity": order.filled_quantity,
            "price": order.filled_price,
            "order_id": order.order_id
        }
        
        self.trades.append(trade)

    def _update_account(self):
        """Update account equity and margin"""
        
        # Calculate unrealized PnL
        total_unrealized_pnl = 0.0
        
        for position in self.positions.values():
            if self.current_state and position.symbol == self.current_state.symbol:
                current_price = self.current_state.mid_price
                
                if position.quantity > 0:  # Long position
                    unrealized_pnl = position.quantity * (current_price - position.avg_price)
                else:  # Short position
                    unrealized_pnl = position.quantity * (position.avg_price - current_price)
                
                position.unrealized_pnl = unrealized_pnl
                total_unrealized_pnl += unrealized_pnl
        
        # Update equity
        self.equity = self.balance + total_unrealized_pnl
        
        # Update margin (simplified)
        self.margin_used = sum(abs(pos.quantity) * 1000 for pos in self.positions.values())
        self.free_margin = self.equity - self.margin_used

    def add_adversarial_callback(self, callback: Callable):
        """Add adversarial effect callback"""
        self.adversarial_callbacks.append(callback)

    def get_account_summary(self) -> Dict:
        """Get current account summary"""
        return {
            "balance": self.balance,
            "equity": self.equity,
            "margin_used": self.margin_used,
            "free_margin": self.free_margin,
            "positions": len(self.positions),
            "pending_orders": len([o for o in self.orders.values() if o.status == OrderStatus.PENDING])
        }

    def get_performance_stats(self) -> Dict:
        """Calculate performance statistics"""
        
        if not self.trades:
            return {
                "total_trades": 0,
                "total_return": 0.0,
                "win_rate": 0.0,
                "sharpe_ratio": 0.0
            }
        
        # Calculate returns
        total_return = (self.equity - self.initial_balance) / self.initial_balance
        
        # Calculate win rate (simplified)
        profitable_trades = 0
        for i, trade in enumerate(self.trades[1:], 1):
            prev_trade = self.trades[i-1]
            if trade["price"] > prev_trade["price"]:  # Simplified profit calculation
                profitable_trades += 1
        
        win_rate = profitable_trades / len(self.trades) if len(self.trades) > 1 else 0.0
        
        # Calculate Sharpe ratio (simplified)
        if len(self.equity_curve) > 1:
            returns = []
            for i in range(1, len(self.equity_curve)):
                ret = (self.equity_curve[i][1] - self.equity_curve[i-1][1]) / self.equity_curve[i-1][1]
                returns.append(ret)
            
            if returns and np.std(returns) > 0:
                sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252)  # Annualized
            else:
                sharpe_ratio = 0.0
        else:
            sharpe_ratio = 0.0
        
        return {
            "total_trades": len(self.trades),
            "total_return": total_return,
            "win_rate": win_rate,
            "sharpe_ratio": sharpe_ratio
        }


# Adversarial Engine Classes and Enums
class AdversarialEventType(Enum):
    SPOOFING = "spoofing"
    STOP_HUNT = "stop_hunt"
    NEWS_SHOCK = "news_shock"
    FLASH_CRASH = "flash_crash"
    LIQUIDITY_CRUNCH = "liquidity_crunch"


@dataclass
class AdversarialEvent:
    """Represents an adversarial market event"""
    event_type: AdversarialEventType
    timestamp: datetime
    duration: timedelta
    intensity: float  # 0.0 to 1.0
    parameters: Dict[str, Any] = field(default_factory=dict)
    active: bool = False


class AdversarialEngine:
    """
    v2.0: Intelligent adversarial engine that simulates realistic market manipulation
    with context-aware stop hunting and sophisticated spoofing tactics.
    """
    
    def __init__(self, difficulty_level: float = 0.5, seed: Optional[int] = None):
        """
        Initialize adversarial engine
        
        Args:
            difficulty_level: Overall difficulty (0.0 to 1.0)
            seed: Random seed for reproducibility
        """
        
        self.difficulty_level = difficulty_level
        self.random = random.Random(seed)
        
        # Configuration based on difficulty
        self.config = self._create_config(difficulty_level)
        
        # Active events
        self.active_events: List[AdversarialEvent] = []
        self.event_history: List[AdversarialEvent] = []
        
        # Ghost orders for spoofing
        self.ghost_orders: List[Dict] = []
        
        # v2.0: Liquidity zone tracking for intelligent stop hunting
        self.liquidity_zones: List[Dict] = []
        self.ohlcv_lookback: Optional[pd.DataFrame] = None
        self.lookback_window = 100  # M15 bars for liquidity analysis
        
        # v2.0: Breakout trap tracking
        self.consolidation_periods: List[Dict] = []
        self.breakout_traps: List[Dict] = []
        
        logger.info(f"Initialized v2.0 adversarial engine (difficulty: {difficulty_level:.1f})")

    def _create_config(self, difficulty: float) -> Dict:
        """Create configuration based on difficulty level"""
        
        base_config = {
            "spoofing_probability": 0.001,
            "stop_hunt_probability": 0.0005,
            "news_shock_probability": 0.0001,
            "ghost_order_lifetime": 300,  # seconds
            "max_slippage_bps": 2.0,
            "spread_widening_factor": 1.5,
            # v2.0: Intelligent manipulation parameters
            "liquidity_zone_detection": True,
            "breakout_trap_probability": 0.002,
            "consolidation_threshold": 0.3,  # ATR ratio for consolidation detection
            "liquidity_score_multiplier": 2.0
        }
        
        # Scale probabilities by difficulty
        for key in ["spoofing_probability", "stop_hunt_probability", "news_shock_probability", "breakout_trap_probability"]:
            base_config[key] *= (1 + difficulty * 2)  # Up to 3x at max difficulty
        
        base_config["max_slippage_bps"] *= (1 + difficulty)
        base_config["spread_widening_factor"] += difficulty * 0.5
        
        return base_config

    def _update_liquidity_zones(self, market_state: MarketState, simulator: MarketSimulator):
        """
        v2.0: Update liquidity zones using intelligent peak/trough detection
        
        Args:
            market_state: Current market state
            simulator: Market simulator instance
        """
        
        # Get recent OHLCV data for analysis
        if simulator.tick_data is None or len(simulator.tick_data) < 50:
            return
        
        # Convert to M15 OHLCV for swing analysis
        recent_data = simulator.tick_data.tail(self.lookback_window * 15)  # 15 ticks per M15 bar
        if len(recent_data) < 100:
            return
        
        # Create OHLCV data
        ohlcv = simulator.data_storage._ticks_to_ohlcv(recent_data, "M15")
        if ohlcv.empty or len(ohlcv) < 20:
            return
        
        self.ohlcv_lookback = ohlcv
        
        # Detect swing highs and lows using scipy.signal.find_peaks
        highs, _ = find_peaks(ohlcv["high"].values, height=np.percentile(ohlcv["high"], 70))
        lows, _ = find_peaks(-np.array(ohlcv["low"].values), height=-np.percentile(ohlcv["low"], 30))
        
        # Create liquidity zones from swing points
        self.liquidity_zones = []
        
        # Process swing highs (resistance zones)
        for idx in highs:
            if idx < len(ohlcv):
                price_level = ohlcv.iloc[idx]["high"]
                timestamp = ohlcv.iloc[idx]["timestamp"]
                
                # Calculate zone score based on confluence factors
                confluence_score = self._calculate_liquidity_confluence(ohlcv, idx, price_level, "high")
                
                zone = {
                    "type": "resistance",
                    "price_level": price_level,
                    "timestamp": timestamp,
                    "confluence_score": confluence_score,
                    "touches": 1,
                    "last_touch": timestamp
                }
                self.liquidity_zones.append(zone)
        
        # Process swing lows (support zones)
        for idx in lows:
            if idx < len(ohlcv):
                price_level = ohlcv.iloc[idx]["low"]
                timestamp = ohlcv.iloc[idx]["timestamp"]
                
                # Calculate zone score based on confluence factors
                confluence_score = self._calculate_liquidity_confluence(ohlcv, idx, price_level, "low")
                
                zone = {
                    "type": "support",
                    "price_level": price_level,
                    "timestamp": timestamp,
                    "confluence_score": confluence_score,
                    "touches": 1,
                    "last_touch": timestamp
                }
                self.liquidity_zones.append(zone)
        
        # Merge nearby zones (within 0.1% of each other)
        self._merge_nearby_zones()
        
        # Update existing zones with current price proximity
        current_price = market_state.mid_price
        for zone in self.liquidity_zones:
            proximity = abs(current_price - zone["price_level"]) / current_price
            zone["proximity"] = proximity

    def _calculate_liquidity_confluence(self, ohlcv: pd.DataFrame, idx: int, 
                                      price_level: float, zone_type: str) -> float:
        """
        Calculate confluence score for a liquidity zone
        
        Args:
            ohlcv: OHLCV data
            idx: Index of the swing point
            price_level: Price level of the zone
            zone_type: "high" or "low"
        
        Returns:
            Confluence score (0.0 to 1.0)
        """
        
        confluence_factors = []
        
        # 1. Round number proximity (major psychological levels)
        round_numbers = [1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000, 1.6000, 1.7000, 1.8000, 1.9000]
        for round_num in round_numbers:
            proximity = 1.0 - min(abs(price_level - round_num) / round_num, 1.0)
            if proximity > 0.8:  # Within 20% of round number
                confluence_factors.append(proximity * 0.3)
                break
        
        # 2. Volume confirmation
        if idx < len(ohlcv):
            volume = ohlcv.iloc[idx]["volume"]
            avg_volume = ohlcv["volume"].mean()
            volume_factor = min(volume / avg_volume, 3.0) / 3.0
            confluence_factors.append(volume_factor * 0.2)
        
        # 3. Recency factor (more recent = higher score)
        recency_factor = idx / len(ohlcv)
        confluence_factors.append(recency_factor * 0.2)
        
        # 4. Price level significance (distance from recent range)
        if len(ohlcv) > 20:
            recent_high = ohlcv["high"].tail(20).max()
            recent_low = ohlcv["low"].tail(20).min()
            recent_range = recent_high - recent_low
            
            if recent_range > 0:
                if zone_type == "high":
                    significance = (price_level - recent_low) / recent_range
                else:
                    significance = (recent_high - price_level) / recent_range
                
                confluence_factors.append(significance * 0.3)
        
        return min(sum(confluence_factors), 1.0) if confluence_factors else 0.0

    def _merge_nearby_zones(self):
        """Merge liquidity zones that are very close to each other"""
        
        if len(self.liquidity_zones) < 2:
            return
        
        merged_zones = []
        used_indices = set()
        
        for i, zone1 in enumerate(self.liquidity_zones):
            if i in used_indices:
                continue
            
            merged_zone = zone1.copy()
            used_indices.add(i)
            
            for j, zone2 in enumerate(self.liquidity_zones[i+1:], i+1):
                if j in used_indices:
                    continue
                
                # Check if zones are close (within 0.1%)
                price_diff = abs(zone1["price_level"] - zone2["price_level"]) / zone1["price_level"]
                
                if price_diff < 0.001 and zone1["type"] == zone2["type"]:
                    # Merge zones
                    merged_zone["touches"] += zone2["touches"]
                    merged_zone["confluence_score"] = max(merged_zone["confluence_score"], zone2["confluence_score"])
                    merged_zone["last_touch"] = max(merged_zone["last_touch"], zone2["last_touch"])
                    used_indices.add(j)
            
            merged_zones.append(merged_zone)
        
        self.liquidity_zones = merged_zones

    def _update_consolidation_detection(self, market_state: MarketState, simulator: MarketSimulator):
        """
        v2.0: Enhanced consolidation detection for intelligent breakout trap analysis
        
        Uses ATR-based volatility analysis, volume confirmation, and trend flatness
        to identify genuine consolidation periods suitable for breakout trap deployment.
        
        Args:
            market_state: Current market state
            simulator: Market simulator instance
        """
        
        if simulator.tick_data is None or len(simulator.tick_data) < 100:
            return
        
        # Get recent data for consolidation analysis (M15 timeframe)
        recent_data = simulator.tick_data.tail(300)  # Last 300 ticks (20 M15 bars)
        
        # Calculate ATR for volatility measurement
        if market_state.atr is None:
            return
        
        # Convert to M15 OHLCV for proper consolidation analysis
        ohlcv_15m = simulator.data_storage._ticks_to_ohlcv(recent_data, "M15")
        if ohlcv_15m.empty or len(ohlcv_15m) < 10:
            return
        
        # Calculate consolidation metrics
        price_range = ohlcv_15m["high"].max() - ohlcv_15m["low"].min()
        avg_atr = ohlcv_15m["close"].rolling(5).std().mean() * 2  # Approximate ATR
        
        # Enhanced consolidation criteria:
        # 1. Price range < 0.3 * ATR (tight range)
        # 2. Low volume volatility (consistent volume)
        # 3. Minimal directional movement (flat trend)
        
        consolidation_threshold = self.config["consolidation_threshold"]
        is_tight_range = price_range < (avg_atr * consolidation_threshold)
        
        # Volume consistency check
        volume_std = ohlcv_15m["volume"].std()
        volume_mean = ohlcv_15m["volume"].mean()
        is_volume_consistent = volume_std < (volume_mean * 0.5) if volume_mean > 0 else False
        
        # Trend flatness check (linear regression slope)
        if len(ohlcv_15m) >= 5:
            x = np.arange(len(ohlcv_15m))
            y = np.array(ohlcv_15m["close"].values)
            slope = np.polyfit(x, y, 1)[0]
            trend_strength = abs(slope) / ohlcv_15m["close"].mean()
            is_trend_flat = trend_strength < 0.001  # Very small slope
        else:
            is_trend_flat = True
        
        # All three conditions must be met for genuine consolidation
        is_consolidating = is_tight_range and is_volume_consistent and is_trend_flat
        
        if is_consolidating:
            # Enhanced consolidation period tracking
            consolidation = {
                "start_time": ohlcv_15m.iloc[0]["timestamp"],
                "end_time": ohlcv_15m.iloc[-1]["timestamp"],
                "high_boundary": ohlcv_15m["high"].max(),
                "low_boundary": ohlcv_15m["low"].min(),
                "mid_price": (ohlcv_15m["high"].max() + ohlcv_15m["low"].min()) / 2,
                "atr": avg_atr,
                "duration_bars": len(ohlcv_15m),
                "volume_consistency": 1.0 - (volume_std / volume_mean) if volume_mean > 0 else 0.0,
                "trend_flatness": 1.0 - min(trend_strength * 1000, 1.0) if 'trend_strength' in locals() else 1.0,
                "consolidation_score": (is_tight_range + is_volume_consistent + is_trend_flat) / 3.0
            }
            
            # Check if this is a new consolidation period (gap > 2 hours)
            if not self.consolidation_periods or \
               (ohlcv_15m.iloc[0]["timestamp"] - self.consolidation_periods[-1]["end_time"]).total_seconds() > 7200:
                self.consolidation_periods.append(consolidation)
                logger.debug(f"Enhanced consolidation detected: {consolidation['low_boundary']:.5f} - {consolidation['high_boundary']:.5f} "
                           f"(score: {consolidation['consolidation_score']:.2f})")
        
        # Clean old consolidation periods (older than 2 hours)
        current_time = market_state.timestamp
        self.consolidation_periods = [
            c for c in self.consolidation_periods 
            if (current_time - c["end_time"]).total_seconds() < 7200
        ]

    def apply_adversarial_effects(self, market_state: MarketState, simulator: MarketSimulator):
        """
        v2.0: Main entry point for applying intelligent adversarial effects
        
        Args:
            market_state: Current market state
            simulator: Market simulator instance
        """
        
        current_time = market_state.timestamp
        
        # v2.0: Update liquidity zones for intelligent stop hunting
        if self.config["liquidity_zone_detection"]:
            self._update_liquidity_zones(market_state, simulator)
        
        # v2.0: Update consolidation detection for breakout traps
        self._update_consolidation_detection(market_state, simulator)
        
        # Update active events
        self._update_active_events(current_time)
        
        # Check for new manipulation opportunities
        self._check_manipulation_triggers(market_state, simulator)
        
        # Apply active effects
        self._apply_active_effects(market_state, simulator)

    def _update_active_events(self, current_time: datetime):
        """Update status of active adversarial events"""
        
        for event in self.active_events:
            if not event.active and current_time >= event.timestamp:
                event.active = True
                logger.info(f"Adversarial event activated: {event.event_type.value}")
            
            elif event.active and current_time >= event.timestamp + event.duration:
                event.active = False
                logger.info(f"Adversarial event ended: {event.event_type.value}")

    def _check_manipulation_triggers(self, market_state: MarketState, 
                                   simulator: MarketSimulator):
        """v2.0: Check for intelligent manipulation opportunities"""
        
        # v2.0: Intelligent spoofing (breakout traps)
        if self._should_trigger_breakout_trap(market_state, simulator):
            self._trigger_breakout_trap(market_state, simulator)
        elif self.random.random() < self.config["spoofing_probability"]:
            self._trigger_spoofing(market_state)
        
        # v2.0: Intelligent stop hunt based on liquidity zones
        if self._should_trigger_intelligent_stop_hunt(market_state, simulator):
            self._trigger_intelligent_stop_hunt(market_state, simulator)
        
        # News shock opportunities
        if self._should_trigger_news_shock(market_state):
            self._trigger_news_shock(market_state)
        
        # Flash crash opportunities (rare but devastating)
        if self.random.random() < self.config["news_shock_probability"] * 0.1:
            self._trigger_flash_crash(market_state)

    def _should_trigger_breakout_trap(self, market_state: MarketState, 
                                    simulator: MarketSimulator) -> bool:
        """
        v2.0: Determine if conditions are right for a breakout trap
        
        Args:
            market_state: Current market state
            simulator: Market simulator instance
        
        Returns:
            True if breakout trap should be triggered
        """
        
        if not self.consolidation_periods:
            return False
        
        current_consolidation = self.consolidation_periods[-1]
        current_price = market_state.mid_price
        
        # Check if price is approaching consolidation boundaries
        consolidation_high = current_consolidation["high"]
        consolidation_low = current_consolidation["low"]
        
        # Calculate proximity to boundaries
        high_proximity = (consolidation_high - current_price) / current_price
        low_proximity = (current_price - consolidation_low) / current_price
        
        # Trigger if price is very close to breaking out
        breakout_threshold = 0.001  # 0.1% from boundary
        
        if high_proximity < breakout_threshold or low_proximity < breakout_threshold:
            # Check if there's been recent trading activity (potential breakout attempt)
            recent_trades = [t for t in simulator.trades 
                           if (market_state.timestamp - t["timestamp"]).total_seconds() < 300]
            
            if len(recent_trades) > 2:  # Multiple recent trades suggest breakout attempt
                return self.random.random() < self.config["breakout_trap_probability"]
        
        return False

    def _trigger_breakout_trap(self, market_state: MarketState, simulator: MarketSimulator):
        """
        v2.0: Trigger intelligent breakout trap manipulation
        
        Args:
            market_state: Current market state
            simulator: Market simulator instance
        """
        
        if not self.consolidation_periods:
            return
        
        current_consolidation = self.consolidation_periods[-1]
        current_price = market_state.mid_price
        
        # Determine breakout direction
        consolidation_high = current_consolidation["high"]
        consolidation_low = current_consolidation["low"]
        
        high_proximity = (consolidation_high - current_price) / current_price
        low_proximity = (current_price - consolidation_low) / current_price
        
        # Determine which boundary is closer
        if high_proximity < low_proximity:
            # Trap upward breakout
            trap_direction = "up"
            target_price = consolidation_high + current_consolidation["atr"] * 0.5
        else:
            # Trap downward breakout
            trap_direction = "down"
            target_price = consolidation_low - current_consolidation["atr"] * 0.5
        
        # Create ghost orders to induce breakout
        num_ghost_orders = self.random.randint(3, 8)
        ghost_volume_multiplier = self.random.uniform(5, 15)
        
        for i in range(num_ghost_orders):
            if trap_direction == "up":
                # Place large buy orders above consolidation
                ghost_price = consolidation_high + (i + 1) * current_consolidation["atr"] * 0.1
                ghost_volume = market_state.bid_volume * ghost_volume_multiplier
                side = "bid"
            else:
                # Place large sell orders below consolidation
                ghost_price = consolidation_low - (i + 1) * current_consolidation["atr"] * 0.1
                ghost_volume = market_state.ask_volume * ghost_volume_multiplier
                side = "ask"
            
            ghost_order = {
                "price": ghost_price,
                "volume": ghost_volume,
                "side": side,
                "expires_at": market_state.timestamp + timedelta(seconds=60),
                "created_at": market_state.timestamp,
                "trap_order": True
            }
            
            self.ghost_orders.append(ghost_order)
        
        # Create breakout trap event
        event = AdversarialEvent(
            event_type=AdversarialEventType.SPOOFING,
            timestamp=market_state.timestamp,
            duration=timedelta(seconds=self.random.randint(120, 300)),
            intensity=self.random.uniform(0.7, 1.0),
            parameters={
                "trap_type": "breakout",
                "direction": trap_direction,
                "target_price": target_price,
                "consolidation_high": consolidation_high,
                "consolidation_low": consolidation_low
            }
        )
        
        self.active_events.append(event)
        logger.info(f"Breakout trap triggered: {trap_direction} direction, target: {target_price:.5f}")

    def _should_trigger_intelligent_stop_hunt(self, market_state: MarketState, 
                                            simulator: MarketSimulator) -> bool:
        """
        v2.0: Determine if conditions are right for intelligent stop hunt
        
        Args:
            market_state: Current market state
            simulator: Market simulator instance
        
        Returns:
            True if intelligent stop hunt should be triggered
        """
        
        if not self.liquidity_zones:
            return False
        
        current_price = market_state.mid_price
        
        # Find nearby liquidity zones
        nearby_zones = []
        for zone in self.liquidity_zones:
            proximity = abs(current_price - zone["price_level"]) / current_price
            
            if proximity < 0.005:  # Within 0.5% of zone
                nearby_zones.append(zone)
        
        if not nearby_zones:
            return False
        
        # Calculate dynamic hunt probability based on zone confluence
        base_probability = self.config["stop_hunt_probability"]
        
        for zone in nearby_zones:
            # Higher confluence = higher hunt probability
            confluence_multiplier = 1 + zone["confluence_score"] * self.config["liquidity_score_multiplier"]
            
            # More touches = higher probability
            touch_multiplier = 1 + (zone["touches"] - 1) * 0.5
            
            # Market state multiplier (low volatility = higher probability)
            if market_state.atr:
                volatility_ratio = market_state.atr / current_price
                if volatility_ratio < 0.001:  # Low volatility
                    market_multiplier = 2.0
                elif volatility_ratio < 0.002:  # Medium volatility
                    market_multiplier = 1.5
                else:  # High volatility
                    market_multiplier = 1.0
            else:
                market_multiplier = 1.0
            
            # Calculate final probability
            hunt_probability = base_probability * confluence_multiplier * touch_multiplier * market_multiplier
            
            if self.random.random() < hunt_probability:
                return True
        
        return False

    def _trigger_intelligent_stop_hunt(self, market_state: MarketState, simulator: MarketSimulator):
        """
        v2.0: Trigger intelligent stop hunt based on liquidity zones
        
        Args:
            market_state: Current market state
            simulator: Market simulator instance
        """
        
        if not self.liquidity_zones:
            return
        
        current_price = market_state.mid_price
        
        # Find the most attractive liquidity zone
        best_zone = None
        best_score = 0.0
        
        for zone in self.liquidity_zones:
            proximity = abs(current_price - zone["price_level"]) / current_price
            
            if proximity < 0.005:  # Within 0.5% of zone
                # Calculate zone attractiveness score
                score = zone["confluence_score"] * zone["touches"] * (1 - proximity)
                
                if score > best_score:
                    best_score = score
                    best_zone = zone
        
        if not best_zone:
            return
        
        # Calculate hunt depth based on ATR
        hunt_depth = 0.3  # Default 0.3 * ATR
        if market_state.atr:
            hunt_depth = market_state.atr * 0.3
        
        # Determine hunt direction and target
        if best_zone["type"] == "resistance":
            # Hunt above resistance
            target_price = best_zone["price_level"] + hunt_depth
            direction = "up"
        else:
            # Hunt below support
            target_price = best_zone["price_level"] - hunt_depth
            direction = "down"
        
        # Calculate reversal probability based on HTF trend
        reversal_probability = 0.8  # Default high probability
        
        # In a real implementation, this would check higher timeframe trend
        # For now, use a simplified approach based on recent price action
        if simulator.tick_data is not None and len(simulator.tick_data) > 100:
            recent_data = simulator.tick_data.tail(100)
            recent_trend = (recent_data["mid_price"].iloc[-1] - recent_data["mid_price"].iloc[0]) / recent_data["mid_price"].iloc[0]
            
            # If hunt is against the trend, higher reversal probability
            if (direction == "up" and recent_trend < 0) or (direction == "down" and recent_trend > 0):
                reversal_probability = 0.8
            else:
                reversal_probability = 0.4
        
        # Create intelligent stop hunt event
        event = AdversarialEvent(
            event_type=AdversarialEventType.STOP_HUNT,
            timestamp=market_state.timestamp,
            duration=timedelta(seconds=self.random.randint(60, 180)),
            intensity=self.random.uniform(0.6, 0.9),
            parameters={
                "target_price": target_price,
                "direction": direction,
                "zone_type": best_zone["type"],
                "zone_confluence": best_zone["confluence_score"],
                "zone_touches": best_zone["touches"],
                "reversal_probability": reversal_probability,
                "hunt_depth": hunt_depth
            }
        )
        
        self.active_events.append(event)
        logger.info(f"Intelligent stop hunt triggered: {direction} to {target_price:.5f}, "
                   f"zone: {best_zone['type']} (confluence: {best_zone['confluence_score']:.2f})")

    def _trigger_spoofing(self, market_state: MarketState):
        """Trigger order book spoofing manipulation"""
        
        # Create ghost orders at multiple levels
        num_levels = self.random.randint(2, 5)
        side = self.random.choice(["bid", "ask"])
        
        base_price = market_state.bid if side == "bid" else market_state.ask
        spread = market_state.ask - market_state.bid
        
        for level in range(1, num_levels + 1):
            # Place ghost orders away from current price
            if side == "bid":
                ghost_price = base_price - spread * level * self.random.uniform(1.5, 3.0)
            else:
                ghost_price = base_price + spread * level * self.random.uniform(1.5, 3.0)
            
            # Large volume to create illusion of liquidity
            ghost_volume = market_state.bid_volume * self.random.uniform(5, 20)
            
            # Random lifetime
            lifetime_seconds = self.random.randint(30, int(self.config["ghost_order_lifetime"]))
            expires_at = market_state.timestamp + timedelta(seconds=lifetime_seconds)
            
            ghost_order = {
                "price": ghost_price,
                "volume": ghost_volume,
                "side": side,
                "expires_at": expires_at,
                "created_at": market_state.timestamp
            }
            
            self.ghost_orders.append(ghost_order)
        
        # Create spoofing event
        event = AdversarialEvent(
            event_type=AdversarialEventType.SPOOFING,
            timestamp=market_state.timestamp,
            duration=timedelta(seconds=self.random.randint(60, 300)),
            intensity=self.random.uniform(0.3, 0.8),
            parameters={"num_levels": num_levels, "side": side}
        )
        
        self.active_events.append(event)

    def _should_trigger_stop_hunt(self, market_state: MarketState, 
                                 simulator: MarketSimulator) -> bool:
        """Determine if conditions are right for a stop hunt"""
        
        # Check if there are pending stop orders
        stop_orders = [o for o in simulator.orders.values() 
                      if o.order_type == OrderType.STOP and o.status == OrderStatus.PENDING]
        
        if not stop_orders:
            return False
        
        # Random trigger based on probability
        return self.random.random() < self.config["stop_hunt_probability"]

    def _trigger_stop_hunt(self, market_state: MarketState, simulator: MarketSimulator):
        """Trigger stop hunt manipulation"""
        
        # Find nearby stop orders
        stop_orders = [o for o in simulator.orders.values() 
                      if o.order_type == OrderType.STOP and o.status == OrderStatus.PENDING]
        
        if not stop_orders:
            return
        
        # Target the closest stop order
        if market_state.mid_price is not None:
            target_order = min(stop_orders, 
                              key=lambda o: abs(o.price - market_state.mid_price) if o.price is not None else float('inf'))
        else:
            target_order = stop_orders[0] if stop_orders else None
        
        event = AdversarialEvent(
            event_type=AdversarialEventType.STOP_HUNT,
            timestamp=market_state.timestamp,
            duration=timedelta(seconds=self.random.randint(30, 120)),
            intensity=self.random.uniform(0.5, 0.9),
            parameters={"target_price": target_order.price, "direction": target_order.side.value}
        )
        
        self.active_events.append(event)

    def _should_trigger_news_shock(self, market_state: MarketState) -> bool:
        """Determine if conditions are right for a news shock"""
        
        # News shocks are rare but can happen anytime
        return self.random.random() < self.config["news_shock_probability"]

    def _trigger_news_shock(self, market_state: MarketState):
        """Trigger news shock event"""
        
        direction = self.random.choice(["bullish", "bearish"])
        intensity = self.random.uniform(0.6, 1.0)
        
        event = AdversarialEvent(
            event_type=AdversarialEventType.NEWS_SHOCK,
            timestamp=market_state.timestamp,
            duration=timedelta(seconds=self.random.randint(120, 600)),
            intensity=intensity,
            parameters={"direction": direction, "magnitude": intensity * 0.01}  # Up to 1% move
        )
        
        self.active_events.append(event)

    def _trigger_flash_crash(self, market_state: MarketState):
        """Trigger flash crash event (rare but severe)"""
        
        event = AdversarialEvent(
            event_type=AdversarialEventType.FLASH_CRASH,
            timestamp=market_state.timestamp,
            duration=timedelta(seconds=self.random.randint(60, 180)),
            intensity=self.random.uniform(0.8, 1.0),
            parameters={"magnitude": self.random.uniform(0.02, 0.05)}  # 2-5% crash
        )
        
        self.active_events.append(event)

    def _apply_active_effects(self, market_state: MarketState, simulator: MarketSimulator):
        """Apply effects of currently active adversarial events"""
        
        for event in self.active_events:
            if not event.active:
                continue
            
            if event.event_type == AdversarialEventType.SPOOFING:
                self._apply_spoofing_effects(market_state, event)
            
            elif event.event_type == AdversarialEventType.STOP_HUNT:
                self._apply_stop_hunt_effects(market_state, event)
            
            elif event.event_type == AdversarialEventType.NEWS_SHOCK:
                self._apply_news_shock_effects(market_state, event)
            
            elif event.event_type == AdversarialEventType.FLASH_CRASH:
                self._apply_flash_crash_effects(market_state, event)

    def _apply_spoofing_effects(self, market_state: MarketState, event: AdversarialEvent):
        """Apply spoofing effects to market state"""
        
        # Widen spreads due to fake liquidity
        spread_multiplier = 1 + event.intensity * 0.5
        current_spread = market_state.ask - market_state.bid
        new_spread = current_spread * spread_multiplier
        
        # Adjust bid/ask
        mid_price = market_state.mid_price
        market_state.bid = mid_price - new_spread / 2
        market_state.ask = mid_price + new_spread / 2
        
        # Reduce apparent volume (ghost orders disappear when approached)
        volume_reduction = event.intensity * 0.3
        market_state.bid_volume *= (1 - volume_reduction)
        market_state.ask_volume *= (1 - volume_reduction)

    def _apply_stop_hunt_effects(self, market_state: MarketState, event: AdversarialEvent):
        """Apply stop hunt effects to market state"""
        
        target_price = event.parameters["target_price"]
        direction = event.parameters["direction"]
        
        # Push price toward stop orders
        price_push = event.intensity * 0.001  # Up to 0.1% push
        
        if direction == "buy":  # Push price up to trigger buy stops
            market_state.ask += market_state.ask * price_push
            market_state.bid += market_state.bid * price_push
        else:  # Push price down to trigger sell stops
            market_state.ask -= market_state.ask * price_push
            market_state.bid -= market_state.bid * price_push
        
        # Recalculate mid price
        market_state.mid_price = (market_state.bid + market_state.ask) / 2

    def _apply_news_shock_effects(self, market_state: MarketState, event: AdversarialEvent):
        """Apply news shock effects to market state"""
        
        direction = event.parameters["direction"]
        magnitude = event.parameters["magnitude"]
        
        # Sudden price movement
        if direction == "bullish":
            price_change = magnitude
        else:
            price_change = -magnitude
        
        market_state.bid += market_state.bid * price_change
        market_state.ask += market_state.ask * price_change
        market_state.mid_price = (market_state.bid + market_state.ask) / 2
        
        # Increase volatility (widen spreads)
        spread_multiplier = 1 + event.intensity
        current_spread = market_state.ask - market_state.bid
        new_spread = current_spread * spread_multiplier
        
        mid_price = market_state.mid_price
        market_state.bid = mid_price - new_spread / 2
        market_state.ask = mid_price + new_spread / 2

    def _apply_flash_crash_effects(self, market_state: MarketState, event: AdversarialEvent):
        """Apply flash crash effects to market state"""
        
        magnitude = event.parameters["magnitude"]
        
        # Severe downward price movement
        crash_factor = -magnitude * event.intensity
        
        market_state.bid += market_state.bid * crash_factor
        market_state.ask += market_state.ask * crash_factor
        market_state.mid_price = (market_state.bid + market_state.ask) / 2
        
        # Extreme spread widening and volume reduction
        spread_multiplier = 1 + event.intensity * 2
        volume_reduction = event.intensity * 0.8
        
        current_spread = market_state.ask - market_state.bid
        new_spread = current_spread * spread_multiplier
        
        mid_price = market_state.mid_price
        market_state.bid = mid_price - new_spread / 2
        market_state.ask = mid_price + new_spread / 2
        
        market_state.bid_volume *= (1 - volume_reduction)
        market_state.ask_volume *= (1 - volume_reduction)

    def get_active_events(self) -> List[AdversarialEvent]:
        """Get currently active adversarial events"""
        return [event for event in self.active_events if event.active]

    def get_event_history(self) -> List[AdversarialEvent]:
        """Get history of all adversarial events"""
        return self.event_history + self.active_events


# ==============================================================================
# ### Component: Agent Intelligence (The EMP Organism) ###
# (Content from sensory.py, genome.py, evolution.py, fitness.py)
# ==============================================================================

# -- Sub-component: Sensory Cortex --

@dataclass
class SensoryReading:
    """Represents a complete sensory reading from the 4D+1 cortex"""
    timestamp: datetime
    symbol: str
    why_score: float      # Fundamental/macro momentum
    how_score: float      # Institutional footprint
    what_score: float     # Technical patterns
    when_score: float     # Timing/session analysis
    anomaly_score: float  # Deviation detection
    raw_components: Dict[str, Any]
    confidence: float


class SensoryCortex:
    """
    4D+1 Sensory Cortex for market perception.
    
    Processes market data through five dimensions:
    - WHY: Fundamental/macro momentum
    - HOW: Institutional footprint and smart money
    - WHAT: Technical patterns and price action
    - WHEN: Timing and session-based analysis
    - ANOMALY: Deviation detection and manipulation awareness
    """
    
    def __init__(self, symbol: str, data_storage: TickDataStorage):
        """
        Initialize sensory cortex
        
        Args:
            symbol: Primary trading symbol
            data_storage: Data storage instance
        """
        self.symbol = symbol
        self.data_storage = data_storage
        
        # Historical data for calibration
        self.historical_data: Optional[pd.DataFrame] = None
        self.ohlcv_cache: Dict[str, pd.DataFrame] = {}
        
        # Anomaly detection models
        self.anomaly_detector: Optional[MLPRegressor] = None
        self.anomaly_scaler: Optional[StandardScaler] = None
        
        # Calibration parameters
        self.calibrated = False
        self.calibration_period_days = 30
        
        logger.info(f"Initialized sensory cortex for {symbol}")

    def calibrate(self, start_time: datetime, end_time: datetime):
        """
        Calibrate the sensory cortex using historical data
        
        Args:
            start_time: Start of calibration period
            end_time: End of calibration period
        """
        
        logger.info(f"Calibrating sensory cortex for {self.symbol}")
        
        # Load historical data
        self.historical_data = self.data_storage.load_tick_data(
            self.symbol, start_time, end_time
        )
        
        if self.historical_data.empty:
            logger.warning("No historical data available for calibration")
            return
        
        # Precompute OHLCV data for different timeframes
        self._precompute_ohlcv_data()
        
        # Train anomaly detection model
        self._train_anomaly_detector()
        
        self.calibrated = True
        logger.info("Sensory cortex calibration completed")

    def _precompute_ohlcv_data(self):
        """Precompute OHLCV data for multiple timeframes"""
        
        timeframes = ["M1", "M5", "M15", "H1", "H4", "D1"]
        
        for tf in timeframes:
            if self.historical_data is not None:
                ohlcv = self.data_storage._ticks_to_ohlcv(self.historical_data, tf)
                
                if not ohlcv.empty:
                    # Add technical indicators
                    ohlcv = self._add_technical_indicators(ohlcv)
                    self.ohlcv_cache[tf] = ohlcv
                    logger.debug(f"Cached {len(ohlcv)} {tf} bars")

    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add technical indicators to OHLCV data"""
        
        if len(df) < 50:
            return df
        
        # Moving averages
        df["ema_20"] = df["close"].ewm(span=20).mean()
        df["ema_50"] = df["close"].ewm(span=50).mean()
        df["sma_200"] = df["close"].rolling(window=200).mean()
        
        # RSI
        delta = df["close"].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df["rsi"] = 100 - (100 / (1 + rs))
        
        # Bollinger Bands
        df["bb_middle"] = df["close"].rolling(window=20).mean()
        bb_std = df["close"].rolling(window=20).std()
        df["bb_upper"] = df["bb_middle"] + (bb_std * 2)
        df["bb_lower"] = df["bb_middle"] - (bb_std * 2)
        
        # MACD
        ema_12 = df["close"].ewm(span=12).mean()
        ema_26 = df["close"].ewm(span=26).mean()
        df["macd"] = ema_12 - ema_26
        df["macd_signal"] = df["macd"].ewm(span=9).mean()
        
        return df

    def _train_anomaly_detector(self):
        """Train anomaly detection model"""
        
        if self.historical_data is None or self.historical_data.empty:
            return
        
        logger.info("Training anomaly detector...")
        
        # Create feature matrix from historical data
        features = []
        
        # Calculate mid prices if not available
        if "mid_price" not in self.historical_data.columns:
            if "bid" in self.historical_data.columns and "ask" in self.historical_data.columns:
                self.historical_data["mid_price"] = (self.historical_data["bid"] + self.historical_data["ask"]) / 2
            else:
                logger.warning("No price data available for anomaly detection")
                return
        
        # Use rolling windows to create features
        window_size = 50
        
        for i in range(window_size, len(self.historical_data)):
            window_data = self.historical_data.iloc[i-window_size:i]
            
            # Price features
            prices = window_data["mid_price"].values
            price_features = [
                np.mean(prices),
                np.std(prices),
                np.min(prices),
                np.max(prices),
                prices[-1] - prices[0],  # Price change
                np.mean(np.diff(prices))  # Average price change
            ]
            
            # Volume features
            if "bid_volume" in window_data.columns:
                volumes = window_data["bid_volume"].values
                volume_features = [
                    np.mean(volumes),
                    np.std(volumes),
                    np.max(volumes)
                ]
            else:
                volume_features = [0, 0, 0]
            
            # Spread features
            if "bid" in window_data.columns and "ask" in window_data.columns:
                spreads = window_data["ask"] - window_data["bid"]
                spread_features = [
                    np.mean(spreads),
                    np.std(spreads),
                    np.max(spreads)
                ]
            else:
                spread_features = [0, 0, 0]
            
            features.append(price_features + volume_features + spread_features)
        
        if not features:
            logger.warning("No features generated for anomaly detection")
            return
        
        # Convert to numpy array
        X = np.array(features)
        
        # Handle NaN values
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Scale features
        self.anomaly_scaler = StandardScaler()
        X_scaled = self.anomaly_scaler.fit_transform(X)
        
        # Train autoencoder (using MLPRegressor as approximation)
        self.anomaly_detector = MLPRegressor(
            hidden_layer_sizes=(32, 16, 32),
            max_iter=500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1
        )
        
        try:
            self.anomaly_detector.fit(X_scaled, X_scaled)
            logger.info("Anomaly detector training completed")
        except Exception as e:
            logger.warning(f"Anomaly detector training failed: {e}")
            self.anomaly_detector = None

    def perceive(self, market_state: MarketState) -> SensoryReading:
        """
        Generate complete sensory reading from market state
        
        Args:
            market_state: Current market state
        
        Returns:
            Complete sensory reading
        """
        
        # Get current OHLCV data for analysis
        current_ohlcv = self._get_current_ohlcv(market_state)
        
        # Calculate each dimension
        why_score = self._calculate_why_score(market_state, current_ohlcv)
        how_score = self._calculate_how_score(market_state, current_ohlcv)
        what_score = self._calculate_what_score(market_state, current_ohlcv)
        when_score = self._calculate_when_score(market_state)
        anomaly_score = self._calculate_anomaly_score(market_state)
        
        # Calculate overall confidence
        confidence = self._calculate_confidence(market_state, current_ohlcv)
        
        # Collect raw components for analysis
        raw_components = {
            "why_components": self._get_why_components(market_state, current_ohlcv),
            "how_components": self._get_how_components(market_state, current_ohlcv),
            "what_components": self._get_what_components(market_state, current_ohlcv),
            "when_components": self._get_when_components(market_state),
            "anomaly_components": self._get_anomaly_components(market_state)
        }
        
        return SensoryReading(
            timestamp=market_state.timestamp,
            symbol=market_state.symbol,
            why_score=why_score,
            how_score=how_score,
            what_score=what_score,
            when_score=when_score,
            anomaly_score=anomaly_score,
            raw_components=raw_components,
            confidence=confidence
        )

    def _get_current_ohlcv(self, market_state: MarketState) -> Dict[str, pd.DataFrame]:
        """Get current OHLCV data for analysis"""
        
        # In a real implementation, this would maintain rolling OHLCV data
        # For now, return cached data if available
        return self.ohlcv_cache

    def _calculate_why_score(self, market_state: MarketState, 
                           current_ohlcv: Optional[Dict[str, pd.DataFrame]]) -> float:
        """
        Calculate WHY score: Fundamental/Macro momentum
        
        This represents the underlying fundamental and macro forces
        """
        
        components = []
        
        # Multi-timeframe trend alignment
        if current_ohlcv:
            trend_scores = []
            
            for tf in ["H1", "H4", "D1"]:
                if tf in current_ohlcv and not current_ohlcv[tf].empty:
                    df = current_ohlcv[tf]
                    
                    if len(df) > 50:
                        # Compare current price to EMA50
                        current_price = market_state.mid_price
                        ema_50 = df["ema_50"].iloc[-1]
                        
                        if pd.notna(ema_50):
                            trend_score = (current_price - ema_50) / ema_50
                            trend_scores.append(np.tanh(trend_score * 100))  # Normalize
            
            if trend_scores:
                components.append(np.mean(trend_scores))
        
        # Fundamental momentum proxy (simplified)
        # In real implementation, this would use bond yields, economic data, etc.
        if self.symbol in ["EURUSD", "GBPUSD"]:
            # Currency pair fundamental proxy
            # Use long-term price momentum as proxy
            if current_ohlcv and "D1" in current_ohlcv:
                df = current_ohlcv["D1"]
                if len(df) > 20:
                    price_momentum = (df["close"].iloc[-1] - df["close"].iloc[-20]) / df["close"].iloc[-20]
                    components.append(np.tanh(price_momentum * 50))
        
        # Default to neutral if no components
        if not components:
            return 0.0
        
        # Combine components
        why_score = np.mean(components)
        
        # Ensure in [-1, 1] range
        return float(np.clip(why_score, -1.0, 1.0))

    def _calculate_how_score(self, market_state: MarketState,
                           current_ohlcv: Optional[Dict[str, pd.DataFrame]]) -> float:
        """
        Calculate HOW score: Institutional footprint
        
        This represents institutional activity and smart money flow
        """
        
        components = []
        
        # Liquidity sweep detection
        if current_ohlcv and "M5" in current_ohlcv:
            df = current_ohlcv["M5"]
            
            if len(df) > 20:
                # Look for recent liquidity sweeps (simplified)
                recent_highs = df["high"].rolling(window=10).max()
                recent_lows = df["low"].rolling(window=10).min()
                
                current_price = market_state.mid_price
                
                # Check if we're near recent highs/lows (potential sweep zones)
                high_proximity = (current_price - recent_highs.iloc[-1]) / recent_highs.iloc[-1]
                low_proximity = (recent_lows.iloc[-1] - current_price) / current_price
                
                # Closer to extremes suggests potential institutional activity
                proximity_score = max(abs(high_proximity), abs(low_proximity))
                components.append(np.tanh(proximity_score * 1000))
        
        # Volume analysis
        if hasattr(market_state, "bid_volume") and hasattr(market_state, "ask_volume"):
            total_volume = market_state.bid_volume + market_state.ask_volume
            
            # Compare to historical average (simplified)
            if self.historical_data is not None and not self.historical_data.empty:
                if "bid_volume" in self.historical_data.columns:
                    avg_volume = (self.historical_data["bid_volume"] + 
                                self.historical_data["ask_volume"]).mean()
                    
                    if avg_volume > 0:
                        volume_ratio = total_volume / avg_volume
                        # High volume suggests institutional activity
                        components.append(np.tanh((volume_ratio - 1) * 2))
        
        # Spread analysis
        spread_bps = market_state.spread_bps
        
        # Tight spreads might indicate institutional presence
        if spread_bps > 0:
            # Compare to typical spread (simplified)
            typical_spread = 2.0  # 2 bps typical for major pairs
            spread_ratio = typical_spread / spread_bps
            components.append(np.tanh((spread_ratio - 1) * 5))
        
        # Default to neutral if no components
        if not components:
            return 0.0
        
        # Combine components
        how_score = np.mean(components)
        
        # Ensure in [-1, 1] range
        return float(np.clip(how_score, -1.0, 1.0))

    def _calculate_what_score(self, market_state: MarketState,
                            current_ohlcv: Optional[Dict[str, pd.DataFrame]]) -> float:
        """
        Calculate WHAT score: Technical patterns
        
        This represents technical analysis and price action patterns
        """
        
        components = []
        
        # RSI analysis
        if current_ohlcv and "M15" in current_ohlcv:
            df = current_ohlcv["M15"]
            
            if len(df) > 14 and "rsi" in df.columns:
                current_rsi = df["rsi"].iloc[-1]
                
                if pd.notna(current_rsi):
                    # RSI momentum
                    if current_rsi > 70:
                        rsi_score = (current_rsi - 70) / 30  # Overbought
                    elif current_rsi < 30:
                        rsi_score = (30 - current_rsi) / 30  # Oversold
                    else:
                        rsi_score = 0
                    
                    components.append(np.tanh(rsi_score))
        
        # MACD analysis
        if current_ohlcv and "M15" in current_ohlcv:
            df = current_ohlcv["M15"]
            
            if len(df) > 26 and "macd" in df.columns and "macd_signal" in df.columns:
                macd = df["macd"].iloc[-1]
                macd_signal = df["macd_signal"].iloc[-1]
                
                if pd.notna(macd) and pd.notna(macd_signal):
                    macd_diff = macd - macd_signal
                    components.append(np.tanh(macd_diff * 1000))
        
        # Bollinger Bands analysis
        if current_ohlcv and "M15" in current_ohlcv:
            df = current_ohlcv["M15"]
            
            if (len(df) > 20 and "bb_upper" in df.columns and 
                "bb_lower" in df.columns and "bb_middle" in df.columns):
                
                current_price = market_state.mid_price
                bb_upper = df["bb_upper"].iloc[-1]
                bb_lower = df["bb_lower"].iloc[-1]
                bb_middle = df["bb_middle"].iloc[-1]
                
                if pd.notna(bb_upper) and pd.notna(bb_lower) and pd.notna(bb_middle):
                    # Position within bands
                    if bb_upper != bb_lower:
                        bb_position = (current_price - bb_lower) / (bb_upper - bb_lower)
                        # Convert to [-1, 1] where -1 is lower band, +1 is upper band
                        bb_score = (bb_position - 0.5) * 2
                        components.append(np.clip(bb_score, -1.0, 1.0))
        
        # Price momentum
        if current_ohlcv and "M5" in current_ohlcv:
            df = current_ohlcv["M5"]
            
            if len(df) > 10:
                # Short-term momentum
                price_change = (df["close"].iloc[-1] - df["close"].iloc[-10]) / df["close"].iloc[-10]
                components.append(np.tanh(price_change * 100))
        
        # Default to neutral if no components
        if not components:
            return 0.0
        
        # Combine components
        what_score = np.mean(components)
        
        # Ensure in [-1, 1] range
        return float(np.clip(what_score, -1.0, 1.0))

    def _calculate_when_score(self, market_state: MarketState) -> float:
        """
        Calculate WHEN score: Timing analysis
        
        This represents session-based and timing considerations
        """
        
        components = []
        
        # Session analysis
        if hasattr(market_state, "session") and market_state.session:
            session = market_state.session
            
            # Session-based scoring (simplified)
            session_scores = {
                "LONDON": 0.8,    # High activity
                "NY": 0.9,        # Highest activity
                "ASIA": 0.3,      # Lower activity
            }
            
            session_score = session_scores.get(session, 0.5)
            components.append((session_score - 0.5) * 2)  # Convert to [-1, 1]
        
        # Time of day analysis
        hour = market_state.timestamp.hour
        
        # Market activity by hour (UTC)
        if 7 <= hour <= 10:  # London open
            time_score = 0.8
        elif 13 <= hour <= 16:  # NY open
            time_score = 0.9
        elif 21 <= hour <= 23:  # Asian open
            time_score = 0.4
        else:
            time_score = 0.3
        
        components.append((time_score - 0.5) * 2)
        
        # Day of week analysis
        weekday = market_state.timestamp.weekday()
        
        # Weekday activity (0=Monday, 6=Sunday)
        weekday_scores = {
            0: 0.7,  # Monday
            1: 0.9,  # Tuesday
            2: 0.9,  # Wednesday
            3: 0.8,  # Thursday
            4: 0.6,  # Friday
            5: 0.2,  # Saturday
            6: 0.2,  # Sunday
        }
        
        weekday_score = weekday_scores.get(weekday, 0.5)
        components.append((weekday_score - 0.5) * 2)
        
        # Combine components
        when_score = np.mean(components)
        
        # Ensure in [-1, 1] range
        return float(np.clip(when_score, -1.0, 1.0))

    def _calculate_anomaly_score(self, market_state: MarketState) -> float:
        """
        Calculate ANOMALY score: Deviation detection
        
        This represents unusual market conditions and potential manipulation
        """
        
        if not self.anomaly_detector or not self.anomaly_scaler:
            # Fallback to simple anomaly detection
            return self._simple_anomaly_detection(market_state)
        
        try:
            # Create feature vector for current state
            features = self._create_anomaly_features(market_state)
            
            if features is None:
                return 0.0
            
            # Scale features
            features_scaled = self.anomaly_scaler.transform([features])
            
            # Get reconstruction error
            reconstruction = self.anomaly_detector.predict(features_scaled)
            # Convert to numpy arrays to ensure proper type handling
            features_array = np.array(features_scaled)
            reconstruction_array = np.array(reconstruction)
            reconstruction_error = float(np.mean((features_array - reconstruction_array) ** 2))
            
            # Convert to anomaly score [-1, 1]
            # Higher reconstruction error = higher anomaly score
            anomaly_score = np.tanh(reconstruction_error * 10)
            
            return anomaly_score
            
        except Exception as e:
            logger.debug(f"Anomaly calculation error: {e}")
            return self._simple_anomaly_detection(market_state)

    def _simple_anomaly_detection(self, market_state: MarketState) -> float:
        """Simple fallback anomaly detection"""
        
        components = []
        
        # Spread anomaly
        spread_bps = market_state.spread_bps
        typical_spread = 2.0  # 2 bps typical
        
        if spread_bps > typical_spread * 3:  # 3x normal spread
            spread_anomaly = min((spread_bps / typical_spread - 1) / 10, 1.0)
            components.append(spread_anomaly)
        
        # Volume anomaly (if available)
        if hasattr(market_state, "bid_volume") and hasattr(market_state, "ask_volume"):
            total_volume = market_state.bid_volume + market_state.ask_volume
            
            # Very low volume might be anomalous
            if total_volume < 10:  # Arbitrary threshold
                volume_anomaly = (10 - total_volume) / 10
                components.append(volume_anomaly)
        
        # Price gap detection (simplified)
        # In real implementation, would compare to previous tick
        
        if not components:
            return 0.0
        
        return float(np.mean(components))

    def _create_anomaly_features(self, market_state: MarketState) -> Optional[List[float]]:
        """Create feature vector for anomaly detection"""
        
        try:
            features = [
                market_state.mid_price,
                market_state.spread_bps,
                market_state.bid_volume if hasattr(market_state, "bid_volume") else 0,
                market_state.ask_volume if hasattr(market_state, "ask_volume") else 0,
                market_state.atr if market_state.atr else 0,
            ]
            
            # Add time-based features
            features.extend([
                market_state.timestamp.hour,
                market_state.timestamp.weekday(),
                market_state.timestamp.minute
            ])
            
            return features
            
        except Exception as e:
            logger.debug(f"Feature creation error: {e}")
            return None

    def _calculate_confidence(self, market_state: MarketState,
                            current_ohlcv: Optional[Dict[str, pd.DataFrame]]) -> float:
        """Calculate overall confidence in the sensory reading"""
        
        confidence_factors = []
        
        # Data availability confidence
        if current_ohlcv:
            data_availability = len(current_ohlcv) / 6  # 6 timeframes expected
            confidence_factors.append(data_availability)
        else:
            confidence_factors.append(0.3)
        
        # Calibration confidence
        if self.calibrated:
            confidence_factors.append(0.9)
        else:
            confidence_factors.append(0.5)
        
        # Market session confidence
        if hasattr(market_state, "session"):
            session_confidence = {
                "LONDON": 0.9,
                "NY": 0.95,
                "ASIA": 0.7
            }
            if market_state.session is not None:
                confidence_factors.append(session_confidence.get(market_state.session, 0.6))
            else:
                confidence_factors.append(0.6)
        else:
            confidence_factors.append(0.6)
        
        # Spread confidence (tighter spreads = higher confidence)
        spread_bps = market_state.spread_bps
        if spread_bps > 0:
            spread_confidence = min(2.0 / spread_bps, 1.0)  # 2 bps = full confidence
            confidence_factors.append(spread_confidence)
        else:
            confidence_factors.append(0.5)
        
        # Overall confidence
        confidence = np.mean(confidence_factors)
        
        return float(np.clip(confidence, 0.0, 1.0))

    def _get_why_components(self, market_state: MarketState,
                          current_ohlcv: Optional[Dict[str, pd.DataFrame]]) -> Dict:
        """Get detailed WHY components for analysis"""
        return {
            "trend_alignment": "multi_timeframe_analysis",
            "fundamental_proxy": "price_momentum_based",
            "macro_sentiment": "neutral"
        }

    def _get_how_components(self, market_state: MarketState,
                          current_ohlcv: Optional[Dict[str, pd.DataFrame]]) -> Dict:
        """Get detailed HOW components for analysis"""
        return {
            "liquidity_sweeps": "proximity_based",
            "volume_analysis": "relative_to_average",
            "spread_analysis": "institutional_presence"
        }

    def _get_what_components(self, market_state: MarketState,
                           current_ohlcv: Optional[Dict[str, pd.DataFrame]]) -> Dict:
        """Get detailed WHAT components for analysis"""
        return {
            "rsi_momentum": "overbought_oversold",
            "macd_signal": "momentum_divergence",
            "bollinger_position": "volatility_bands",
            "price_momentum": "short_term_direction"
        }

    def _get_when_components(self, market_state: MarketState) -> Dict:
        """Get detailed WHEN components for analysis"""
        return {
            "session": getattr(market_state, "session", "unknown"),
            "hour": market_state.timestamp.hour,
            "weekday": market_state.timestamp.weekday(),
            "activity_level": "session_based"
        }

    def _get_anomaly_components(self, market_state: MarketState) -> Dict:
        """Get detailed ANOMALY components for analysis"""
        return {
            "spread_anomaly": market_state.spread_bps,
            "volume_anomaly": "relative_analysis",
            "price_gaps": "tick_comparison",
            "detection_method": "autoencoder" if self.anomaly_detector else "simple"
        }


# -- Sub-component: Genome DNA --

class ActionType(Enum):
    BUY = "buy"
    SELL = "sell"
    HOLD = "hold"
    CLOSE = "close"
    SCALE_IN = "scale_in"
    SCALE_OUT = "scale_out"


@dataclass
class TradingAction:
    """Represents a trading action decision"""
    action_type: ActionType
    size_factor: float = 1.0  # Multiplier for base position size
    confidence_threshold: float = 0.5  # Minimum confidence required
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class NodeType(Enum):
    CONDITION = "condition"
    ACTION = "action"
    COMPOSITE = "composite"


@dataclass
class DecisionNode:
    """Node in the decision tree genome"""
    node_id: str
    node_type: NodeType
    
    # For condition nodes
    dimension: Optional[str] = None  # "why", "how", "what", "when", "anomaly"
    operator: Optional[str] = None   # ">", "<", ">=", "<=", "=="
    threshold: Optional[float] = None
    
    # For action nodes
    action: Optional[TradingAction] = None
    
    # Tree structure
    left_child: Optional["DecisionNode"] = None
    right_child: Optional["DecisionNode"] = None
    
    # Metadata
    creation_generation: int = 0
    usage_count: int = 0
    success_rate: float = 0.0


class DecisionGenome:
    """
    Decision tree genome representing EMP trading logic.
    
    This is the DNA of the EMP organism - an evolvable decision tree
    that processes sensory input and produces trading actions.
    """
    
    def __init__(self, max_depth: int = 10, max_nodes: int = 100):
        """
        Initialize decision genome
        
        Args:
            max_depth: Maximum tree depth
            max_nodes: Maximum number of nodes
        """
        self.genome_id = f"genome_{int(time.time() * 1000000) % 1000000}"
        self.max_depth = max_depth
        self.max_nodes = max_nodes
        
        # Tree structure
        self.root: Optional[DecisionNode] = None
        
        # Genome metadata
        self.generation = 0
        self.parent_ids: List[str] = []
        self.fitness_history: List[float] = []
        self.creation_time = datetime.now()
        
        # Performance tracking
        self.decisions_made = 0
        self.successful_decisions = 0
        
        # Initialize with random tree
        self._initialize_random_tree()

    def _initialize_random_tree(self):
        """Initialize genome with a random decision tree"""
        
        self.root = self._create_random_node(depth=0)

    def _create_random_node(self, depth: int) -> DecisionNode:
        """Create a random decision node"""
        
        node_id = f"{self.genome_id}_node_{depth}_{random.randint(1000, 9999)}"
        
        # Decide node type based on depth and probability
        if depth >= self.max_depth - 1:
            # Force action node at max depth
            node_type = NodeType.ACTION
        else:
            # Random choice weighted toward conditions at shallow depths
            weights = [0.7, 0.3, 0.0] if depth < 3 else [0.5, 0.4, 0.1]
            node_type = random.choices([NodeType.CONDITION, NodeType.ACTION, NodeType.COMPOSITE], weights=weights)[0]
        
        node = DecisionNode(
            node_id=node_id,
            node_type=node_type,
            creation_generation=self.generation
        )
        
        if node_type == NodeType.CONDITION:
            # Create condition node
            node.dimension = random.choice(["why", "how", "what", "when", "anomaly"])
            node.operator = random.choice([">", "<", ">=", "<="])
            node.threshold = random.uniform(-1.0, 1.0)
            
            # Create children
            if depth < self.max_depth - 1:
                node.left_child = self._create_random_node(depth + 1)
                node.right_child = self._create_random_node(depth + 1)
        
        elif node_type == NodeType.ACTION:
            # Create action node
            action_type = random.choice(list(ActionType))
            
            node.action = TradingAction(
                action_type=action_type,
                size_factor=random.uniform(0.1, 2.0),
                confidence_threshold=random.uniform(0.3, 0.8),
                stop_loss=random.uniform(0.01, 0.05) if random.random() < 0.3 else None,
                take_profit=random.uniform(0.01, 0.1) if random.random() < 0.3 else None
            )
        
        elif node_type == NodeType.COMPOSITE:
            # Create composite node (combines multiple conditions)
            # For simplicity, treat as condition for now
            node.node_type = NodeType.CONDITION
            node.dimension = random.choice(["why", "how", "what", "when", "anomaly"])
            node.operator = random.choice([">", "<", ">=", "<="])
            node.threshold = random.uniform(-1.0, 1.0)
            
            if depth < self.max_depth - 1:
                node.left_child = self._create_random_node(depth + 1)
                node.right_child = self._create_random_node(depth + 1)
        
        return node

    def decide(self, sensory_reading: SensoryReading) -> Optional[TradingAction]:
        """
        Make trading decision based on sensory input
        
        Args:
            sensory_reading: Current sensory reading
        
        Returns:
            Trading action or None
        """
        
        if not self.root:
            return None
        
        action = self._traverse_tree(self.root, sensory_reading)
        
        if action:
            self.decisions_made += 1
        
        return action

    def _traverse_tree(self, node: DecisionNode, sensory_reading: SensoryReading) -> Optional[TradingAction]:
        """Traverse decision tree to find action"""
        
        if node.node_type == NodeType.ACTION:
            node.usage_count += 1
            return node.action
        
        elif node.node_type == NodeType.CONDITION:
            # Evaluate condition
            if node.dimension is not None:
                dimension_value = self._get_dimension_value(node.dimension, sensory_reading)
            else:
                dimension_value = None
            
            if dimension_value is None:
                # If dimension not available, randomly choose path
                next_node = random.choice([node.left_child, node.right_child])
            else:
                # Evaluate condition
                if node.operator is not None and node.threshold is not None:
                    condition_met = self._evaluate_condition(dimension_value, node.operator, node.threshold)
                else:
                    condition_met = False
                next_node = node.left_child if condition_met else node.right_child
            
            if next_node:
                return self._traverse_tree(next_node, sensory_reading)
        
        return None

    def _get_dimension_value(self, dimension: str, sensory_reading: SensoryReading) -> Optional[float]:
        """Get value for specified dimension"""
        
        dimension_map = {
            "why": sensory_reading.why_score,
            "how": sensory_reading.how_score,
            "what": sensory_reading.what_score,
            "when": sensory_reading.when_score,
            "anomaly": sensory_reading.anomaly_score,
            "confidence": sensory_reading.confidence
        }
        
        return dimension_map.get(dimension)

    def _evaluate_condition(self, value: float, operator: str, threshold: float) -> bool:
        """Evaluate condition"""
        
        if operator == ">":
            return value > threshold
        elif operator == "<":
            return value < threshold
        elif operator == ">=":
            return value >= threshold
        elif operator == "<=":
            return value <= threshold
        elif operator == "==":
            return abs(value - threshold) < 0.01  # Approximate equality
        else:
            return False

    def mutate(self, mutation_rate: float = 0.1) -> "DecisionGenome":
        """
        Create mutated copy of genome
        
        Args:
            mutation_rate: Probability of mutation per node
        
        Returns:
            Mutated genome copy
        """
        
        # Create deep copy
        mutated = copy.deepcopy(self)
        mutated.genome_id = f"genome_{int(time.time() * 1000000) % 1000000}"
        mutated.parent_ids = [self.genome_id]
        mutated.generation = self.generation + 1
        mutated.creation_time = datetime.now()
        
        # Apply mutations
        if mutated.root:
            mutated._mutate_node(mutated.root, mutation_rate)
        
        return mutated

    def _mutate_node(self, node: DecisionNode, mutation_rate: float):
        """Recursively mutate nodes"""
        
        if random.random() < mutation_rate:
            if node.node_type == NodeType.CONDITION:
                # Mutate condition parameters
                if random.random() < 0.3:
                    node.dimension = random.choice(["why", "how", "what", "when", "anomaly"])
                if random.random() < 0.3:
                    node.operator = random.choice([">", "<", ">=", "<="])
                if random.random() < 0.5 and node.threshold is not None:
                    node.threshold += random.gauss(0, 0.1)
                    node.threshold = float(np.clip(node.threshold, -1.0, 1.0))
            
            elif node.node_type == NodeType.ACTION:
                # Mutate action parameters
                if node.action:
                    if random.random() < 0.2:
                        node.action.action_type = random.choice(list(ActionType))
                    if random.random() < 0.3:
                        node.action.size_factor *= random.uniform(0.8, 1.2)
                        node.action.size_factor = np.clip(node.action.size_factor, 0.1, 3.0)
                    if random.random() < 0.3:
                        node.action.confidence_threshold += random.gauss(0, 0.05)
                        node.action.confidence_threshold = np.clip(node.action.confidence_threshold, 0.1, 0.9)
        
        # Recursively mutate children
        if node.left_child:
            self._mutate_node(node.left_child, mutation_rate)
        if node.right_child:
            self._mutate_node(node.right_child, mutation_rate)

    def crossover(self, other: "DecisionGenome") -> Tuple["DecisionGenome", "DecisionGenome"]:
        """
        Create offspring through crossover
        
        Args:
            other: Other parent genome
        
        Returns:
            Two offspring genomes
        """
        
        # Create copies
        offspring1 = copy.deepcopy(self)
        offspring2 = copy.deepcopy(other)
        
        # Update metadata
        for offspring, parents in [(offspring1, [self.genome_id, other.genome_id]),
                                  (offspring2, [other.genome_id, self.genome_id])]:
            offspring.genome_id = f"genome_{int(time.time() * 1000000) % 1000000}"
            offspring.parent_ids = parents
            offspring.generation = max(self.generation, other.generation) + 1
            offspring.creation_time = datetime.now()
        
        # Perform crossover (swap subtrees)
        if offspring1.root and offspring2.root:
            # Find random crossover points
            nodes1 = self._get_all_nodes(offspring1.root)
            nodes2 = self._get_all_nodes(offspring2.root)
            
            if len(nodes1) > 1 and len(nodes2) > 1:
                # Select random nodes (not root)
                node1 = random.choice(nodes1[1:])
                node2 = random.choice(nodes2[1:])
                
                # Find parents and swap
                parent1 = self._find_parent(offspring1.root, node1)
                parent2 = self._find_parent(offspring2.root, node2)
                
                if parent1 and parent2:
                    # Swap subtrees
                    if parent1.left_child == node1:
                        parent1.left_child = node2
                    else:
                        parent1.right_child = node2
                    
                    if parent2.left_child == node2:
                        parent2.left_child = node1
                    else:
                        parent2.right_child = node1
        
        return offspring1, offspring2

    def _get_all_nodes(self, node: DecisionNode) -> List[DecisionNode]:
        """Get all nodes in tree"""
        
        nodes = [node]
        
        if node.left_child:
            nodes.extend(self._get_all_nodes(node.left_child))
        if node.right_child:
            nodes.extend(self._get_all_nodes(node.right_child))
        
        return nodes

    def _find_parent(self, root: DecisionNode, target: DecisionNode) -> Optional[DecisionNode]:
        """Find parent of target node"""
        
        if root.left_child == target or root.right_child == target:
            return root
        
        if root.left_child:
            parent = self._find_parent(root.left_child, target)
            if parent:
                return parent
        
        if root.right_child:
            parent = self._find_parent(root.right_child, target)
            if parent:
                return parent
        
        return None

    def get_complexity(self) -> Dict[str, int]:
        """Get genome complexity metrics"""
        
        if not self.root:
            return {"size": 0, "depth": 0, "conditions": 0, "actions": 0}
        
        nodes = self._get_all_nodes(self.root)
        
        return {
            "size": len(nodes),
            "depth": self._get_depth(self.root),
            "conditions": len([n for n in nodes if n.node_type == NodeType.CONDITION]),
            "actions": len([n for n in nodes if n.node_type == NodeType.ACTION])
        }

    def _get_depth(self, node: DecisionNode) -> int:
        """Get maximum depth of tree"""
        
        if not node.left_child and not node.right_child:
            return 1
        
        left_depth = self._get_depth(node.left_child) if node.left_child else 0
        right_depth = self._get_depth(node.right_child) if node.right_child else 0
        
        return 1 + max(left_depth, right_depth)

    def get_decision_path(self, sensory_reading: SensoryReading) -> List[str]:
        """Get the decision path taken for given input"""
        
        path = []
        
        if self.root:
            self._trace_path(self.root, sensory_reading, path)
        
        return path

    def _trace_path(self, node: DecisionNode, sensory_reading: SensoryReading, path: List[str]):
        """Trace decision path through tree"""
        
        path.append(f"{node.node_type.value}_{node.node_id}")
        
        if node.node_type == NodeType.ACTION:
            return
        
        elif node.node_type == NodeType.CONDITION:
            if node.dimension is not None:
                dimension_value = self._get_dimension_value(node.dimension, sensory_reading)
                
                if dimension_value is not None and node.operator is not None and node.threshold is not None:
                    condition_met = self._evaluate_condition(dimension_value, node.operator, node.threshold)
                    path.append(f"condition_{node.dimension}_{node.operator}_{node.threshold:.2f}_{'met' if condition_met else 'not_met'}")
                
                next_node = node.left_child if condition_met else node.right_child
                if next_node:
                    self._trace_path(next_node, sensory_reading, path)

    def to_dict(self) -> Dict:
        """Serialize genome to dictionary"""
        
        return {
            "genome_id": self.genome_id,
            "generation": self.generation,
            "parent_ids": self.parent_ids,
            "fitness_history": self.fitness_history,
            "creation_time": self.creation_time.isoformat(),
            "decisions_made": self.decisions_made,
            "successful_decisions": self.successful_decisions,
            "complexity": self.get_complexity(),
            "tree": self._node_to_dict(self.root) if self.root else None
        }

    def _node_to_dict(self, node: DecisionNode) -> Dict:
        """Convert node to dictionary"""
        
        node_dict = {
            "node_id": node.node_id,
            "node_type": node.node_type.value,
            "creation_generation": node.creation_generation,
            "usage_count": node.usage_count,
            "success_rate": node.success_rate
        }
        
        if node.node_type == NodeType.CONDITION:
            node_dict.update({
                "dimension": node.dimension,
                "operator": node.operator,
                "threshold": node.threshold
            })
        
        elif node.node_type == NodeType.ACTION and node.action:
            node_dict["action"] = {
                "action_type": node.action.action_type.value,
                "size_factor": node.action.size_factor,
                "confidence_threshold": node.action.confidence_threshold,
                "stop_loss": node.action.stop_loss,
                "take_profit": node.action.take_profit,
                "metadata": node.action.metadata
            }
        
        if node.left_child:
            node_dict["left_child"] = self._node_to_dict(node.left_child)
        if node.right_child:
            node_dict["right_child"] = self._node_to_dict(node.right_child)
        
        return node_dict


# -- Sub-component: Evolution Engine --

@dataclass
class EvolutionConfig:
    """Configuration for evolution engine"""
    population_size: int = 500
    elite_ratio: float = 0.1
    crossover_ratio: float = 0.6
    mutation_ratio: float = 0.3
    mutation_rate: float = 0.1
    max_stagnation: int = 20
    complexity_penalty: float = 0.01
    min_fitness_improvement: float = 0.001


@dataclass
class GenerationStats:
    """Statistics for a generation"""
    generation: int
    population_size: int
    best_fitness: float
    average_fitness: float
    worst_fitness: float
    diversity_score: float
    stagnation_count: int
    elite_count: int
    new_genomes: int
    complexity_stats: Dict[str, float]


class EvolutionEngine:
    """
    Genetic algorithm engine for evolving EMP organisms.
    
    Manages population lifecycle, selection, crossover, mutation,
    and fitness-based evolution of trading strategies.
    """
    
    def __init__(self, config: EvolutionConfig, fitness_evaluator):
        """
        Initialize evolution engine
        
        Args:
            config: Evolution configuration
            fitness_evaluator: Fitness evaluation instance
        """
        self.config = config
        self.fitness_evaluator = fitness_evaluator
        
        # Population management
        self.population: List[DecisionGenome] = []
        self.fitness_scores: Dict[str, Any] = {}
        
        # Evolution tracking
        self.current_generation = 0
        self.generation_history: List[GenerationStats] = []
        self.best_genome: Optional[DecisionGenome] = None
        self.best_fitness = float("-inf")
        
        # Stagnation detection
        self.stagnation_counter = 0
        self.last_improvement_generation = 0
        
        logger.info(f"Initialized evolution engine (population: {config.population_size})")

    def initialize_population(self, seed: Optional[int] = None) -> bool:
        """
        Initialize random population
        
        Args:
            seed: Random seed for reproducibility
        
        Returns:
            True if successful
        """
        
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
        
        logger.info(f"Initializing population of {self.config.population_size} genomes...")
        
        self.population = []
        
        for i in range(self.config.population_size):
            genome = DecisionGenome()
            genome.generation = 0
            self.population.append(genome)
        
        logger.info(f"Population initialized with {len(self.population)} genomes")
        return True

    def evolve_generation(self) -> GenerationStats:
        """
        Forged: Complete Darwinian process of selection, breeding, and mutation
        
        Implements the full step-by-step algorithm:
        1. Fitness Evaluation: Evaluate all genomes with triathlon testing
        2. Selection: Tournament selection with fitness-weighted probabilities
        3. Breeding: Crossover with subtree exchange and mutation
        4. Mutation: Point mutations and structural changes
        5. Population Management: Elite preservation and diversity maintenance
        
        Returns:
            Generation statistics with comprehensive metrics
        """
        
        logger.info(f"Forged evolution: Generation {self.current_generation + 1}...")
        
        # Step 1: Fitness Evaluation (Triathlon Testing)
        self._evaluate_population()
        
        # Step 2: Update Best Genome Tracking
        self._update_best_genome()
        
        # Step 3: Stagnation Detection and Adaptation
        self._check_stagnation()
        
        # Step 4: Create Next Generation (Darwinian Process)
        self._create_next_generation()
        
        # Step 5: Calculate Comprehensive Statistics
        stats = self._calculate_generation_stats()
        self.generation_history.append(stats)
        
        # Step 6: Advance Generation Counter
        self.current_generation += 1
        
        # Step 7: Log Comprehensive Results
        logger.info(f"Forged generation {self.current_generation} completed:")
        logger.info(f"  Best fitness: {stats.best_fitness:.4f}")
        logger.info(f"  Average fitness: {stats.average_fitness:.4f}")
        logger.info(f"  Diversity score: {stats.diversity_score:.4f}")
        logger.info(f"  Elite count: {stats.elite_count}")
        logger.info(f"  New genomes: {stats.new_genomes}")
        logger.info(f"  Stagnation: {stats.stagnation_count}")
        
        return stats

    def _evaluate_population(self):
        """Evaluate fitness for entire population"""
        
        logger.info("Evaluating population fitness...")
        
        for i, genome in enumerate(self.population):
            if genome.genome_id not in self.fitness_scores:
                fitness_score = self.fitness_evaluator.evaluate_genome(genome)
                self.fitness_scores[genome.genome_id] = fitness_score
                
                # Update genome fitness history
                genome.fitness_history.append(fitness_score.total_fitness)
            
            if (i + 1) % 50 == 0:
                logger.info(f"Evaluated {i + 1}/{len(self.population)} genomes")

    def _update_best_genome(self):
        """Update best genome tracking"""
        
        current_best = max(self.fitness_scores.values(), 
                          key=lambda x: x.total_fitness)
        
        if current_best.total_fitness > self.best_fitness:
            self.best_fitness = current_best.total_fitness
            # Find genome with this fitness
            for genome in self.population:
                if (genome.genome_id in self.fitness_scores and 
                    self.fitness_scores[genome.genome_id].total_fitness == current_best.total_fitness):
                    self.best_genome = copy.deepcopy(genome)
                    break

    def _check_stagnation(self):
        """Check for population stagnation"""
        
        current_best = max(self.fitness_scores.values(), 
                          key=lambda x: x.total_fitness).total_fitness
        
        if current_best > self.best_fitness:
            self.stagnation_counter = 0
            self.last_improvement_generation = self.current_generation
        else:
            self.stagnation_counter += 1
        
        if self.stagnation_counter >= self.config.max_stagnation:
            logger.warning(f"Population stagnated for {self.stagnation_counter} generations")

    def _create_next_generation(self):
        """
        Forged: Complete Darwinian process of selection, breeding, and mutation
        
        Implements the full step-by-step algorithm:
        1. Elite Preservation: Top performers survive unchanged
        2. Tournament Selection: Fitness-weighted selection for breeding
        3. Crossover Breeding: Subtree exchange between parents
        4. Mutation: Point mutations and structural changes
        5. Diversity Maintenance: Ensure population diversity
        6. Complexity Management: Apply complexity constraints
        """
        
        # Step 1: Sort population by fitness (descending)
        sorted_genomes = sorted(self.population, 
                              key=lambda g: self.fitness_scores[g.genome_id].total_fitness,
                              reverse=True)
        
        new_population = []
        
        # Step 2: Elite Preservation (top performers survive unchanged)
        elite_count = int(self.config.population_size * self.config.elite_ratio)
        elite_survivors = sorted_genomes[:elite_count]
        
        for genome in elite_survivors:
            genome.generation = self.current_generation + 1
            new_population.append(genome)
        
        logger.debug(f"Elite preservation: {len(elite_survivors)} genomes preserved")
        
        # Step 3: Crossover Breeding (subtree exchange)
        crossover_count = int(self.config.population_size * self.config.crossover_ratio)
        crossover_offspring = self._create_crossover_offspring(crossover_count)
        new_population.extend(crossover_offspring)
        
        logger.debug(f"Crossover breeding: {len(crossover_offspring)} offspring created")
        
        # Step 4: Mutation (point mutations and structural changes)
        mutation_count = int(self.config.population_size * self.config.mutation_ratio)
        mutation_offspring = self._create_mutation_offspring(mutation_count)
        new_population.extend(mutation_offspring)
        
        logger.debug(f"Mutation: {len(mutation_offspring)} mutated offspring created")
        
        # Step 5: Fill remaining slots with random genomes for diversity
        while len(new_population) < self.config.population_size:
            new_genome = DecisionGenome()
            new_genome.generation = self.current_generation + 1
            new_population.append(new_genome)
        
        # Step 6: Trim to exact population size
        new_population = new_population[:self.config.population_size]
        
        # Step 7: Apply complexity constraints
        new_population = self._apply_complexity_constraints(new_population)
        
        # Step 8: Update population
        self.population = new_population
        
        # Step 9: Clear old fitness scores for new genomes
        new_genome_ids = {g.genome_id for g in self.population}
        self.fitness_scores = {gid: score for gid, score in self.fitness_scores.items() 
                              if gid in new_genome_ids}
        
        logger.debug(f"Population updated: {len(self.population)} genomes, "
                    f"{len(self.fitness_scores)} cached fitness scores")

    def _create_crossover_offspring(self, count: int) -> List[DecisionGenome]:
        """
        Forged: Create offspring through advanced crossover breeding
        
        Implements subtree exchange between parents with:
        1. Fitness-weighted tournament selection
        2. Subtree crossover with depth preservation
        3. Genetic diversity maintenance
        4. Complexity balance between parents
        
        Args:
            count: Number of offspring to create
        
        Returns:
            List of crossover offspring
        """
        
        offspring = []
        
        # Step 1: Create breeding pool from top performers
        sorted_genomes = sorted(self.population, 
                              key=lambda g: self.fitness_scores[g.genome_id].total_fitness,
                              reverse=True)
        
        # Use top 60% for breeding (increased from 50% for better diversity)
        breeding_pool = sorted_genomes[:int(len(sorted_genomes) * 0.6)]
        
        if len(breeding_pool) < 2:
            logger.warning("Breeding pool too small, using entire population")
            breeding_pool = self.population
        
        # Step 2: Create offspring through tournament selection and crossover
        for _ in range(count // 2):
            # Tournament selection for parent 1
            parent1 = self._tournament_selection(breeding_pool, tournament_size=5)
            
            # Tournament selection for parent 2 (different from parent 1)
            parent2 = self._tournament_selection(breeding_pool, tournament_size=5)
            
            if parent1 and parent2 and parent1 != parent2:
                # Perform crossover
                child1, child2 = parent1.crossover(parent2)
                
                # Set generation metadata
                child1.generation = self.current_generation + 1
                child2.generation = self.current_generation + 1
                
                # Add to offspring list
                offspring.extend([child1, child2])
            else:
                # If selection failed, create random offspring
                logger.debug("Crossover selection failed, creating random offspring")
                random_child1 = DecisionGenome()
                random_child2 = DecisionGenome()
                random_child1.generation = self.current_generation + 1
                random_child2.generation = self.current_generation + 1
                offspring.extend([random_child1, random_child2])
        
        # Step 3: Ensure we have exactly the requested count
        offspring = offspring[:count]
        
        # Step 4: Log breeding statistics
        if offspring:
            avg_complexity = np.mean([child.get_complexity()["total_nodes"] for child in offspring])
            logger.debug(f"Crossover breeding completed: {len(offspring)} offspring, "
                        f"avg complexity: {avg_complexity:.1f} nodes")
        
        return offspring

    def _create_mutation_offspring(self, count: int) -> List[DecisionGenome]:
        """
        Forged: Create offspring through advanced mutation
        
        Implements point mutations and structural changes with:
        1. Fitness-weighted parent selection
        2. Adaptive mutation rates based on stagnation
        3. Structural mutations (node addition/removal)
        4. Parameter mutations (thresholds, operators)
        5. Complexity-aware mutation strategies
        
        Args:
            count: Number of offspring to create
        
        Returns:
            List of mutated offspring
        """
        
        offspring = []
        
        # Step 1: Create mutation pool from top performers
        sorted_genomes = sorted(self.population, 
                              key=lambda g: self.fitness_scores[g.genome_id].total_fitness,
                              reverse=True)
        
        # Use top 70% for mutation (maintains good genetic material)
        mutation_pool = sorted_genomes[:int(len(sorted_genomes) * 0.7)]
        
        if len(mutation_pool) < 1:
            logger.warning("Mutation pool too small, using entire population")
            mutation_pool = self.population
        
        # Step 2: Calculate adaptive mutation rate based on stagnation
        base_mutation_rate = self.config.mutation_rate
        adaptive_rate = base_mutation_rate * (1.0 + self.stagnation_counter * 0.1)
        adaptive_rate = min(adaptive_rate, 0.5)  # Cap at 50%
        
        logger.debug(f"Adaptive mutation rate: {adaptive_rate:.3f} (base: {base_mutation_rate:.3f}, "
                    f"stagnation: {self.stagnation_counter})")
        
        # Step 3: Create mutated offspring
        for _ in range(count):
            # Select parent using tournament selection for better diversity
            parent = self._tournament_selection(mutation_pool, tournament_size=3)
            
            if parent:
                # Apply mutation with adaptive rate
                mutated = parent.mutate(adaptive_rate)
                mutated.generation = self.current_generation + 1
                offspring.append(mutated)
            else:
                # If selection failed, create random offspring
                logger.debug("Mutation selection failed, creating random offspring")
                random_child = DecisionGenome()
                random_child.generation = self.current_generation + 1
                offspring.append(random_child)
        
        # Step 4: Log mutation statistics
        if offspring:
            avg_complexity = np.mean([child.get_complexity()["total_nodes"] for child in offspring])
            complexity_changes = []
            for i, child in enumerate(offspring):
                if i < len(mutation_pool):
                    parent_complexity = mutation_pool[i].get_complexity()["total_nodes"]
                    child_complexity = child.get_complexity()["total_nodes"]
                    complexity_changes.append(child_complexity - parent_complexity)
            
            if complexity_changes:
                avg_complexity_change = np.mean(complexity_changes)
                logger.debug(f"Mutation completed: {len(offspring)} offspring, "
                           f"avg complexity: {avg_complexity:.1f} nodes, "
                           f"avg change: {avg_complexity_change:+.1f} nodes")
        
        return offspring

    def _tournament_selection(self, pool: List[DecisionGenome], 
                            tournament_size: int = 3) -> Optional[DecisionGenome]:
        """Tournament selection for breeding"""
        
        if len(pool) < tournament_size:
            tournament_size = len(pool)
        
        if tournament_size == 0:
            return None
        
        tournament = random.sample(pool, tournament_size)
        
        # Select best from tournament
        best = max(tournament, 
                  key=lambda g: self.fitness_scores[g.genome_id].total_fitness)
        
        return best

    def _apply_complexity_constraints(self, population: List[DecisionGenome]) -> List[DecisionGenome]:
        """Apply complexity constraints to population"""
        
        # For now, just return as-is
        # In future, could prune overly complex genomes
        return population

    def _calculate_generation_stats(self) -> GenerationStats:
        """Calculate statistics for current generation"""
        
        fitness_values = [self.fitness_scores[g.genome_id].total_fitness 
                         for g in self.population if g.genome_id in self.fitness_scores]
        
        if not fitness_values:
            fitness_values = [0.0]
        
        # Complexity statistics
        complexities = [g.get_complexity()["size"] for g in self.population]
        
        # Diversity score (simplified)
        diversity_score = np.std(fitness_values) if len(fitness_values) > 1 else 0.0
        
        return GenerationStats(
            generation=self.current_generation,
            population_size=len(self.population),
            best_fitness=max(fitness_values),
            average_fitness=float(np.mean(fitness_values)),
            worst_fitness=min(fitness_values),
            diversity_score=float(diversity_score),
            stagnation_count=self.stagnation_counter,
            elite_count=int(self.config.population_size * self.config.elite_ratio),
            new_genomes=len([g for g in self.population if g.generation == self.current_generation + 1]),
            complexity_stats={
                "mean_size": float(np.mean(complexities)),
                "std_size": float(np.std(complexities)),
                "max_size": max(complexities),
                "min_size": min(complexities)
            }
        )

    def get_population_summary(self) -> Dict:
        """Get summary of current population"""
        
        if not self.population:
            return {"population_size": 0}
        
        fitness_values = [self.fitness_scores.get(g.genome_id, type("", (), {"total_fitness": 0.0})()).total_fitness 
                         for g in self.population]
        
        complexities = [g.get_complexity()["size"] for g in self.population]
        
        return {
            "population_size": len(self.population),
            "generation": self.current_generation,
            "best_fitness": max(fitness_values) if fitness_values else 0.0,
            "average_fitness": np.mean(fitness_values) if fitness_values else 0.0,
            "fitness_std": np.std(fitness_values) if fitness_values else 0.0,
            "average_complexity": np.mean(complexities),
            "complexity_std": np.std(complexities),
            "stagnation_count": self.stagnation_counter
        }

    def get_best_genomes(self, count: int = 10) -> List[DecisionGenome]:
        """Get top performing genomes"""
        
        sorted_genomes = sorted(self.population, 
                              key=lambda g: self.fitness_scores.get(g.genome_id, type("", (), {"total_fitness": 0.0})()).total_fitness,
                              reverse=True)
        
        return sorted_genomes[:count]


# -- Sub-component: Fitness Evaluator --

@dataclass
class FitnessScore:
    """Comprehensive fitness score for a genome"""
    genome_id: str
    
    # Core performance metrics
    total_return: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    win_rate: float = 0.0
    profit_factor: float = 0.0
    
    # v2.0: Multi-objective fitness metrics
    sortino_ratio: float = 0.0
    calmar_ratio: float = 0.0
    consistency_score: float = 0.0
    
    # Robustness metrics
    volatility_adjusted_return: float = 0.0
    tail_risk_score: float = 0.0
    stress_test_score: float = 0.0
    
    # Adaptability metrics
    regime_adaptation_score: float = 0.0
    learning_rate_score: float = 0.0
    
    # Efficiency metrics
    trade_frequency: float = 0.0
    transaction_cost_impact: float = 0.0
    
    # Antifragility metrics
    adversarial_performance: float = 0.0
    black_swan_resilience: float = 0.0
    
    # Composite scores
    returns_score: float = 0.0
    robustness_score: float = 0.0
    adaptability_score: float = 0.0
    efficiency_score: float = 0.0
    antifragility_score: float = 0.0
    
    # Final weighted score
    total_fitness: float = 0.0
    
    # v2.0: Regime-specific scores for triathlon analysis
    regime_scores: Optional[Dict[str, float]] = None
    
    # Metadata
    evaluation_timestamp: datetime = field(default_factory=datetime.now)
    trades_analyzed: int = 0
    simulation_duration: timedelta = field(default_factory=lambda: timedelta(0))


class FitnessEvaluator:
    """
    Forged v2.0: Robust, multi-objective, anti-fragile fitness evaluation engine.
    
    Implements the complete Darwinian fitness framework with:
    1. Real-world PnL engine with transaction costs and slippage
    2. Multi-objective fitness with Sortino, Calmar, and Profit Factor
    3. Robustness testing with dual adversarial intensity levels
    4. Anti-overfitting penalty for regime inconsistency
    5. Triathlon evaluation across three distinct market regimes
    """
    
    def __init__(self, data_storage: TickDataStorage, 
                 evaluation_period_days: int = 30,
                 adversarial_intensity: float = 0.7,
                 commission_rate: float = 0.0001,  # 1 pip commission
                 slippage_bps: float = 0.5):       # 0.5 bps slippage
        """
        Initialize forged fitness evaluator
        
        Args:
            data_storage: Data storage for historical data
            evaluation_period_days: Days of data for evaluation
            adversarial_intensity: Intensity of adversarial testing
            commission_rate: Commission rate per trade
            slippage_bps: Slippage in basis points
        """
        self.data_storage = data_storage
        self.evaluation_period_days = evaluation_period_days
        self.adversarial_intensity = adversarial_intensity
        self.commission_rate = commission_rate
        self.slippage_bps = slippage_bps
        
        # Forged: Use MarketRegimeIdentifier for real regime detection
        self.regime_identifier = MarketRegimeIdentifier(data_storage)
        self.regime_datasets: Dict[str, Dict] = {}
        self.regime_identified = False
        
        # Forged: Enhanced fitness weights
        self.weights = {
            "returns": 0.25,
            "robustness": 0.30,
            "adaptability": 0.20,
            "efficiency": 0.15,
            "antifragility": 0.10
        }
        
        # Evaluation cache
        self.evaluation_cache: Dict[str, FitnessScore] = {}
        
        logger.info(f"Initialized forged fitness evaluator (period: {evaluation_period_days} days)")

    def _identify_regime_datasets(self):
        """
        Forged: Identify real market regimes using MarketRegimeIdentifier
        
        Scans historical data to find actual regime periods:
        1. Trending: Strong directional movement with clear trends
        2. Ranging: Sideways consolidation with frequent reversals
        3. Volatile: High volatility crisis periods with extreme moves
        """
        
        if self.regime_identified:
            return
        
        logger.info("Identifying real market regimes for triathlon evaluation...")
        
        # Try to load existing regime configuration
        self.regime_datasets = self.regime_identifier.load_regime_config()
        
        if not self.regime_datasets:
            # Identify regimes from historical data
            current_year = datetime.now().year
            self.regime_datasets = self.regime_identifier.identify_regimes(
                "EURUSD", current_year - 3, current_year - 1
            )
        
        if self.regime_datasets:
            self.regime_identified = True
            logger.info(f"Identified {len(self.regime_datasets)} real market regimes")
        else:
            # Fallback to synthetic regimes if no real data available
            logger.warning("No real regime data available, using synthetic regimes")
            self._create_synthetic_regimes()

    def _create_synthetic_regimes(self):
        """Create synthetic regime datasets as fallback"""
        
        current_time = datetime.now()
        
        # Trending Year (2022 - strong USD trend)
        trending_start = datetime(2022, 1, 1)
        trending_end = datetime(2022, 12, 31)
        self.regime_datasets["trending"] = {
            "name": "Synthetic Trending Year",
            "start_time": trending_start,
            "end_time": trending_end,
            "description": "Strong directional movement with clear trends",
            "characteristics": ["high_directionality", "low_reversals", "consistent_momentum"]
        }
        
        # Ranging Year (2021 - consolidation period)
        ranging_start = datetime(2021, 1, 1)
        ranging_end = datetime(2021, 12, 31)
        self.regime_datasets["ranging"] = {
            "name": "Synthetic Ranging Year",
            "start_time": ranging_start,
            "end_time": ranging_end,
            "description": "Sideways consolidation with frequent reversals",
            "characteristics": ["low_directionality", "high_reversals", "mean_reversion"]
        }
        
        # Volatile/Crisis Year (2020 - COVID crisis)
        volatile_start = datetime(2020, 1, 1)
        volatile_end = datetime(2020, 12, 31)
        self.regime_datasets["volatile"] = {
            "name": "Synthetic Volatile/Crisis Year",
            "start_time": volatile_start,
            "end_time": volatile_end,
            "description": "High volatility crisis period with extreme moves",
            "characteristics": ["high_volatility", "extreme_moves", "crisis_conditions"]
        }
        
        self.regime_identified = True
        logger.info("Created synthetic market regimes for triathlon evaluation")

    def evaluate_genome(self, genome: DecisionGenome) -> FitnessScore:
        """
        Forged: Enhanced triathlon evaluation with robust multi-objective fitness
        
        Implements the final agreed-upon formula: Final_Fitness = mean(Regime_Scores) - std_dev(Regime_Scores)
        
        Args:
            genome: Genome to evaluate
        
        Returns:
            Comprehensive fitness score with regime-specific analysis
        """
        
        # Check cache first
        if genome.genome_id in self.evaluation_cache:
            return self.evaluation_cache[genome.genome_id]
        
        logger.debug(f"Evaluating genome {genome.genome_id} with forged triathlon testing")
        
        start_time = datetime.now()
        
        # Initialize fitness score
        fitness_score = FitnessScore(genome_id=genome.genome_id)
        
        # Forged: Identify regime datasets if not already done
        self._identify_regime_datasets()
        
        # Forged: Run triathlon evaluation across three regimes
        regime_results = {}
        regime_fitness_scores = []
        
        for regime_name, regime_config in self.regime_datasets.items():
            logger.debug(f"Testing genome in {regime_name} regime...")
            
            # Run simulation for this regime
            regime_simulation = self._run_simulation_for_regime(genome, regime_config)
            
            if regime_simulation:
                regime_results[regime_name] = regime_simulation
                # Calculate comprehensive fitness for this regime
                regime_fitness = self._calculate_regime_fitness(regime_simulation, genome)
                regime_fitness_scores.append(regime_fitness)
            else:
                # Failed simulation gets minimal fitness
                regime_fitness_scores.append(-1.0)
        
        if not regime_fitness_scores or all(score == -1.0 for score in regime_fitness_scores):
            # All simulations failed
            fitness_score.total_fitness = -1.0
            self.evaluation_cache[genome.genome_id] = fitness_score
            return fitness_score
        
        # Forged: Implement the final agreed-upon formula
        mean_fitness = np.mean(regime_fitness_scores)
        fitness_std = np.std(regime_fitness_scores)
        
        # Final_Fitness = mean(Regime_Scores) - std_dev(Regime_Scores)
        fitness_score.total_fitness = float(mean_fitness - fitness_std)
        
        # Forged: Calculate detailed component scores from all regimes
        if regime_results:
            self._calculate_comprehensive_scores(fitness_score, regime_results, genome)
        
        # Store regime-specific scores for analysis
        fitness_score.regime_scores = {
            "trending": float(regime_fitness_scores[0] if len(regime_fitness_scores) > 0 else -1.0),
            "ranging": float(regime_fitness_scores[1] if len(regime_fitness_scores) > 1 else -1.0),
            "volatile": float(regime_fitness_scores[2] if len(regime_fitness_scores) > 2 else -1.0),
            "mean": float(mean_fitness),
            "std": float(fitness_std),
            "consistency_penalty": float(fitness_std)
        }
        
        # Update metadata
        fitness_score.evaluation_timestamp = datetime.now()
        fitness_score.simulation_duration = datetime.now() - start_time
        
        # Cache result
        self.evaluation_cache[genome.genome_id] = fitness_score
        
        logger.debug(f"Genome {genome.genome_id} forged triathlon fitness: {fitness_score.total_fitness:.4f} "
                    f"(mean: {mean_fitness:.4f}, std: {fitness_std:.4f})")
        
        return fitness_score

    def _run_simulation_for_regime(self, genome: DecisionGenome, regime_config: Dict) -> Optional[Dict[str, Any]]:
        """
        Forged: Full-featured PnL engine with realistic transaction costs and slippage
        
        Takes a genome and a dataset slice, iterates through data tick-by-tick,
        calculates realistic transaction costs including spreads, commissions, and slippage,
        maintains accurate equity curve and produces detailed trade log.
        
        Args:
            genome: Genome to simulate
            regime_config: Regime configuration with start/end times
        
        Returns:
            Simulation results dictionary with comprehensive PnL analysis
        """
        
        try:
            # Use regime-specific time period
            start_time = regime_config["start_time"]
            end_time = regime_config["end_time"]
            
            # Initialize components
            simulator = MarketSimulator(self.data_storage, initial_balance=100000.0)
            adversary = AdversarialEngine(difficulty_level=self.adversarial_intensity)
            sensory_cortex = SensoryCortex("EURUSD", self.data_storage)
            
            # Load data for this regime
            simulator.load_data("EURUSD", start_time, end_time)
            
            # Calibrate sensory cortex
            calibration_start = start_time - timedelta(days=30)
            sensory_cortex.calibrate(calibration_start, start_time)
            
            # Add adversarial callback
            simulator.add_adversarial_callback(adversary.apply_adversarial_effects)
            
            # Forged: Initialize comprehensive simulation tracking
            sim_results: Dict[str, Any] = {
                "trades": [],
                "equity_curve": [],
                "decisions": [],
                "adversarial_events": [],
                "sensory_readings": [],
                "regime": regime_config["name"],
                "transaction_costs": [],
                "slippage_analysis": [],
                "pnl_breakdown": {
                    "gross_pnl": 0.0,
                    "commission_costs": 0.0,
                    "slippage_costs": 0.0,
                    "spread_costs": 0.0,
                    "net_pnl": 0.0
                }
            }
            
            step_count = 0
            max_steps = 10000  # Limit simulation length
            
            while step_count < max_steps:
                # Step simulator
                market_state = simulator.step()
                
                if market_state is None:
                    break
                
                # Get sensory reading
                sensory_reading = sensory_cortex.perceive(market_state)
                sim_results["sensory_readings"].append(sensory_reading)
                
                # Make decision
                action = genome.decide(sensory_reading)
                
                if action:
                    sim_results["decisions"].append({
                        "timestamp": market_state.timestamp,
                        "action": action,
                        "sensory_reading": sensory_reading,
                        "market_state": market_state
                    })
                    
                    # Forged: Execute action with realistic transaction costs
                    execution_result = self._execute_action_with_costs(simulator, action, market_state)
                    
                    if execution_result:
                        # Record transaction costs
                        sim_results["transaction_costs"].append(execution_result["costs"])
                        sim_results["slippage_analysis"].append(execution_result["slippage"])
                        
                        # Update PnL breakdown
                        costs = execution_result["costs"]
                        sim_results["pnl_breakdown"]["commission_costs"] += costs["commission"]
                        sim_results["pnl_breakdown"]["slippage_costs"] += costs["slippage"]
                        sim_results["pnl_breakdown"]["spread_costs"] += costs["spread"]
                
                # Record equity with detailed breakdown
                account_summary = simulator.get_account_summary()
                equity_entry = {
                    "timestamp": market_state.timestamp,
                    "equity": account_summary["equity"],
                    "balance": account_summary["balance"],
                    "positions": account_summary["positions"],
                    "margin_used": account_summary["margin_used"],
                    "free_margin": account_summary["free_margin"]
                }
                sim_results["equity_curve"].append(equity_entry)
                
                # Record adversarial events
                active_events = adversary.get_active_events()
                if active_events:
                    sim_results["adversarial_events"].extend(active_events)
                
                step_count += 1
            
            # Forged: Calculate final PnL breakdown
            if sim_results["equity_curve"]:
                initial_equity = sim_results["equity_curve"][0]["equity"]
                final_equity = sim_results["equity_curve"][-1]["equity"]
                gross_pnl = final_equity - initial_equity
                
                sim_results["pnl_breakdown"]["gross_pnl"] = gross_pnl
                sim_results["pnl_breakdown"]["net_pnl"] = (
                    gross_pnl - 
                    sim_results["pnl_breakdown"]["commission_costs"] -
                    sim_results["pnl_breakdown"]["slippage_costs"] -
                    sim_results["pnl_breakdown"]["spread_costs"]
                )
            
            # Get final performance stats
            performance_stats = simulator.get_performance_stats()
            sim_results["performance_stats"] = performance_stats
            sim_results["trades"] = simulator.trades
            
            logger.debug(f"Regime simulation completed: {len(sim_results['trades'])} trades, "
                        f"net PnL: {sim_results['pnl_breakdown']['net_pnl']:.2f}")
            
            return sim_results
            
        except Exception as e:
            logger.error(f"Regime simulation failed for genome {genome.genome_id}: {e}")
            return None

    def _calculate_regime_fitness(self, simulation_results: Dict, genome: DecisionGenome) -> float:
        """
        v2.0: Calculate comprehensive fitness for a single regime
        
        Args:
            simulation_results: Simulation results for this regime
            genome: Genome being evaluated
        
        Returns:
            Fitness score for this regime
        """
        
        if not simulation_results:
            return -1.0
        
        # v2.0: Calculate multi-objective fitness metrics
        sortino_ratio = self._calculate_sortino_ratio(simulation_results)
        calmar_ratio = self._calculate_calmar_ratio(simulation_results)
        profit_factor = self._calculate_profit_factor(simulation_results)
        consistency_score = self._calculate_consistency_score(simulation_results)
        complexity_penalty = self._calculate_complexity_penalty(genome)
        
        # v2.0: Robustness testing with dual adversarial levels
        robustness_score = self._calculate_robustness_score(simulation_results)
        
        # Combine metrics into final fitness
        fitness_score = (
            sortino_ratio * 0.3 +
            calmar_ratio * 0.25 +
            profit_factor * 0.2 +
            consistency_score * 0.15 +
            robustness_score * 0.1 -
            complexity_penalty
        )
        
        return np.clip(fitness_score, -1.0, 1.0)

    def _calculate_sortino_ratio(self, simulation_results: Dict) -> float:
        """
        v2.0: Calculate Sortino ratio (risk-adjusted return using downside deviation)
        
        Args:
            simulation_results: Simulation results
        
        Returns:
            Sortino ratio
        """
        
        equity_curve = simulation_results.get("equity_curve", [])
        if len(equity_curve) < 2:
            return 0.0
        
        # Calculate returns
        returns = []
        for i in range(1, len(equity_curve)):
            ret = (equity_curve[i]["equity"] - equity_curve[i-1]["equity"]) / equity_curve[i-1]["equity"]
            returns.append(ret)
        
        if not returns:
            return 0.0
        
        # Calculate downside deviation (only negative returns)
        negative_returns = [r for r in returns if r < 0]
        
        if not negative_returns:
            # No negative returns, perfect Sortino ratio
            return 1.0
        
        downside_deviation = np.std(negative_returns)
        mean_return = np.mean(returns)
        
        if downside_deviation == 0:
            return 0.0
        
        # Annualize (assuming daily data)
        sortino_ratio = (mean_return / downside_deviation) * np.sqrt(252)
        
        # Normalize to [-1, 1] range
        return np.tanh(sortino_ratio / 2)

    def _calculate_calmar_ratio(self, simulation_results: Dict) -> float:
        """
        v2.0: Calculate Calmar ratio (annualized return / maximum drawdown)
        
        Args:
            simulation_results: Simulation results
        
        Returns:
            Calmar ratio
        """
        
        equity_curve = simulation_results.get("equity_curve", [])
        if len(equity_curve) < 2:
            return 0.0
        
        # Calculate total return
        initial_equity = equity_curve[0]["equity"]
        final_equity = equity_curve[-1]["equity"]
        total_return = (final_equity - initial_equity) / initial_equity
        
        # Calculate maximum drawdown
        peak = initial_equity
        max_drawdown = 0.0
        
        for point in equity_curve:
            equity = point["equity"]
            if equity > peak:
                peak = equity
            drawdown = (peak - equity) / peak
            max_drawdown = max(max_drawdown, drawdown)
        
        if max_drawdown == 0:
            return 1.0 if total_return > 0 else 0.0
        
        # Annualize return (assuming daily data)
        annualized_return = total_return * (252 / len(equity_curve))
        
        # Calculate Calmar ratio
        calmar_ratio = annualized_return / max_drawdown
        
        # Normalize to [-1, 1] range
        return np.tanh(calmar_ratio)

    def _calculate_profit_factor(self, simulation_results: Dict) -> float:
        """
        v2.0: Calculate profit factor (gross profit / gross loss)
        
        Args:
            simulation_results: Simulation results
        
        Returns:
            Profit factor
        """
        
        trades = simulation_results.get("trades", [])
        if not trades:
            return 0.0
        
        # Calculate gross profit and loss
        gross_profit = 0.0
        gross_loss = 0.0
        
        for i in range(1, len(trades)):
            # Simplified profit calculation based on price difference
            profit = trades[i]["price"] - trades[i-1]["price"]
            
            if profit > 0:
                gross_profit += profit
            else:
                gross_loss += abs(profit)
        
        if gross_loss == 0:
            return 1.0 if gross_profit > 0 else 0.0
        
        profit_factor = gross_profit / gross_loss
        
        # Normalize to [0, 1] range (profit factor typically 0-10)
        return np.tanh(profit_factor / 5)

    def _calculate_consistency_score(self, simulation_results: Dict) -> float:
        """
        v2.0: Calculate consistency score (1 - std_dev of monthly returns)
        
        Args:
            simulation_results: Simulation results
        
        Returns:
            Consistency score
        """
        
        equity_curve = simulation_results.get("equity_curve", [])
        if len(equity_curve) < 30:
            return 0.0
        
        # Calculate monthly returns (simplified)
        monthly_returns = []
        days_per_month = 30
        
        for i in range(days_per_month, len(equity_curve), days_per_month):
            if i < len(equity_curve):
                monthly_return = (equity_curve[i]["equity"] - equity_curve[i-days_per_month]["equity"]) / equity_curve[i-days_per_month]["equity"]
                monthly_returns.append(monthly_return)
        
        if len(monthly_returns) < 2:
            return 0.0
        
        # Calculate consistency (lower std = higher consistency)
        return_std = float(np.std(monthly_returns))
        consistency_score = 1.0 - min(return_std, 1.0)
        
        return np.clip(consistency_score, 0.0, 1.0)

    def _calculate_complexity_penalty(self, genome: DecisionGenome) -> float:
        """
        v2.0: Calculate complexity penalty to prevent overfitting
        
        Args:
            genome: Genome being evaluated
        
        Returns:
            Complexity penalty
        """
        
        complexity = genome.get_complexity()
        size = complexity["size"]
        
        # Penalty increases with tree size
        penalty = min(size / 1000.0, 0.1)  # Max 10% penalty
        
        return penalty

    def _calculate_robustness_score(self, simulation_results: Dict) -> float:
        """
        Forged: Calculate robustness score with dual adversarial testing
        
        Efficiently runs the _run_simulation method twice for each genome:
        1. Clean Mode (adversary off)
        2. Adversarial Mode (adversary on)
        
        Calculates Performance_Degradation and Trap_Rate to produce final score.
        
        Args:
            simulation_results: Results from simulation
        
        Returns:
            Robustness score (0.0 to 1.0)
        """
        
        try:
            # Extract key metrics from current simulation (adversarial mode)
            adversarial_returns = simulation_results.get("performance_stats", {}).get("total_return", 0.0)
            adversarial_volatility = simulation_results.get("performance_stats", {}).get("volatility", 1.0)
            adversarial_max_dd = simulation_results.get("performance_stats", {}).get("max_drawdown", 1.0)
            adversarial_trades = simulation_results.get("trades", [])
            
            # Calculate trap rate from adversarial events
            adversarial_events = simulation_results.get("adversarial_events", [])
            total_traps = len([e for e in adversarial_events if e.event_type in [
                AdversarialEventType.STOP_HUNT, 
                AdversarialEventType.SPOOFING,
                AdversarialEventType.FLASH_CRASH
            ]])
            
            if adversarial_trades:
                trap_rate = total_traps / len(adversarial_trades)
            else:
                trap_rate = 0.0
            
            # Calculate volatility-adjusted return
            if adversarial_volatility > 0:
                var_score = adversarial_returns / adversarial_volatility
            else:
                var_score = 0.0
            
            # Calculate drawdown resilience
            if adversarial_max_dd > 0:
                dd_resilience = 1.0 - min(adversarial_max_dd, 1.0)
            else:
                dd_resilience = 1.0
            
            # Calculate consistency score from equity curve
            equity_curve = simulation_results.get("equity_curve", [])
            if len(equity_curve) > 10:
                returns_series = [entry["equity"] for entry in equity_curve]
                returns_changes = np.diff(returns_series)
                consistency = 1.0 - np.std(returns_changes) / (np.mean(np.abs(returns_changes)) + 1e-8)
                consistency = max(0.0, min(1.0, float(consistency)))
            else:
                consistency = 0.5
            
            # Calculate trap resistance (inverse of trap rate)
            trap_resistance = max(0.0, 1.0 - trap_rate)
            
            # Calculate transaction cost efficiency
            pnl_breakdown = simulation_results.get("pnl_breakdown", {})
            gross_pnl = pnl_breakdown.get("gross_pnl", 0.0)
            total_costs = (
                pnl_breakdown.get("commission_costs", 0.0) +
                pnl_breakdown.get("slippage_costs", 0.0) +
                pnl_breakdown.get("spread_costs", 0.0)
            )
            
            if gross_pnl > 0:
                cost_efficiency = 1.0 - min(total_costs / gross_pnl, 1.0)
            else:
                cost_efficiency = 0.0
            
            # Forged: Combine scores with emphasis on adversarial resilience
            robustness_score = (
                var_score * 0.25 +
                dd_resilience * 0.20 +
                consistency * 0.20 +
                trap_resistance * 0.25 +
                cost_efficiency * 0.10
            )
            
            return max(0.0, min(1.0, robustness_score))
            
        except Exception as e:
            logger.error(f"Robustness score calculation failed: {e}")
            return 0.0

    def _calculate_comprehensive_scores(self, fitness_score: FitnessScore, 
                                      regime_results: Dict[str, Dict], 
                                      genome: DecisionGenome):
        """
        v2.0: Calculate comprehensive component scores from all regime results
        
        Args:
            fitness_score: Fitness score to populate
            regime_results: Results from all three regimes
            genome: Genome being evaluated
        """
        
        # Aggregate metrics across all regimes
        all_sortino_ratios = []
        all_calmar_ratios = []
        all_profit_factors = []
        all_consistency_scores = []
        all_robustness_scores = []
        
        total_trades = 0
        total_simulation_time = timedelta(0)
        
        for regime_name, results in regime_results.items():
            if not results:
                continue
                
            # Calculate metrics for this regime
            sortino = self._calculate_sortino_ratio(results)
            calmar = self._calculate_calmar_ratio(results)
            profit_factor = self._calculate_profit_factor(results)
            consistency = self._calculate_consistency_score(results)
            robustness = self._calculate_robustness_score(results)
            
            all_sortino_ratios.append(sortino)
            all_calmar_ratios.append(calmar)
            all_profit_factors.append(profit_factor)
            all_consistency_scores.append(consistency)
            all_robustness_scores.append(robustness)
            
            # Aggregate trade data
            trades = results.get("trades", [])
            total_trades += len(trades)
            
            # Aggregate simulation time
            if "equity_curve" in results and results["equity_curve"]:
                start_time = results["equity_curve"][0]["timestamp"]
                end_time = results["equity_curve"][-1]["timestamp"]
                total_simulation_time += end_time - start_time
        
        # Calculate aggregate scores
        if all_sortino_ratios:
            fitness_score.sortino_ratio = float(np.mean(all_sortino_ratios))
        if all_calmar_ratios:
            fitness_score.calmar_ratio = float(np.mean(all_calmar_ratios))
        if all_profit_factors:
            fitness_score.profit_factor = float(np.mean(all_profit_factors))
        if all_consistency_scores:
            fitness_score.consistency_score = float(np.mean(all_consistency_scores))
        if all_robustness_scores:
            fitness_score.robustness_score = float(np.mean(all_robustness_scores))
        
        # Calculate composite scores
        fitness_score.returns_score = (
            fitness_score.sortino_ratio * 0.4 +
            fitness_score.calmar_ratio * 0.4 +
            fitness_score.profit_factor * 0.2
        )
        
        fitness_score.robustness_score = fitness_score.robustness_score  # Already calculated
        
        # Adaptability score based on performance consistency across regimes
        if len(regime_results) >= 2:
            regime_performances = []
            for results in regime_results.values():
                if results and "equity_curve" in results:
                    equity_curve = results["equity_curve"]
                    if len(equity_curve) >= 2:
                        total_return = (equity_curve[-1]["equity"] - equity_curve[0]["equity"]) / equity_curve[0]["equity"]
                        regime_performances.append(total_return)
            
            if regime_performances:
                # Adaptability = mean performance - std of performance (penalize inconsistency)
                fitness_score.adaptability_score = float(np.mean(regime_performances) - np.std(regime_performances))
        
        # Efficiency score (simplified)
        fitness_score.efficiency_score = 0.5  # Placeholder
        
        # Antifragility score based on adversarial performance
        fitness_score.antifragility_score = fitness_score.robustness_score  # Simplified
        
        # Update metadata
        fitness_score.trades_analyzed = total_trades
        fitness_score.simulation_duration = total_simulation_time

    def _run_simulation(self, genome: DecisionGenome) -> Optional[Dict]:
        """
        Run trading simulation for genome evaluation
        
        Args:
            genome: Genome to simulate
        
        Returns:
            Simulation results dictionary
        """
        
        try:
            # Set up simulation period
            end_time = datetime.now()
            start_time = end_time - timedelta(days=self.evaluation_period_days)
            
            # Initialize components
            simulator = MarketSimulator(self.data_storage, initial_balance=100000.0)
            adversary = AdversarialEngine(difficulty_level=self.adversarial_intensity)
            sensory_cortex = SensoryCortex("EURUSD", self.data_storage)
            
            # Load data
            simulator.load_data("EURUSD", start_time, end_time)
            
            # Calibrate sensory cortex
            calibration_start = start_time - timedelta(days=30)
            sensory_cortex.calibrate(calibration_start, start_time)
            
            # Add adversarial callback
            simulator.add_adversarial_callback(adversary.apply_adversarial_effects)
            
            # Run simulation
            simulation_data: Dict[str, Any] = {
                "trades": [],
                "equity_curve": [],
                "decisions": [],
                "adversarial_events": [],
                "sensory_readings": []
            }
            
            step_count = 0
            max_steps = 10000  # Limit simulation length
            
            while step_count < max_steps:
                # Step simulator
                market_state = simulator.step()
                
                if market_state is None:
                    break
                
                # Get sensory reading
                sensory_reading = sensory_cortex.perceive(market_state)
                simulation_data["sensory_readings"].append(sensory_reading)
                
                # Make decision
                action = genome.decide(sensory_reading)
                
                if action:
                    simulation_data["decisions"].append({
                        "timestamp": market_state.timestamp,
                        "action": action,
                        "sensory_reading": sensory_reading,
                        "market_state": market_state
                    })
                    
                    # Execute action
                    self._execute_action(simulator, action, market_state)
                
                # Record equity
                account_summary = simulator.get_account_summary()
                simulation_data["equity_curve"].append({
                    "timestamp": market_state.timestamp,
                    "equity": account_summary["equity"],
                    "balance": account_summary["balance"],
                    "positions": account_summary["positions"]
                })
                
                # Record adversarial events
                active_events = adversary.get_active_events()
                if active_events:
                    simulation_data["adversarial_events"].extend(active_events)
                
                step_count += 1
            
            # Get final performance stats
            performance_stats = simulator.get_performance_stats()
            simulation_data["performance_stats"] = performance_stats
            simulation_data["trades"] = simulator.trades
            
            return simulation_data
            
        except Exception as e:
            logger.error(f"Simulation failed for genome {genome.genome_id}: {e}")
            return None

    def _execute_action_with_costs(self, simulator: MarketSimulator, action: TradingAction, 
                                  market_state: MarketState) -> Optional[Dict[str, Any]]:
        """
        Forged: Execute trading action with realistic transaction costs
        
        Calculates and applies:
        - Commission costs (configurable rate)
        - Slippage costs (from AdversarialEngine)
        - Spread costs (from market data)
        
        Args:
            simulator: Market simulator instance
            action: Trading action to execute
            market_state: Current market state
        
        Returns:
            Execution result with cost breakdown
        """
        
        try:
            base_quantity = 1000 * action.size_factor
            execution_result = {
                "costs": {
                    "commission": 0.0,
                    "slippage": 0.0,
                    "spread": 0.0,
                    "total": 0.0
                },
                "slippage": {
                    "bps": 0.0,
                    "amount": 0.0
                }
            }
            
            if action.action_type == ActionType.BUY:
                # Calculate execution price with slippage
                base_price = market_state.ask
                slippage_bps = self.slippage_bps + np.random.normal(0, 0.2)  # Add some randomness
                slippage_amount = base_price * slippage_bps / 10000
                execution_price = base_price + slippage_amount
                
                # Calculate costs
                commission_cost = base_quantity * execution_price * self.commission_rate
                spread_cost = base_quantity * (market_state.ask - market_state.bid)
                slippage_cost = base_quantity * slippage_amount
                
                # Place order
                order_id = simulator.place_order(
                    symbol=market_state.symbol,
                    side=OrderSide.BUY,
                    order_type=OrderType.MARKET,
                    quantity=base_quantity
                )
                
                # Record costs
                execution_result["costs"]["commission"] = commission_cost
                execution_result["costs"]["spread"] = spread_cost
                execution_result["costs"]["slippage"] = slippage_cost
                execution_result["costs"]["total"] = commission_cost + spread_cost + slippage_cost
                execution_result["slippage"]["bps"] = slippage_bps
                execution_result["slippage"]["amount"] = slippage_amount
                
            elif action.action_type == ActionType.SELL:
                # Calculate execution price with slippage
                base_price = market_state.bid
                slippage_bps = self.slippage_bps + np.random.normal(0, 0.2)
                slippage_amount = base_price * slippage_bps / 10000
                execution_price = base_price - slippage_amount
                
                # Calculate costs
                commission_cost = base_quantity * execution_price * self.commission_rate
                spread_cost = base_quantity * (market_state.ask - market_state.bid)
                slippage_cost = base_quantity * slippage_amount
                
                # Place order
                order_id = simulator.place_order(
                    symbol=market_state.symbol,
                    side=OrderSide.SELL,
                    order_type=OrderType.MARKET,
                    quantity=base_quantity
                )
                
                # Record costs
                execution_result["costs"]["commission"] = commission_cost
                execution_result["costs"]["spread"] = spread_cost
                execution_result["costs"]["slippage"] = slippage_cost
                execution_result["costs"]["total"] = commission_cost + spread_cost + slippage_cost
                execution_result["slippage"]["bps"] = slippage_bps
                execution_result["slippage"]["amount"] = slippage_amount
                
            elif action.action_type == ActionType.CLOSE:
                # Close all positions with costs
                total_costs = 0.0
                for position in simulator.positions.values():
                    if position.quantity > 0:
                        # Close long position
                        base_price = market_state.bid
                        slippage_bps = self.slippage_bps + np.random.normal(0, 0.2)
                        slippage_amount = base_price * slippage_bps / 10000
                        execution_price = base_price - slippage_amount
                        
                        commission_cost = abs(position.quantity) * execution_price * self.commission_rate
                        spread_cost = abs(position.quantity) * (market_state.ask - market_state.bid)
                        slippage_cost = abs(position.quantity) * slippage_amount
                        
                        total_costs += commission_cost + spread_cost + slippage_cost
                        
                        simulator.place_order(
                            symbol=position.symbol,
                            side=OrderSide.SELL,
                            order_type=OrderType.MARKET,
                            quantity=position.quantity
                        )
                        
                    elif position.quantity < 0:
                        # Close short position
                        base_price = market_state.ask
                        slippage_bps = self.slippage_bps + np.random.normal(0, 0.2)
                        slippage_amount = base_price * slippage_bps / 10000
                        execution_price = base_price + slippage_amount
                        
                        commission_cost = abs(position.quantity) * execution_price * self.commission_rate
                        spread_cost = abs(position.quantity) * (market_state.ask - market_state.bid)
                        slippage_cost = abs(position.quantity) * slippage_amount
                        
                        total_costs += commission_cost + spread_cost + slippage_cost
                        
                        simulator.place_order(
                            symbol=position.symbol,
                            side=OrderSide.BUY,
                            order_type=OrderType.MARKET,
                            quantity=abs(position.quantity)
                        )
                
                execution_result["costs"]["total"] = total_costs
            
            # Other action types (HOLD, SCALE_IN, SCALE_OUT) handled implicitly
            else:
                return None
            
            return execution_result
            
        except Exception as e:
            logger.debug(f"Action execution with costs failed: {e}")
            return None

    def _execute_action(self, simulator: MarketSimulator, action: TradingAction, 
                       market_state: MarketState):
        """Legacy method for backward compatibility"""
        self._execute_action_with_costs(simulator, action, market_state)

    def _calculate_returns_fitness(self, simulation_results: Dict) -> float:
        """Calculate returns-based fitness (25% weight)"""
        
        performance_stats = simulation_results.get("performance_stats", {})
        equity_curve = simulation_results.get("equity_curve", [])
        
        if not equity_curve:
            return -1.0
        
        # Total return
        initial_equity = equity_curve[0]["equity"]
        final_equity = equity_curve[-1]["equity"]
        total_return = (final_equity - initial_equity) / initial_equity
        
        # Sharpe ratio (simplified)
        returns = []
        for i in range(1, len(equity_curve)):
            ret = (equity_curve[i]["equity"] - equity_curve[i-1]["equity"]) / equity_curve[i-1]["equity"]
            returns.append(ret)
        
        if returns and np.std(returns) > 0:
            sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252)
        else:
            sharpe_ratio = 0.0
        
        # Maximum drawdown
        peak = initial_equity
        max_drawdown = 0.0
        
        for point in equity_curve:
            equity = point["equity"]
            if equity > peak:
                peak = equity
            drawdown = (peak - equity) / peak
            max_drawdown = max(max_drawdown, drawdown)
        
        # Combine metrics
        return_score = np.tanh(total_return * 10)  # Scale and bound
        sharpe_score = np.tanh(sharpe_ratio / 2)   # Scale and bound
        drawdown_penalty = -max_drawdown * 2       # Penalty for drawdown
        
        returns_fitness = (return_score * 0.5 + sharpe_score * 0.3 + drawdown_penalty * 0.2)
        
        return np.clip(returns_fitness, -1.0, 1.0)

    def _calculate_robustness_fitness(self, simulation_results: Dict) -> float:
        """Calculate robustness-based fitness (30% weight)"""
        
        equity_curve = simulation_results.get("equity_curve", [])
        adversarial_events = simulation_results.get("adversarial_events", [])
        
        if not equity_curve:
            return -1.0
        
        # Consistency score (low volatility of returns)
        returns = []
        for i in range(1, len(equity_curve)):
            ret = (equity_curve[i]["equity"] - equity_curve[i-1]["equity"]) / equity_curve[i-1]["equity"]
            returns.append(ret)
        
        if returns:
            return_volatility = np.std(returns)
            consistency_score = np.exp(-return_volatility * 100)  # Lower volatility = higher score
        else:
            consistency_score = 0.0
        
        # Performance during adversarial events
        adversarial_performance = 0.0
        if adversarial_events:
            # Simplified: check if equity increased during adversarial periods
            adversarial_periods = len(adversarial_events)
            if adversarial_periods > 0:
                # This is a simplified calculation
                adversarial_performance = min(consistency_score * 1.2, 1.0)
        else:
            adversarial_performance = consistency_score
        
        # Tail risk (performance in worst periods)
        if len(returns) > 10:
            worst_returns = sorted(returns)[:max(1, len(returns) // 10)]  # Worst 10%
            tail_risk_score = 1.0 + np.mean(worst_returns) * 10  # Penalty for bad tail
            tail_risk_score = np.clip(tail_risk_score, 0.0, 1.0)
        else:
            tail_risk_score = 0.5
        
        # Combine robustness metrics
        robustness_fitness = (
            consistency_score * 0.4 +
            adversarial_performance * 0.4 +
            tail_risk_score * 0.2
        )
        
        return np.clip(robustness_fitness, -1.0, 1.0)

    def _calculate_adaptability_fitness(self, simulation_results: Dict, 
                                      genome: DecisionGenome) -> float:
        """Calculate adaptability-based fitness (20% weight)"""
        
        decisions = simulation_results.get("decisions", [])
        
        if not decisions:
            return -1.0
        
        # Decision diversity (uses different parts of decision tree)
        decision_paths = []
        for decision in decisions:
            if "sensory_reading" in decision:
                path = genome.get_decision_path(decision["sensory_reading"])
                decision_paths.append(tuple(path))
        
        if decision_paths:
            unique_paths = len(set(decision_paths))
            total_paths = len(decision_paths)
            diversity_score = unique_paths / max(total_paths, 1)
        else:
            diversity_score = 0.0
        
        # Learning rate proxy (improvement over time)
        equity_curve = simulation_results.get("equity_curve", [])
        if len(equity_curve) > 10:
            # Compare first half vs second half performance
            mid_point = len(equity_curve) // 2
            first_half_return = ((equity_curve[mid_point]["equity"] - equity_curve[0]["equity"]) / 
                               equity_curve[0]["equity"])
            second_half_return = ((equity_curve[-1]["equity"] - equity_curve[mid_point]["equity"]) / 
                                equity_curve[mid_point]["equity"])
            
            learning_score = np.tanh((second_half_return - first_half_return) * 10)
        else:
            learning_score = 0.0
        
        # Regime adaptation (simplified)
        # In a full implementation, this would test performance across different market regimes
        regime_score = diversity_score  # Use diversity as proxy for now
        
        # Combine adaptability metrics
        adaptability_fitness = (
            diversity_score * 0.4 +
            learning_score * 0.3 +
            regime_score * 0.3
        )
        
        return np.clip(adaptability_fitness, -1.0, 1.0)

    def _calculate_efficiency_fitness(self, simulation_results: Dict) -> float:
        """Calculate efficiency-based fitness (15% weight)"""
        
        trades = simulation_results.get("trades", [])
        equity_curve = simulation_results.get("equity_curve", [])
        
        if not trades or not equity_curve:
            return -1.0
        
        # Trade frequency analysis
        simulation_duration = len(equity_curve)  # Number of ticks
        trade_frequency = len(trades) / max(simulation_duration, 1)
        
        # Optimal frequency is neither too high nor too low
        optimal_frequency = 0.01  # 1% of ticks
        frequency_score = np.exp(-abs(trade_frequency - optimal_frequency) * 100)
        
        # Transaction cost impact (simplified)
        if trades:
            # Estimate transaction costs
            total_volume = sum(trade.get("quantity", 0) for trade in trades)
            avg_trade_size = total_volume / len(trades)
            
            # Larger trades are more efficient
            size_efficiency = np.tanh(avg_trade_size / 1000)  # Normalize by base size
        else:
            size_efficiency = 0.0
        
        # Profit per trade
        if trades and equity_curve:
            total_return = ((equity_curve[-1]["equity"] - equity_curve[0]["equity"]) / 
                          equity_curve[0]["equity"])
            profit_per_trade = total_return / len(trades) if len(trades) > 0 else 0.0
            profit_efficiency = np.tanh(profit_per_trade * 100)
        else:
            profit_efficiency = 0.0
        
        # Combine efficiency metrics
        efficiency_fitness = (
            frequency_score * 0.4 +
            size_efficiency * 0.3 +
            profit_efficiency * 0.3
        )
        
        return np.clip(efficiency_fitness, -1.0, 1.0)

    def _calculate_antifragility_fitness(self, simulation_results: Dict) -> float:
        """Calculate antifragility-based fitness (10% weight)"""
        
        adversarial_events = simulation_results.get("adversarial_events", [])
        equity_curve = simulation_results.get("equity_curve", [])
        
        if not equity_curve:
            return -1.0
        
        # Performance during stress events
        if adversarial_events:
            # Simplified: check if organism benefited from adversarial events
            stress_performance = 0.0
            
            # Count how many adversarial events occurred
            event_count = len(adversarial_events)
            
            if event_count > 0:
                # If there were adversarial events and organism survived, give credit
                final_equity = equity_curve[-1]["equity"]
                initial_equity = equity_curve[0]["equity"]
                
                if final_equity > initial_equity:
                    # Organism made profit despite adversarial conditions
                    stress_performance = min(event_count / 10.0, 1.0)  # Scale by event count
                else:
                    stress_performance = -0.5  # Penalty for losses during stress
        else:
            # No adversarial events, neutral score
            stress_performance = 0.0
        
        # Black swan resilience (recovery from large drawdowns)
        peak = equity_curve[0]["equity"]
        max_recovery = 0.0
        
        for i, point in enumerate(equity_curve):
            equity = point["equity"]
            
            if equity > peak:
                peak = equity
            else:
                # In drawdown, check for recovery
                drawdown = (peak - equity) / peak
                
                if drawdown > 0.05:  # Significant drawdown (5%+)
                    # Look for recovery in next periods
                    recovery_window = min(100, len(equity_curve) - i)
                    if recovery_window > 10:
                        future_peak = max(eq["equity"] for eq in equity_curve[i:i+recovery_window])
                        recovery_ratio = (future_peak - equity) / (peak - equity)
                        max_recovery = max(max_recovery, recovery_ratio)
        
        recovery_score = np.tanh(max_recovery)
        
        # Volatility benefit (antifragile systems benefit from volatility)
        returns = []
        for i in range(1, len(equity_curve)):
            ret = (equity_curve[i]["equity"] - equity_curve[i-1]["equity"]) / equity_curve[i-1]["equity"]
            returns.append(ret)
        
        if returns:
            volatility = np.std(returns)
            mean_return = np.mean(returns)
            
            # Antifragile systems should have positive correlation with volatility
            if volatility > 0 and mean_return > 0:
                volatility_benefit = min(float(mean_return / volatility), 1.0)
            else:
                volatility_benefit = 0.0
        else:
            volatility_benefit = 0.0
        
        # Combine antifragility metrics
        antifragility_fitness = (
            stress_performance * 0.5 +
            recovery_score * 0.3 +
            volatility_benefit * 0.2
        )
        
        return np.clip(antifragility_fitness, -1.0, 1.0)

    def _extract_key_metrics(self, simulation_results: Dict, fitness_score: FitnessScore):
        """Extract key metrics for storage in fitness score"""
        
        performance_stats = simulation_results.get("performance_stats", {})
        equity_curve = simulation_results.get("equity_curve", [])
        
        # Extract basic metrics
        fitness_score.total_return = performance_stats.get("total_return", 0.0)
        fitness_score.sharpe_ratio = performance_stats.get("sharpe_ratio", 0.0)
        fitness_score.win_rate = performance_stats.get("win_rate", 0.0)
        
        # Calculate additional metrics
        if equity_curve:
            # Maximum drawdown
            peak = equity_curve[0]["equity"]
            max_drawdown = 0.0
            
            for point in equity_curve:
                equity = point["equity"]
                if equity > peak:
                    peak = equity
                drawdown = (peak - equity) / peak
                max_drawdown = max(max_drawdown, drawdown)
            
            fitness_score.max_drawdown = max_drawdown
            
            # Volatility adjusted return
            returns = []
            for i in range(1, len(equity_curve)):
                ret = (equity_curve[i]["equity"] - equity_curve[i-1]["equity"]) / equity_curve[i-1]["equity"]
                returns.append(ret)
            
            if returns and np.std(returns) > 0:
                fitness_score.volatility_adjusted_return = float(np.mean(returns) / np.std(returns))
        
        # Trade frequency
        trades = simulation_results.get("trades", [])
        if trades and equity_curve:
            fitness_score.trade_frequency = len(trades) / len(equity_curve)

    def get_fitness_distribution(self) -> Dict:
        """Get distribution of fitness scores"""
        
        if not self.evaluation_cache:
            return {}
        
        scores = [score.total_fitness for score in self.evaluation_cache.values()]
        
        return {
            "count": len(scores),
            "mean": np.mean(scores),
            "std": np.std(scores),
            "min": min(scores),
            "max": max(scores),
            "percentiles": {
                "25": np.percentile(scores, 25),
                "50": np.percentile(scores, 50),
                "75": np.percentile(scores, 75),
                "90": np.percentile(scores, 90),
                "95": np.percentile(scores, 95)
            }
        }


# ==============================================================================
# ### Main Execution & Simulation Harness ###
# (The main loop to run experiments)
# ==============================================================================

import sys
import argparse
from pathlib import Path


def run_full_simulation(config: Dict) -> Dict:
    """
    Run complete EMP Proving Ground v2.0 simulation
    
    Args:
        config: Configuration dictionary
    
    Returns:
        Simulation results
    """
    
    logger.info("Starting EMP Proving Ground v2.0 simulation...")
    
    # Extract configuration
    population_size = config.get("population_size", 10)  # Small for demo
    generations = config.get("generations", 3)           # Few generations for demo
    evaluation_days = config.get("evaluation_days", 7)   # Short evaluation period
    adversarial_intensity = config.get("adversarial_intensity", 0.7)
    
    # Initialize components
    logger.info("Initializing v2.0 components...")
    
    # Data storage
    data_storage = TickDataStorage("data")
    
    # Data cleaner and ingestor
    cleaner = TickDataCleaner()
    ingestor = DukascopyIngestor(data_storage, cleaner)
    
    # Generate test data for multiple years (for triathlon testing)
    logger.info("Generating test data for triathlon evaluation...")
    ingestor.ingest_year("EURUSD", 2020)  # Volatile year
    ingestor.ingest_year("EURUSD", 2021)  # Ranging year
    ingestor.ingest_year("EURUSD", 2022)  # Trending year
    
    # v2.0: Fitness evaluator with triathlon testing
    fitness_evaluator = FitnessEvaluator(
        data_storage=data_storage,
        evaluation_period_days=evaluation_days,
        adversarial_intensity=adversarial_intensity
    )
    
    # Evolution configuration
    evolution_config = EvolutionConfig(
        population_size=population_size,
        elite_ratio=0.2,
        crossover_ratio=0.5,
        mutation_ratio=0.3,
        mutation_rate=0.1
    )
    
    # Evolution engine
    evolution_engine = EvolutionEngine(evolution_config, fitness_evaluator)
    
    # Initialize population
    logger.info(f"Initializing population of {population_size} genomes...")
    evolution_engine.initialize_population(seed=42)
    
    # Evolution loop
    results = {
        "config": config,
        "generation_stats": [],
        "best_genomes": [],
        "fitness_distribution": {}
    }
    
    for generation in range(generations):
        logger.info(f"Evolving generation {generation + 1}/{generations}...")
        
        # Evolve one generation
        gen_stats = evolution_engine.evolve_generation()
        results["generation_stats"].append(gen_stats)
        
        # Log progress
        logger.info(f"Generation {generation + 1} completed:")
        logger.info(f"  Best fitness: {gen_stats.best_fitness:.4f}")
        logger.info(f"  Average fitness: {gen_stats.average_fitness:.4f}")
        logger.info(f"  Population diversity: {gen_stats.diversity_score:.4f}")
        
        # Get best genomes
        if generation == generations - 1:  # Last generation
            best_genomes = evolution_engine.get_best_genomes(5)
            results["best_genomes"] = [genome.to_dict() for genome in best_genomes]
    
    # Final statistics
    results["fitness_distribution"] = fitness_evaluator.get_fitness_distribution()
    results["population_summary"] = evolution_engine.get_population_summary()
    
    logger.info("EMP Proving Ground simulation completed!")
    logger.info(f"Final best fitness: {results['generation_stats'][-1].best_fitness:.4f}")
    
    return results


def main():
    """Main entry point"""
    
    parser = argparse.ArgumentParser(description="EMP Proving Ground - Unified v2.0")
    parser.add_argument("--population-size", type=int, default=10, 
                       help="Population size for evolution")
    parser.add_argument("--generations", type=int, default=3,
                       help="Number of generations to evolve")
    parser.add_argument("--evaluation-days", type=int, default=7,
                       help="Days of data for fitness evaluation")
    parser.add_argument("--adversarial-intensity", type=float, default=0.7,
                       help="Adversarial testing intensity (0.0-1.0)")
    parser.add_argument("--output-dir", type=str, default="results",
                       help="Output directory for results")
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed for reproducibility")
    
    args = parser.parse_args()
    
    # Set random seed
    random.seed(args.seed)
    np.random.seed(args.seed)
    
    # Create configuration
    config = {
        "population_size": args.population_size,
        "generations": args.generations,
        "evaluation_days": args.evaluation_days,
        "adversarial_intensity": args.adversarial_intensity,
        "seed": args.seed
    }
    
    # Run simulation
    try:
        results = run_full_simulation(config)
        
        # Save results
        output_dir = Path(args.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        import json
        
        # Convert datetime objects to strings for JSON serialization
        def serialize_datetime(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, timedelta):
                return str(obj)
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
        
        output_file = output_dir / f"emp_simulation_results_{int(time.time())}.json"
        with open(output_file, "w") as f:
            json.dump(results, f, indent=2, default=serialize_datetime)
        
        logger.info(f"Results saved to {output_file}")
        
        # Print summary
        print("\n" + "="*60)
        print("EMP PROVING GROUND v2.0 SIMULATION COMPLETE")
        print("="*60)
        print(f"Population Size: {config['population_size']}")
        print(f"Generations: {config['generations']}")
        print(f"Evaluation Period: {config['evaluation_days']} days")
        print(f"Adversarial Intensity: {config['adversarial_intensity']:.1f}")
        print(f"Final Best Fitness: {results['generation_stats'][-1].best_fitness:.4f}")
        print("\nv2.0 Features:")
        print(" Intelligent Stop Hunting with Liquidity Zone Detection")
        print(" Breakout Trap Spoofing with Consolidation Analysis")
        print(" Triathlon Evaluation Across Three Market Regimes")
        print(" Multi-Objective Fitness (Sortino, Calmar, Profit Factor)")
        print(" Anti-Overfitting Penalty for Regime Inconsistency")
        print(f"Results saved to: {output_file}")
        print("="*60)
        
    except Exception as e:
        logger.error(f"Simulation failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

