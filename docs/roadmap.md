# AlphaTrade Gap Analysis and Roadmap

## Gap Analysis Report

This section compares the current emp_proving_ground_v1 codebase against the envisioned AlphaTrade architecture. The AlphaTrade blueprint organizes the system into five major components – Perception, Adaptation, Reflection, Risk/Execution, and Governance – each corresponding to a stage in the intelligent trading loop. Below, we identify for each component what is already implemented, what is partially in place, what is missing, and any misalignments between the blueprint and the current implementation.

### Milestone: Reflection Intelligence (TRM) – Phase 1

| Task | Description | Definition of Done |
| --- | --- | --- |
| RIM Design Sign-off | Finalize TRM-based architecture, interfaces, and safety plan. | ADR approved; design doc with diagrams and checklists merged. |
| Tooling & Schemas | Ship CLI stubs, JSON Schemas, config template, and validation tests. | `tools/rim_shadow_run.py`, `tools/rim_validate.py`, `interfaces/rim_types.json`, and tests green in CI. |
| Shadow Observability | Capture telemetry and publish sample artifacts for governance rehearsal. | Shadow run emits JSONL suggestions + telemetry logs; `make rim-validate` passes. |
| Governance Integration Prep | Document governance queue handoff and runbook. | API doc + troubleshooting guide available; governance reviewers acknowledge ingestion flow. |

**Phase Exit Criteria:** Shadow-mode RIM produces audited suggestions, telemetry, and governance-ready documentation without impacting live trading.

**Documentation & Ops Readiness:** The dedicated TRM design dossier now captures the runner’s architecture, recursive loop sequencing, data contracts, interfaces, telemetry, and safety controls so the milestone has an authoritative blueprint to work from.【F:docs/design/trm_reflection_design.md†L1-L179】 Operations inherit a troubleshooting runbook that walks through shadow bring-up, validation, retention hygiene, and common failure modes, ensuring the roadmap’s governance hand-off task is actionable rather than aspirational.【F:docs/runbooks/reflection_troubleshooting.md†L3-L41】 Configuration templates and sample suggestion artifacts round out the evidence pack, giving reviewers concrete defaults (`rim.config.example.yml`) and JSONL examples to reference while wiring downstream automation.【F:config/reflection/rim.config.example.yml†L1-L15】【F:docs/examples/rim_suggestion_examples.jsonl†L1-L3】

### Perception (Sensory Ingest & Belief Formation)

**Implemented:** The code establishes a layered perception pipeline that ingests sensory data (market signals) into an internal belief state. A BeliefState model is in place to represent posterior beliefs and regime context, including Hebbian-style updates to capture recent patterns[1]. A sensory cortex framework exists with a “real_sensory_organ” module that fuses multiple signal types (WHAT/WHEN/WHY/HOW/ANOMALY) into unified sensory payloads[2]. The belief/regime scaffolding is functional: it buffers sensor inputs, emits finite-state regime indicators, and enforces positive semi-definite (PSD) covariance updates as a stability check[3]. Core architecture for perception reflects the encyclopedia’s layered design (core → sensory → thinking…)[4].
**Partially Implemented:** Many sensory inputs are still mock or synthetic. The real-time data ingest is not fully wired – currently relying on placeholders rather than live market feeds[5][4]. Key sensory organs now execute deterministic logic – HOW clamps telemetry, WHAT/WHEN/WHY emit lineage and quality metadata, and ANOMALY wraps a z-score based `BasicAnomalyDetector` with regression tests – yet they still operate on synthetic frames until live ingest is available. Lineage and telemetry for sensory signals are now exercised by guardrail suites (e.g. sensor lineage/quality tests for the primary dimensions and anomaly detector), and the default WHAT/WHEN/WHY fallbacks now emit the same audit payload (source, quality, lineage) so diaries retain provenance even when raw market data is missing.【src/sensory/what/what_sensor.py:179】【src/sensory/when/when_sensor.py:256】【src/sensory/why/why_sensor.py:226】【tests/sensory/test_primary_dimension_sensors.py:119】【src/sensory/anomaly/basic_detector.py:1】【tests/sensory/test_basic_anomaly_detector.py:1】 The fused real sensory organ mirrors those guarantees when upstream sensors return nothing by synthesising fallback payloads with lineage + quality metadata and regression coverage proving diaries keep the audit context even when inputs are empty.【src/sensory/real_sensory_organ.py:276】【tests/sensory/test_real_sensory_organ.py:276】 Drift detection is present (the Drift Sentry monitors statistical drift in beliefs) but depends on synthetic data and has not been exercised on real streaming input[2]. Executable organs now normalise numeric signal strengths across value payloads and metadata, scanning canonical and fallback keys while discarding non-finite readings so vendor renames stop zeroing strengths, with regression coverage asserting the WHAT organ preserves pattern strength in emitted snapshots.【F:src/sensory/organs/dimensions/executable_organs.py†L72-L112】【F:tests/sensory/test_dimension_organs.py†L204-L206】
Integration tooling now ingests real EURUSD slices from either tracked CSV fixtures or provider-backed downloads into Timescale, hydrates the managed caches via `RealDataManager`, and drives the `RealSensoryOrgan` to emit belief snapshots—proving the operational backbone on live-formatted data rather than purely synthetic fixtures.【F:src/data_integration/real_data_slice.py†L109-L196】【F:tests/integration/test_real_data_slice_ingest.py†L12-L89】【F:tests/data_integration/test_real_data_manager.py†L83-L198】 `RealDataSliceConfig` enforces a single ingest source, `ingest_daily_slice_from_provider` reuses the Timescale orchestrator for remote fetches, and the CLI now accepts `--provider` to bypass CSV defaults while validating symbols and source labelling for sandbox drills.【F:src/data_integration/real_data_slice.py†L161-L196】【F:tools/data_ingest/run_real_data_slice.py†L45-L125】 A dedicated operational-backbone CLI orchestrates the supervised Timescale→Redis→Kafka→sensory pipeline end to end, renders JSON/Markdown summaries, and mirrors the runbook guidance so operators can generate reproducible evidence bundles without bespoke scripts, while the updated real-data slice helper remains available for fixture-scale or provider-backed drills.【F:tools/data_ingest/run_operational_backbone.py†L1-L378】【F:tests/tools/test_run_operational_backbone.py†L17-L105】【F:tools/data_ingest/run_real_data_slice.py†L45-L125】【F:docs/runbooks/data_foundation.md†L171-L234】 The pipeline now bootstraps attached `LiveBeliefManager` and `UnderstandingRouter` components so rehearsals surface belief/regime snapshots, understanding decisions, and ingest-failure telemetry even when Timescale fetches fall back to cached data, satisfying the roadmap’s understanding-loop failover acceptance criteria.【F:src/data_foundation/pipelines/operational_backbone.py†L82-L366】【F:tools/data_ingest/run_operational_backbone.py†L327-L378】【F:tests/integration/test_operational_backbone_pipeline.py†L198-L295】 Latest iteration also exports TaskSupervisor rosters and captured streaming snapshots so evidence bundles include supervised workload states and live sensory payloads, with CLI Markdown/JSON now surfacing `task_supervision`/`streaming_snapshots` blocks and tests asserting supervisor cancellation wiring for the rehearsal path.【F:src/data_foundation/pipelines/operational_backbone.py†L120-L566】【F:tools/data_ingest/run_operational_backbone.py†L400-L520】【F:tests/tools/test_run_operational_backbone.py†L71-L150】
Real-data slices now calibrate both belief covariance and regime thresholds by extracting price series, invoking the shared calibration helpers, and emitting `RegimeSignal` payloads alongside the `BeliefState`; fallback logging handles sparse or malformed data so operators can still ingest partial frames. Tests assert that calibration metadata, scoped volatility scaling, and regime FSM updates materialise in the emitted snapshots, anchoring the roadmap’s requirement for live-data grounded regime intelligence.【F:src/data_integration/real_data_slice.py†L51-L347】【F:src/understanding/belief.py†L35-L248】【F:tests/data_integration/test_real_data_slice_belief.py†L1-L208】
Contextual fusion entry points now standardise on the `analyze_market_understanding` coroutine while the legacy `enhanced_intelligence_engine` shim has been removed in favour of an explicit failure that directs callers to the understanding namespace. The Synthesis contract no longer exposes the deprecated `intelligence_level` alias, keeping downstream code on the canonical `understanding_level` attribute; regression coverage asserts the alias is gone and the sensory README mirrors the rename so telemetry, docs, and demos converge on the understanding-first surface without lingering terminology drift.【F:src/orchestration/enhanced_understanding_engine.py†L63-L73】【F:src/orchestration/enhanced_intelligence_engine.py†L1-L17】【F:src/sensory/tests/test_integration.py†L279-L299】【F:tests/current/test_mi_to_sensory_forwarding.py†L19-L60】【F:src/sensory/README.md†L153-L162】
**Missing:** Production-grade backbone operations still need hardened, always-on Timescale/Redis/Kafka clusters plus real exchange feeds, even though the default connectors now derive credentials from governance extras and publish ingest telemetry through the managed cache/stream pipeline.【F:src/data_integration/real_data_integration.py†L107-L200】【F:tests/data_integration/test_real_data_manager.py†L83-L198】 The system still requires sustained live market ingest (e.g. price feeds, order book updates) feeding the sensory organ. Robust anomaly detection (beyond the new basic z-score detector) and explanatory diagnostics (the WHY organ) still need live data calibration and richer models.[2] Also missing is the continuous calibration of sensors: e.g. adjusting to new instrument data, handling missing data, etc. Without these, the perception layer cannot fully reflect the blueprint’s vision of a rich sensory cortex driving the loop.
**Misalignments:** The AlphaTrade vision calls for a rich, real-time sensory cortex with multiple specialized detectors and trustworthy signals; currently, the code’s perception layer is conceptually aligned but functionally limited to scaffolding[2]. In particular, the blueprint’s emphasis on executable organs and fused signals is only partially realized – the code structure is there, but actual data quality, validation, and anomaly logic are lacking. This means the spirit of Perception (delivering reliable, validated market facts to downstream components) isn’t yet met in practice. The design itself is sound (no major structural misalignment), but the implementation is incomplete, leaving a gap to close before Perception can fully anchor the loop.

### Adaptation (Policy Routing & Fast-Weight Learning)

**Implemented:** The code implements an Understanding Router and a Policy Router that together realize the Adaptation stage of AlphaTrade. The UnderstandingRouter ingests belief snapshots and routes them to a chosen strategy/tactic, functioning as the decision-making “brain” that selects an action or intent[6][7]. Crucially, fast-weight adaptation is integrated: the router supports Hebbian fast-weight updates – short-term adjustments to strategy preferences based on recent evidence[8]. This means the system can amplify or dampen certain tactics dynamically (“neurons that fire together wire together”), consistent with the BDH-inspired fast-weight principle. The PolicyRouter in the thinking layer tracks strategy metadata (objectives, tags) and manages experiment lifecycles, allowing new tactics to be registered and providing reflection data on their performance[9][10]. The overall Adaptation loop (Perception → Adaptation → Reflection) is coded in the AlphaTradeLoopOrchestrator, which ties router decisions to drift checks and governance gating[11][6]. In summary, the adaptive decision-making framework – including configurable strategy routing and fast-weight toggles – is present and anchoring the system’s logic.
**Partially Implemented:** While the routing and fast-weight mechanics exist, the broader Evolution Engine (adaptive intelligence that generates or mutates strategies) remains mostly scaffolding. The alignment plan envisioned an “institutional genome catalogue” and pipeline for evolving new strategies[12], but in code this is limited. The current adaptation relies on predefined strategies and simple heuristic adjustments. Evolution plumbing is incrementally improving: the `EvolutionManager` now seeds trial queues with catalogue-backed variants, preserves lineage metadata, and degrades base weights once loss streaks trip configurable thresholds, so adaptive runs can rotate through real catalogue definitions instead of static fallbacks.【src/thinking/adaptation/evolution_manager.py:70】【tests/thinking/test_evolution_manager.py:60】 When those queues empty, the manager can auto-synthesise lightweight parameter mutations that scale, offset, or clamp numeric parameters and label the variants with governance-friendly identifiers under regression coverage of the mutated weights.【src/thinking/adaptation/evolution_manager.py:207】【tests/thinking/test_evolution_manager.py:187】 Each adaptation now emits an `EvolutionAdaptationResult` summarising the base tactic, observed win-rate, and concrete actions; the AlphaTrade loop merges this payload into iteration metadata and the decision diary so governance reviewers see variant registrations or degradations inline with the trade record.【src/thinking/adaptation/evolution_manager.py:104】【src/orchestration/alpha_trade_loop.py:299】【tests/orchestration/test_alpha_trade_loop.py:485】 Fast-weight experiments are still feature-flagged so adaptive learning is not yet default[12], but the constraint layer is maturing: the refreshed `FastWeightController` tracks inhibitory versus suppressed multipliers, exposes sparsity metrics, and honours configuration overrides parsed from `SystemConfig` extras (baseline, activation threshold, excitatory-only, max-active fraction) so operators can enforce sparse positive activations without code changes.【src/thinking/adaptation/fast_weights.py:21】【src/thinking/adaptation/fast_weights.py:217】【src/understanding/router.py:205】【tests/thinking/test_fast_weights.py:13】【tests/understanding/test_understanding_router.py:216】 Router decisions persist the metrics alongside fast-weight multipliers so reflection digests display activation percentages and participating tactics under regression coverage.【src/thinking/adaptation/policy_router.py:323】【tests/thinking/test_policy_router.py:166】 UnderstandingRouter threads the sparsity payload into decision bundles, and guards against external overrides when fast weights are disabled, keeping sandboxed scripts from skewing routing decisions.【src/understanding/router.py:244】【tests/understanding/test_understanding_router.py:210】 In short, the adaptation layer has the basic fast-weight loop and diary-integrated adaptation summaries but is not yet the full intelligent, self-evolving system described in the blueprint. Sentient memory now exposes an in-memory fallback when the optional `faiss` module is absent, persisting vectors/metadata through the same API so adaptive experiments and decision replays continue to function in lightweight environments while warning operators about the degraded mode.【src/sentient/memory/faiss_pattern_memory.py:18】 Canonical ecosystem surfaces now ship a lightweight `SpeciesManager` with deterministic specialist profiles, history/introspection helpers, and guardrail tests, letting Phase 3 orchestration depend on `src.ecosystem` namespaces instead of legacy thinking shims while integration stubs map old imports onto the canonical package for migration safety.【F:src/ecosystem/species/species_manager.py†L1-L96】【F:tests/ecosystem/test_species_manager.py†L1-L40】【F:tests/util/orchestration_stubs.py†L139-L169】【F:src/thinking/phase3_orchestrator.py†L28-L35】
**Missing:** The “Evolution Engine” is still early. The system has no mechanism to create entirely new strategies or significantly alter algorithms based on performance – there is no genetic algorithm or gradient descent updating the strategy set. The new parameter mutation path covers only simple numeric scaling/clamping rules and still lacks telemetry-informed heuristics, multi-parameter exploration, or crossover across tactics.【F:src/thinking/adaptation/evolution_manager.py†L207-L269】 Advanced adaptation features like long-term memory of successful patterns, or automatic hyperparameter tuning of strategies over time, are not implemented. Moreover, the blueprint’s notion of adaptation includes mutating against real data feeds[14] – since real feeds aren’t hooked up yet (Perception gap), the adaptation cannot truly learn from live market behavior. Sparse positive activations (ensuring that only a small fraction of strategy “neurons” activate at a time, and that activations are non-negative) are not enforced by any module – this could be a design adjustment needed to align with BDH theory. In essence, the system does not yet learn new trading tactics or significantly improve existing ones on its own; it can only tweak weightings of pre-programmed tactics.
**Misalignments:** Conceptually, the code aligns with the blueprint’s Adaptation loop – it has a router with fast-weight updates and the idea of evolving strategy preferences. However, there is a gap between the aspirational adaptive behavior (a rich evolution of strategies) and the current simplistic implementation. The BDH-inspired elements (fast weights with enforced sparsity) are now present, but others (graph-based reasoning dynamics, true evolutionary mutation) are not explicitly present beyond basic data structures. The blueprint expects Adaptation to be highly dynamic and data-driven, whereas the current state is rule-based and limited. No fundamental architecture changes are needed (the scaffolding is in place), but significant development is required to realize the blueprint’s vision of an autonomous, learning “evolution engine” driving this component[13].

### Reflection (Decision Diaries & Learning Feedback)

**Implemented:** The system includes a Reflection stage that records decisions and generates insights from them. A Decision Diary mechanism is implemented – every AlphaTrade loop iteration produces a DecisionDiaryEntry that logs the context, chosen strategy, outcomes, and reasoning notes[15][16]. These diaries are persisted via a DecisionDiaryStore and serve as an auditable trail of “why was this decision made.” The code also provides a PolicyReflectionBuilder which compiles recent decision records into Reflection Artifacts[17][18]. These artifacts summarize emerging tactics, their performance, first/last seen times, and any gating (e.g. if a strategy was forced to paper trading)[10]. In effect, the system can produce a reflection digest for reviewers or automated analysis, so that over a window of time one can see which strategies are gaining or losing favor and why. There’s also a graph diagnostics tool that visualizes the understanding loop (sensory → belief → router → policy) as a DAG, helping reflect on the decision pipeline structure[19]. The presence of these features means the Reflection component – capturing experience and providing data for learning – is acknowledged in the codebase and partially functional. The blueprint’s intent for an auditable reasoning loop is met at least to the extent that every decision is transparently recorded with metadata[20].
**Partially Implemented:** The diaries and reflection summaries exist, but using them for learning feedback is limited. Currently, reflection artifacts are primarily for human governance review (e.g. seeing evidence for strategy promotions) rather than automatically adjusting the system. The fast-weight adaptation loop does not yet incorporate long-term feedback from the diaries – e.g. there is no mechanism like “if a strategy consistently underperforms as seen in diaries, automatically downweight or remove it.” Instead, such adjustments would still be manual (via governance CLI). Some aspects of reflection that the blueprint envisions, like “sigma stability checkpoints” (monitoring the stability of belief updates over time) and other health metrics, are only partly realized via tests (ensuring covariance matrices remain PSD, etc.)[21]. The new `StrategyPerformanceTracker` aggregates ROI, win/loss, and drift loop metrics for each strategy under guardrail coverage, but the resulting reports are still consumed manually – nothing reweights strategies automatically based on those insights.【src/operations/strategy_performance_tracker.py:1】【tests/operations/test_strategy_performance_tracker.py:1】 Interpretability is another partial area: the blueprint highlights interpretability of state and reasoning; the code provides raw data (diaries, graph dumps) but no higher-level analysis like highlighting which “concept synapses” were most active (a nod to BDH’s interpretability). Exported understanding graphs now include fast-weight utilisation, sparsity, and dominant-strategy summaries to highlight overused or idle adapters, yet these metrics come from synthetic decision windows rather than live feedback streams.【F:src/understanding/diagnostics.py†L621-L958】【F:tests/understanding/test_understanding_diagnostics.py†L15-L39】 Router summaries now attach the refreshed fast-weight metrics payload (active/dormant identifiers, activation percentages, sparsity ratios, extrema) alongside multiplier breakdowns so reviewers can see exactly which tactics were boosted, but the data still stems from synthetic exercises.【F:src/thinking/adaptation/policy_router.py†L320-L412】【F:tests/thinking/test_policy_router.py†L121-L176】 The building blocks for reflection are there, but the feedback loop is not closed – insights are gathered but not yet used to automatically tune the system.
**Missing:** Automated post-analysis and learning from the decision records are missing. For example, there’s no module performing trend analysis on the diary entries (e.g. detecting that “Strategy A only works in regime X” or “Strategy B’s performance is deteriorating”). The blueprint suggests a system that can reflect and self-correct; currently, any such reflection-driven changes must be done by a developer or analyst examining the logs. Also missing are formal acceptance tests (validation hooks) for the reflection outputs – while there are some tests (ensuring the diary CLI and reflection builder run[19]), the criteria for a “good” reflection (e.g. it correctly identifies new emerging strategies or anomalies in decisions) are not automated. Graph health metrics now emit utilisation, sparsity, and dominance summaries but still rely on curated synthetic scenarios; the system is not yet feeding real historical windows into the diagnostics or alerting when thresholds are breached, so reviewers must manually interpret the numbers.【src/understanding/diagnostics.py†L621-L958】【tests/tools/test_understanding_graph_cli.py†L8-L19】 In summary, Reflection currently records but does not learn; the system lacks an implementation of “reflection as a teacher” to the adaptation process.
**Misalignments:** The architecture for Reflection is in line with the blueprint – the idea of diaries and reflection artifacts matches the vision of an introspective system. The misalignment lies in purpose and depth: the blueprint (inspired by BDH and similar cognitive architectures) implies that reflection should influence the system’s behavior (closing the loop with adaptation), whereas in the current code reflection is passive (for audit/compliance). Another subtle misalignment is in the metric-driven reflection: the blueprint’s emphasis on metrics like “graph sparsity” or activation patterns for interpretability is only partially reflected – the router now records fast-weight sparsity metrics, but concept-level analysis is still absent.【F:src/thinking/adaptation/policy_router.py†L320-L412】 Addressing these gaps will bring Reflection from a compliance log towards a true learning mechanism.

### Risk/Execution (Risk Management and Trade Execution)

**Implemented:** The codebase contains foundational elements of risk management and execution control. A DriftSentryGate is implemented to act as a risk gate on execution – it evaluates each proposed trade against drift metrics and confidence thresholds, potentially flipping a force_paper flag to prevent live execution if conditions look anomalous[23]. Risk policies (like leverage and exposure limits) are defined in configurations and there are tests ensuring warnings trigger before limits are breached[24]. An execution release router exists to decide how orders are routed (e.g. to paper trading vs live, based on the drift gate’s decision and policy stage)[23]. The system also includes a portfolio monitor for tracking positions and P&L, albeit in a basic form, and an execution readiness journal that logs whether services (like data feeds or brokers) are up before trading[25]. These show that the scaffolding for execution – the order lifecycle and risk checks – mirrors the encyclopedia’s prescribed order of operations[26]. Additionally, compliance telemetry hooks are present (audit logs, incident response stubs) indicating the system’s awareness of compliance needs. In short, the codebase has risk check structures and toggles to ensure that trades are gated by risk evaluations and that execution can be toggled between simulation and real mode.
**Partially Implemented:** Actual trade execution is still running on “paper.” The trading and execution modules operate on simulated orders and mock broker interfaces – there is no live broker integration or order routing to markets yet[4]. Risk enforcement is described as “hollow” in assessments[4]: while limits exist on paper, the system doesn’t yet connect to real capital constraints (e.g. it’s not hooked to an account to truly prevent an order). Recent hardening ensures the risk manager now returns `0.0` when no risk budget or minimum lot is available so orchestration stops before oversizing depleted accounts, but those checks still protect only simulated balances.【F:src/risk/risk_manager_impl.py†L220-L332】【F:tests/risk/test_risk_manager_impl_additional.py†L7-L102】 Canonical symbol handling now normalises and migrates tracked positions to uppercase keys, deduplicates aggregate risk calculations, and surfaces consistent telemetry so audits receive a single exposure view regardless of legacy casing.【F:src/risk/risk_manager_impl.py†L246-L308】【F:tests/risk/test_risk_manager_impl_additional.py†L200-L250】 Some risk checks (like those for position sizing or portfolio diversification) may not be fully implemented beyond placeholders. Async task supervision has matured – the TaskSupervisor now restarts failed workloads with bounded backoff, `RuntimeApplication.task_snapshots()` exposes the active ingest/trading tasks for operators and guardrails, and the `_supervise_background_task` helper plus the new `create_background_task`/`register_background_task` APIs keep ad-hoc coroutines under managed supervision instead of leaking `asyncio.create_task`, yet legacy entrypoints still need to migrate to the unified path before the protection is universal.【F:src/runtime/task_supervisor.py†L1-L204】【F:src/runtime/runtime_builder.py†L298-L326】【F:src/runtime/runtime_builder.py†L896-L918】【F:tests/runtime/test_task_supervisor.py†L63-L138】【F:tests/runtime/test_runtime_builder.py†L1952-L2009】 Compliance monitoring (e.g. checking regulatory rules or logging trade data for compliance) is minimal: there are audit log structures, but no active enforcement beyond risk limits. In summary, the mechanics to simulate trading are there and the safety switches exist, but real execution capabilities and robust risk responses are not yet complete.
A dedicated PaperBrokerExecutionAdapter now bridges release-aware execution into the FIX pilot, capturing deterministic risk context and the last broker error snapshot on every submission under regression coverage, and the runtime can optionally attach a REST paper-trading bridge via `PaperTradingApiAdapter` when extras are provided, with bootstrap cleanup hooks, failure retries, and adapter tests exercising real HTTP round-trips; the stack still stops at paper credentials rather than live exchanges.【src/trading/execution/paper_broker_adapter.py†L1-L310】【src/orchestration/bootstrap_stack.py†L418-L462】【src/trading/integration/paper_trading_api.py†L1-L189】【src/runtime/predator_app.py†L2087-L2149】【tests/integration/test_paper_trading_simulation.py†L1-L192】【tests/runtime/test_bootstrap_paper_broker.py†L1-L147】
**Missing:** Key pieces missing include production integration for execution – for example, connections to brokerage APIs or trading exchanges are not implemented, so the system cannot place a real order. Also missing is risk-based position sizing: the blueprint expects that given a strategy signal, the system calculates an optimal position size under risk limits, but currently there is “no risk sizing” at all[4]. The institutional risk and compliance layer is incomplete – features like pre-trade compliance checks, post-trade reconciliation, or regulatory audit trail generation are absent. Ops readiness items like stress tests or kill-switches for the trading engine need to be added to match the blueprint’s focus on safety (some incident response hooks exist, but likely not end-to-end). Additionally, the blueprint calls for expanding broker coverage after internal gates are solid[27], implying that multi-broker or multi-exchange handling is on the roadmap but currently missing. Finally, while the system can force trades to paper mode, a robust policy enforcement that only allows fully approved strategies to execute live is not yet guaranteed (it depends on humans using the CLI to promote stages). In essence, AlphaTrade cannot yet execute a real trade in production with full confidence – the pipeline from decision to execution is incomplete.
**Misalignments:** The major misalignment is that the current system is still a simulation framework, whereas the AlphaTrade blueprint assumes a trajectory toward a real trading platform with enforceable risk controls. The architecture is on the right track, but practically, capital is not at risk because the system isn’t ready to handle real capital[28]. For instance, the blueprint emphasizes deterministic risk APIs and policy breach telemetry[29] – the code has placeholders for these, but until actual execution is attempted, it’s unclear how effective they are. There’s also a structural note: risk management in the code has been refactored (old risk modules deprecated in favor of a canonical core risk module)[4][30], which aligns with the blueprint’s intent to have a single source of truth for risk. However, until the execution layer is fully functional, the risk management can’t be truly battle-tested. In summary, the design largely aligns with the envisioned risk/execution loop (no major redesign needed), but there is a significant gap in implementation maturity.

### Governance (Policy Governance & Promotion Process)

**Implemented:** A robust Governance layer is present to oversee which strategies can trade and under what conditions. The system includes a Policy Ledger (in src/governance) that records each strategy (“policy”) and its approval stage – e.g. experimental, paper-trade, or live[31][32]. The ledger persists promotion history, approvals, threshold overrides, and links to decision diary evidence for each promotion[33]. On top of this, a suite of governance CLI tools is implemented: for example, rebuild_policy to regenerate risk config from the ledger, promote_policy to approve a strategy’s next stage with proper sign-offs, and alpha_trade_graduation to batch-promote strategies that meet all criteria[34][35]. These tools ensure that any strategy going from backtest to paper or paper to live is traceable and auditable. The AlphaTrade loop orchestrator itself uses the ledger: it queries the LedgerReleaseManager to fetch the current release stage and risk thresholds for the chosen policy, and uses that to enforce stage-appropriate gates (for example, ensuring a strategy in “paper” stage cannot execute real trades, by activating the force_paper flag)[36][37]. Recent hardening introduced filesystem locks and atomic writes around ledger persistence together with multi-approver enforcement, promotion logging, and runbook-linked summaries in the CLI, keeping governance state consistent even when multiple operators promote simultaneously.【src/governance/policy_ledger.py:260】【tools/governance/promote_policy.py:122】【tools/governance/_promotion_helpers.py:13】【tests/tools/test_promote_policy_cli.py:1】 Overall, the governance processes and data structures described in the blueprint (promotion gates, evidence-backed approvals, deterministic governance metadata on each decision) are present and functioning in the code.
**Partially Implemented:** The governance features exist, but their integration into day-to-day operations is partial. Enforcement of governance policies relies on the developers/operators running the CLI tools and monitoring outputs – there isn’t a live UI or automated daemon that, for example, halts an unapproved strategy (though the architecture would allow it via the ledger checks). Some governance checks are enforced in code (like requiring a decision diary entry ID when promoting a policy, to ensure evidence is cited[38]), but others may be more procedural (outside the code’s automatic handling). Recent hardening trims and normalises evidence identifiers before they persist to history, rejects whitespace-only submissions, and raises deterministic errors that test suites exercise, reducing the manual clean-up needed when operators promote tactics.【src/governance/policy_ledger.py:127】【tests/governance/test_policy_ledger.py:147】【tests/governance/test_policy_ledger_locking.py:51】 The governance telemetry (dashboards showing how many strategies at each stage, any pending approvals, etc.) is not fully developed – observability panels exist for the understanding loop and drift, but governance info might only be in logs or JSON manifests. Additionally, while the ledger captures threshold overrides per strategy, the system still needs human input to decide those overrides; there’s no AI deciding “this strategy should have tighter risk limits” – governance in that sense is manual. The blueprint’s notion of deterministic promotion gates is implemented at a basic level (stages in ledger), but dynamic policy enforcement (like auto-downgrading a strategy if it triggers too many alerts) is not implemented. In summary, governance is structurally in place but not yet a “hands-off” autonomous module – it provides tools for humans to govern the system.
**Missing:** A few things are missing to fully realize the governance vision. Real-time governance monitoring – e.g. a continuously running process or dashboard that flags when a strategy is eligible for promotion or needs demotion – is not present. Integration with enterprise governance (approvals via UI, or embedding in a larger workflow system) is beyond the current scope. Also, policy documentation and rationales might need to be auto-generated for each promotion (currently, one must read the diaries and ledger entries manually). The blueprint’s focus on compliance telemetry suggests that every governance action should be visible and testable; while the ledger provides data, the system lacks a user-friendly presentation of compliance status (e.g. “All strategies trading live have passed X criteria”). Another missing piece is Governance of configuration: ensuring any config changes go through similar review (the current ledger is strategy-focused). Finally, as a future feature, one could imagine machine-supported governance (ML suggestions for promotions or flagging anomalies in strategy performance for review) – needless to say, this is not yet implemented. Essentially, the governance is policy-driven but not yet intelligent or fully automated.
**Misalignments:** The code’s governance approach aligns well with the blueprint’s intent: it provides structured, auditable control over system behavior. There is no major misalignment in design; rather, the misalignment is in maturity – the blueprint likely envisions a seamless promotion pipeline with clear metrics at each gate, whereas currently it’s a set of powerful but developer-operated tools. One area to watch is whether all blueprint governance rules are enforced: for example, the blueprint implies that understanding loop outputs should feed governance decisions (ensuring “AlphaTrade parity work can ship without capital risk” by using live-shadow mode until ready[28]). The current system does enforce a “live-shadow” (paper) mode by default for new strategies via drift gating and ledger stage, which is correct. However, if there are any blueprint policies not coded (such as time-based graduation criteria or multi-approval requirements), those would be gaps. In summary, governance in code is largely faithful to the plan, with the remaining work being operational integration and perhaps adding intelligence to assist human governors.
**Summary of Gaps:** In all, the emp_proving_ground_v1 codebase has established the core architecture (the five components exist and interact as intended[4]), but most components are only partially realized. Many subsystems still rely on scaffolding or mocks, and several advanced features from the AlphaTrade vision (live data ingestion, adaptive evolution of strategies, automated reflective learning, real trade execution, and fully automated governance oversight) are incomplete or absent. There are few fundamental design misalignments – the gaps are mostly feature-completeness and integration gaps rather than structural flaws. This is a strong position to be in: the blueprint is validated by the current design, and the task ahead is to close the implementation gaps with focused development in each area[26][39].

## 90-Day Roadmap (Phased Execution Plan)

Below is a refreshed 90-day roadmap to guide the next phase of AlphaTrade development. This plan is organized into three phases, each roughly one month, aligning with: Phase I – Understanding Loop & Data Backbone, Phase II – Governance & Risk Hardening, and Phase III – Full Integration & “Paper-Ready” System. Each phase is broken down into key milestones with checklist deliverables and measurable acceptance criteria (Definitions of Done). We also include a “Start Now” section for immediate next steps (first 48 hours) to build momentum. Throughout, we incorporate BDH-inspired primitives – specifically fast-weights (already in use), sparse positive activations, and graph health metrics – to ensure the system stays aligned with cutting-edge architectural principles. All new development will adhere to the preferred project directory structure (layered by core/sensory/thinking/trading/governance domains) and maintain strict code contracts (typed interfaces, data model schemas, and regression tests) for clarity and reliability[4][40].

### Start Now (Next 48 Hours) – Jumpstart and Quick Wins


- [x] Provision Development Data Services – Setup local instances of TimescaleDB, Redis, and Kafka to begin replacing mock data feeds[5]. DoD: Services running in docker or dev environment; connection parameters added to config; basic connectivity tests pass (e.g. can write/read a test record to Timescale). Completed via the new `docker-compose` services, `env_templates/dev_data_services.env`, and connectivity guardrails in `tests/operations/test_dev_data_services.py`.
  - Progress: Timescale connection settings now honour pool overrides (`TIMESCALEDB_POOL_SIZE`, `TIMESCALEDB_MAX_OVERFLOW`, `TIMESCALEDB_POOL_TIMEOUT`, `TIMESCALEDB_POOL_RECYCLE`, `TIMESCALEDB_POOL_PRE_PING`) pulled from `SystemConfig.extras`, applying them when targeting PostgreSQL/Timescale while skipping SQLite fallbacks; regression coverage locks the Postgres vs SQLite behaviour so institutional rehearsals can tune connection pools without editing code.【F:src/data_foundation/persist/timescale.py†L111-L255】【F:tests/data_foundation/test_timescale_connection_settings.py†L18-L90】

- [ ] Review and Cleanup Namespace Drift – Identify any mismatches between module names and usage (e.g. “intelligence” vs “understanding” in tests) and clean up deprecated references[4]. DoD: All references to removed or moved modules (e.g. get_risk_manager shim) are eliminated or properly routed to new modules; pytest and mypy run clean with no module-not-found or deprecation warnings.
  - Progress: Core package now only re-exports the canonical `RiskManager` facade while a guardrail test blocks legacy `src.market_intelligence`/`src.intelligence` imports. Sensory demos and shims import the supported `src.sensory` namespaces, keeping production examples aligned with the official surfaces under regression coverage.【F:src/core/__init__.py†L1-L52】【F:tests/current/test_namespace_integrity.py†L1-L84】【F:tests/current/test_mi_to_sensory_forwarding.py†L1-L60】【F:scripts/sensory_demo.py†L1-L160】【F:examples/production_example.py†L17-L139】
  - Progress: Coverage triage targets and coverage-matrix fixtures now track the canonical `src.sensory.enhanced` and `src.understanding` packages, preventing analysis jobs from masking regressions that revive deprecated sensory or intelligence modules.【F:scripts/analysis/triage_batch6.py†L60-L76】【F:tests/tools/test_coverage_matrix.py†L1-L210】
  - Progress: Import-linter contracts now elevate `src.understanding` to first-class layer and independence boundaries, and the backlog checklist retargets orchestration clean-up tickets to the renamed enhanced-understanding engine so architectural hygiene enforces the new namespace vocabulary.【F:contracts/importlinter.toml†L9-L49】【F:contracts/importlinter.ini†L8-L48】【F:list.md†L20-L34】
  - Progress: Runtime assembly now resolves the optional `SentientAdaptationEngine` through a shared `_import_optional_class` helper, centralising guardrails for optional dependencies so compose logic stays typed while gracefully degrading when extras are missing, with regression coverage guarding the fallback.【F:src/orchestration/compose.py†L35-L50】【F:tests/current/test_orchestration_compose.py†L100-L142】
  - Progress: Sentient adaptation now lives in the canonical `src.sentient.adaptation` package, with the intelligence facade lazily exporting the engine, a compatibility shim forwarding legacy imports, and a guard test suite proving the duplicate class definitions are retired without breaking callers.【F:src/sentient/adaptation/sentient_adaptation_engine.py†L1-L211】【F:src/intelligence/__init__.py†L23-L193】【F:src/intelligence/sentient_adaptation.py†L1-L22】【F:tests/current/test_public_api_intelligence.py†L1-L87】
  - Progress: Phase 3 intelligence orchestrator now returns `competitive_understanding` as the canonical telemetry while mirroring `competitive_intelligence` for legacy callers, and async guardrails pin the alias so downstream migrations stay lossless.【F:src/intelligence/__init__.py†L212-L284】【F:tests/intelligence/test_phase3_intelligence_orchestrator.py†L1-L83】
  - Progress: Namespace cleanup tooling now remaps legacy `market_intelligence` segments to the sensory domain so duplicate sweeps and hygiene scripts reinforce the understanding vocabulary across the tree.【F:scripts/cleanup/duplicate_map.py†L76-L107】
  - Progress: Integration, trading, validation, and orchestration surfaces now import `RiskManager` via the consolidated `src.risk` package, retiring lingering `.manager` references so the facade governs runtime usage consistently under regression coverage.【F:src/integration/component_integrator.py†L1-L120】【F:src/trading/trading_manager.py†L1-L210】【F:src/validation/phase2d_simple_integration.py†L1-L120】【F:tests/risk/test_risk_manager_impl_additional.py†L1-L80】
  - Progress: `src.core` now lazily exposes only the canonical `RiskManager` export and raises `AttributeError` for the retired `get_risk_manager` shim, with regression coverage ensuring the legacy symbol stays unavailable for downstream callers.【F:src/core/__init__.py†L17-L56】【F:src/risk/__init__.py†L39-L84】【F:tests/risk/test_risk_manager_impl_additional.py†L267-L277】
  - Progress: The legacy `src.validation.accuracy.intelligence_validator` module now raises a targeted `ModuleNotFoundError`, forcing remaining callers onto the understanding namespace while a guardrail test asserts the failure path so namespace drift cannot creep back in silently.【F:src/validation/accuracy/intelligence_validator.py†L1-L15】【F:tests/current/test_understanding_validator.py†L8-L16】
  - Progress: The contextual fusion engine no longer exposes the `analyze_market_intelligence` alias and sensory integration tests assert its absence, keeping namespace cleanup aligned with the understanding vocabulary under regression coverage.【F:src/orchestration/enhanced_understanding_engine.py†L300-L335】【F:src/sensory/tests/test_integration.py†L297-L300】
  - Progress: Coverage matrix reports now surface deprecated `src.market_intelligence` modules as `deprecated_files`, block silent remapping, and log the drift inside JSON/Markdown outputs so cleanup work can target remaining stragglers under guardrail coverage.【F:tools/telemetry/coverage_matrix.py†L52-L219】【F:tests/tools/test_coverage_matrix.py†L120-L172】
  - Progress: Competitive understanding storage now emits `UnderstandingReportTD` payloads with the canonical `understanding_id` only, removing the legacy alias so downstream consumers adopt the updated schema under guardrail coverage of the new telemetry shape.【F:src/thinking/competitive/competitive_intelligence_system.py†L868-L885】【F:src/thinking/models/types.py†L61-L69】【F:tests/current/test_understanding_validator.py†L8-L16】

- [x] Enable Real Data Ingest in a Slice – Pick one data source (e.g. daily price candles for one asset) and connect it end-to-end through the system. This involves writing a small ingestion script using the new Timescale service and feeding the data into the real_sensory_organ. DoD: A new integration test or notebook that ingests real market data into the BeliefState (via the sensory organ) and produces a belief snapshot; ensure the belief update (Hebbian step) runs without errors on this data. Completed via provider-backed Yahoo Finance ingest flows that hydrate Timescale, emit supervised sensory snapshots, and replay the slice into audited belief/regime sequences.
  - Completion: Operational backbone factory now honours injected `TaskSupervisor` instances while hydrating `RealDataManager`, Kafka bridges, and sensory organs so supervised runtimes can assert ownership boundaries; guardrail coverage asserts the pipeline reuses the caller’s supervisor without double-closing resources.【F:src/data_foundation/pipelines/operational_backbone.py†L701-L734】【F:tests/data_foundation/test_operational_backbone_factory.py†L139-L169】【F:tests/integration/test_operational_backbone_pipeline.py†L120-L198】
  - Progress: Institutional operational-backbone wiring now normalises `KAFKA_INGEST_ENABLE_STREAMING` via a shared boolean sentinel helper and automatically requires Timescale/Redis (plus Kafka when streaming is enabled) whenever `SystemConfig.data_backbone_mode` is institutional, raising targeted `RuntimeError`s if prerequisites are missing; tests cover fakeredis-backed managed caches, missing Timescale URLs, and streaming-on Kafka expectations so promotion scripts fail fast with actionable guidance.【F:src/data_foundation/pipelines/operational_backbone.py†L49-L910】【F:tests/data_foundation/test_operational_backbone_factory.py†L87-L239】
  - Completion: Yahoo ingest helpers normalise MultiIndex downloads, pad missing OHLCV fields, and align intraday timestamps so Timescale writers receive canonical columns; regression tests stub yfinance responses and confirm the reshaped frames feed ingest orchestrators without schema regressions.【F:src/data_foundation/ingest/yahoo_ingest.py†L23-L122】【F:tests/data_foundation/test_timescale_ingest.py†L112-L155】
  - Completion: Live provider regression ingests AAPL candles through `run_real_data_slice`, exercises Timescale-backed storage, and proves sensory/belief outputs retain finite confidences when Yahoo returns data (skipping gracefully when the provider is empty).【F:tests/data_foundation/test_real_data_slice_live.py†L12-L55】
  - Completion: Belief sequence replay now ships alongside the slice runner, capturing snapshots, posterior states, regime telemetry, and calibration evidence; the integration suite verifies PSD covariance, telemetry emission, and parity with the terminal sensory snapshot across the replayed window.【F:src/data_integration/real_data_slice.py†L313-L378】【F:tests/integration/test_real_data_belief_sequence.py†L15-L69】

- [x] Quick Win: Implement Basic Anomaly Detector – Fill in a simple implementation for the ANOMALY organ stub. For example, use a z-score or Bollinger band method on recent prices to flag anomalies. DoD: real_sensory_organ no longer raises NotImplemented for anomaly detection; anomaly flags show up in belief snapshots (e.g. boolean “anomaly” field); a unit test feeds a out-of-range value and the anomaly flag triggers.
  - Completion: `BasicAnomalyDetector` now computes rolling z-scores with window/min-sample validation, the anomaly sensor consumes it for both sequence and frame inputs, and regression tests assert anomaly flags, detector metadata, and lineage payloads across the sensory stack.【src/sensory/anomaly/basic_detector.py:1】【src/sensory/anomaly/anomaly_sensor.py:38】【tests/sensory/test_basic_anomaly_detector.py:1】【tests/sensory/test_how_anomaly_sensors.py:170】

- [x] Draft “Definition of Done” Templates – For each upcoming phase’s major feature, write a one-paragraph definition of done. This will guide development and testing (e.g. DoD for “executable HOW organ” or “risk policy enforcement”). DoD: DoD statements written in the project docs (or as comments in alignment briefs) for at least the top 5 upcoming deliverables, including measurable criteria. Alignment briefs for the evolution engine, data backbone, and sensory cortex now host the new templates landed in commit 96cd9e3 so teams have measurable success criteria ahead of implementation.
**Quick Wins vs. Deferrals:** The tasks above focus on low-hanging fruit that de-risk the project (e.g. setting up infrastructure, trivial feature implementations). More speculative or complex features (like sophisticated ML-based anomaly detection or full BDH graph algorithms) are acknowledged in the roadmap but deferred until core functionality is in place. For example, implementing a complete BDH spiking neural network is out of scope for 90 days (strategic deferral), whereas computing simple graph metrics and ensuring weight sparsity are achievable and included.

## Phase I (Days 0–30): Understanding Loop & Data Backbone

**Goal:** Establish a fully functional Understanding Loop v2 running on real data, backed by a solid data pipeline. This phase focuses on closing the Perception gap – replacing mocks with live data ingest – and strengthening the Adaptation loop with initial evolutionary features. We will also ensure the foundation (data and fast-weight logic) is robust with tests and documentation.

- [ ] Operational Data Backbone Online – Deploy real data ingestion and caching services for live-shadow mode. Replace the remaining mock data pathways with actual TimescaleDB (for historical/time-series data), a streaming source via Kafka (for live ticks), and Redis for caching hot data[5]. DoD: The system can ingest data from Timescale (e.g. recent market history) and stream updates via Kafka into the sensory layer. All SQL queries are parameterized (no raw SQL injection risks)[41], and background ingest tasks are supervised (monitored via the runtime supervisor). Regression tests cover a full ingest cycle (store, cache, retrieve) proving that no mocks are needed for core data flows.
  - Progress: `config/system/dev_data_backbone.yaml`, the Timescale init script, and the new dev-services test suite validate the docker-compose stack end-to-end while docs/development/setup.md walks operators through running the services locally; live ingestion into sensory remains outstanding.
  - Progress: New end-to-end coverage exercises the operational backbone pipeline’s Timescale writes, Kafka bridges, cache metrics, and sensory fusion through shutdown handling, anchoring the DoD even though services still run in mocked local mode.【F:tests/integration/test_operational_backbone_pipeline.py†L120-L190】【F:src/data_foundation/streaming/kafka_stream.py†L1064-L1520】
  - Progress: Streaming runs now publish sensory snapshots onto a configurable event-bus topic (`telemetry.sensory.snapshot` by default or `KAFKA_SENSORY_SNAPSHOT_TOPIC` via extras), wiring the operational backbone pipeline to push snapshots alongside callbacks, with integration coverage asserting subscribers receive the payload and the topic is overridable for deployments.【F:src/data_foundation/pipelines/operational_backbone.py†L120-L178】【F:src/data_foundation/pipelines/operational_backbone.py†L720-L903】【F:tests/integration/test_operational_backbone_pipeline.py†L394-L469】
  - Progress: Operational backbone pipeline now boots attached `LiveBeliefManager` and `UnderstandingRouter` components, captures belief/regime snapshots plus understanding decisions, and logs ingest failures while guardrail tests assert cached Timescale frames keep routing alive when the warehouse is offline, fulfilling the acceptance handshake for live-loop failover.【F:src/data_foundation/pipelines/operational_backbone.py†L82-L366】【F:tests/integration/test_operational_backbone_pipeline.py†L198-L295】【F:tools/data_ingest/run_operational_backbone.py†L327-L378】
  - Progress: Streaming helpers now launch Kafka ingest loops under `TaskSupervisor`, track optional metadata, and shut down cleanly; the streaming regression proves event bus telemetry arrives while supervised tasks retire without leaking consumers, validating continuous ingest operations.【F:src/data_foundation/pipelines/operational_backbone.py†L100-L505】【F:tests/integration/test_operational_backbone_pipeline.py†L322-L399】
  - Progress: Real-data slice tooling now supports provider-backed ingestion, enforcing mutual exclusion between CSV paths and provider names, defaulting symbols when fixtures are present, and surfacing Timescale source labels while integration coverage proves the provider fetch pathway and CLI validation.【F:src/data_integration/real_data_slice.py†L109-L196】【F:tools/data_ingest/run_real_data_slice.py†L45-L125】【F:tests/integration/test_real_data_slice_ingest.py†L12-L89】
  - Progress: Operational backbone now enumerates Kafka dimension expectations from composite publishers, synthesises telemetry for successful ingest writes that lack broker echoes, orders the final event list, and records cache metrics plus ingest errors inside the validation snapshot so evidence stays complete even when streaming is flaky.【F:src/data_foundation/pipelines/operational_backbone.py†L305-L499】【F:src/runtime/runtime_builder.py†L1671-L1770】【F:src/data_foundation/streaming/kafka_stream.py†L1374-L1384】
  - Progress: Operational backbone shutdown now honours externally managed data services via a `shutdown_manager_on_close` flag, and the runtime builder tears down streaming tasks while leaving injected managers alive; guardrail tests confirm ingest drills reuse the provided manager without forcing a shutdown.【F:src/data_foundation/pipelines/operational_backbone.py†L120-L654】【F:src/runtime/runtime_builder.py†L1702-L1731】【F:tests/data_foundation/test_ingest_journal.py†L579-L698】
  - Progress: In-memory Kafka broker now mirrors the producer/consumer protocols so regression suites drive the full Timescale→Redis→Kafka→sensory pipeline without external services, and ingest readiness tests assert broker snapshots match emitted telemetry before shutting down the supervised pipeline.【F:src/data_foundation/streaming/in_memory_broker.py†L63-L200】【F:tests/integration/test_operational_backbone_pipeline.py†L151-L188】【F:tests/operations/test_data_backbone.py†L112-L172】
  - Progress: Real data manager now emits `ConnectivityProbeSnapshot` payloads with per-service status strings, latency metrics, masked endpoints, and degradation detail so institutional toggles can gate on verified connectivity rather than booleans; the managed-connectors CLI reuses live Redis clients, surfaces offline caches with explicit error text, and closes clients on exit so runbooks capture actionable health data under expanded regression coverage for degraded fallbacks and outage messaging.【F:src/data_integration/real_data_integration.py†L149-L785】【F:tools/operations/managed_ingest_connectors.py†L245-L292】【F:tests/data_integration/test_real_data_manager.py†L271-L315】【F:tests/tools/test_managed_ingest_connectors.py†L42-L116】
  - Progress: Connectivity reports now aggregate probe severities into an overall status, flag SQLite/other non-Postgres backends as degraded with backend metadata, and rely on hardened Timescale URL parsing so readiness packets surface misconfigured services instead of marking them healthy; refreshed regressions assert degraded/off rollups in JSON payloads.【F:src/data_integration/real_data_integration.py†L184-L207】【F:src/data_integration/real_data_integration.py†L721-L758】【F:src/data_foundation/persist/timescale.py†L168-L175】【F:tests/data_integration/test_real_data_manager.py†L303-L351】
  - Progress: Operational backbone CLI now threads `--require-*` flags through to `RealDataManager`, coercing heterogeneous booleans and failing fast when Timescale, Redis, or Kafka connectors are mandated so institutional drills cannot silently fall back to in-memory mocks; regression coverage asserts the flags reach the manager and guard the CLI entry point used in audits.【F:src/data_integration/real_data_integration.py†L201-L312】【F:tools/data_ingest/run_operational_backbone.py†L117-L506】【F:tests/tools/test_run_operational_backbone.py†L108-L143】
  - Progress: Live-shadow ingest CLI now orchestrates the Timescale→Redis→Kafka pipeline, captures scheduler/cache/streaming evidence, supports JSON/Markdown/text summaries, and honours connector requirement flags so operators can archive supervised drills without bespoke notebooks; CLI regressions verify summary content, streaming snapshots, and flag wiring.【F:tools/data_ingest/run_live_shadow.py†L1-L341】【F:tools/data_ingest/run_live_shadow.py†L241-L341】【F:tests/tools/test_run_live_shadow.py†L111-L210】
  - Progress: Operational backbone streaming now subscribes the sensory organ to Kafka topics, captures per-symbol snapshots, and forwards them to optional callbacks; async regressions assert EURUSD streams populate `streaming_snapshots` under supervised shutdowns.【F:src/data_foundation/pipelines/operational_backbone.py†L120-L792】【F:tests/integration/test_operational_backbone_pipeline.py†L370-L429】
  - Progress: Institutional ingest config honours `KAFKA_INGEST_ENABLE_STREAMING`, threads the flag through provisioner summaries, runtime telemetry, and runbook guidance, and regression coverage proves streaming-disabled rehearsals skip Kafka consumers cleanly.【F:src/data_foundation/ingest/configuration.py†L728-L879】【F:src/data_foundation/ingest/institutional_vertical.py†L525-L848】【F:src/runtime/runtime_builder.py†L1750-L1761】【F:src/runtime/runtime_builder.py†L3340-L3374】【F:tests/runtime/test_institutional_ingest_vertical.py†L150-L594】【F:docs/runbooks/data_foundation.md†L193-L204】
  - Progress: Runtime builder now records and publishes institutional data backbone validation/readiness snapshots, stitching ingest, failover, Redis/Kafka posture, and supervised task metadata into Markdown/JSON telemetry with regression coverage so operators inherit fail-closed evidence even when validation aborts ingest.【src/operations/data_backbone.py:245】【src/operations/data_backbone.py:389】【src/runtime/runtime_builder.py:1765】【tests/operations/test_data_backbone.py:62】
**Acceptance tests:** Simulate a minute of live data (via Kafka) and verify the UnderstandingRouter receives appropriate belief updates. Validate that if Timescale is turned off, the failover mechanisms log warnings but the system continues (using cached data)[42].

- [ ] Executable Sensory Organs (WHAT/WHEN/HOW/WHY/ANOMALY) – Implement the remaining sensory organ logic. Each organ module should process raw data into a feature: e.g. HOW = volatility or trend detection, WHY = news or causal tag (could be stubbed with “N/A” for now), WHEN = timing features (market session or event proximity), WHAT = primary observable (price/volume movement), ANOMALY = outlier detection. DoD: No organ function remains a stub; each produces a defined output in the sensory payload. These outputs feed into the BeliefState construction. The organs publish lineage and quality metadata (timestamp, source, confidence) for each signal[43]. Added tests for each organ: e.g. feed a known pattern into HOW organ, verify it identifies trend up vs down; feed a known anomaly, verify anomaly organ flags it.
  - Progress: WHAT/WHEN/WHY sensors now attach `quality` metadata with timestamps, source identifiers, and confidence values, and the lineage payloads are validated by the new `test_primary_dimension_sensors` suite while the anomaly stack exercises the shared detector path under guardrail coverage.【src/sensory/what/what_sensor.py:131】【src/sensory/when/when_sensor.py:147】【src/sensory/why/why_sensor.py:126】【tests/sensory/test_primary_dimension_sensors.py:1】
  - Progress: Executable sensory organ wrappers now expose WHAT/WHEN/WHY dimensions alongside HOW/ANOMALY, normalising market frames, merging macro/narrative context, and forwarding calibrated sensor telemetry so cortex integrations receive consistent lineage, quality, and threshold metadata under dedicated regression coverage.【F:src/sensory/organs/dimensions/executable_organs.py†L1-L200】【F:tests/sensory/test_dimension_organs.py†L20-L279】
  - Progress: HOW organ now samples bounded jitter through an injectable random source, ingests bid/ask spreads, and distinguishes bullish versus bearish order-flow; regression fixtures assert institutional bias gradients across frames so governance evidence covers live order-book inputs.【F:src/sensory/enhanced/how_dimension.py†L19-L109】【F:src/sensory/how/how_sensor.py†L182-L200】【F:tests/sensory/test_primary_dimension_sensors.py†L146-L197】
**Acceptance tests:** Run the entire sensory organ on a historical dataset and inspect a sample BeliefState. Confirm that all five signal types are present and plausible (e.g. “anomaly” flag raised during a known market shock). Additionally, verify drift detection reacts to the HOW signal (e.g. high volatility triggers drift alert).

- [x] Belief & Regime Integration with Real Data – Ensure the BeliefState and Regime detectors work with the live data feeds. As real data can be noisy, calibrate the Hebbian update (fast-weight decay) parameters and regime thresholds. Possibly introduce a “calm/normal/storm” regime classification using volatility measures. DoD: BeliefState updates remain stable (covariance matrix stays PSD and bounded) under real feed[21]. Regime FSM (finite-state machine) transitions appropriately with real volatility – e.g., in a quiet period, system is in “calm” regime, and switches to “storm” when volatility spikes, publishing an event on the bus. Include guardrail tests for extreme scenarios: feeding constant data (should remain calm) vs. highly erratic data (should trigger storm regime and maybe anomalies).
  - Progress: Belief buffer now records posterior covariance trace, condition, and eigenvalue extrema in lineage metadata while guardrail-marked EURUSD slice suites assert PSD eigenvalues, bounded maxima, non-negative minima, and telemetry emission across calm and storm windows, keeping the real-data stability evidence current.【F:src/understanding/belief.py†L316-L400】【F:tests/understanding/test_belief_real_data_integration.py†L91-L163】【F:tests/data_integration/test_real_data_slice_belief.py†L48-L140】
  - Progress: A dedicated calibrator now derives Hebbian learning rates, decay, and calm/storm thresholds from historical EURUSD prices, bundles diagnostics for governance evidence, and proves the configuration by instantiating calibrated belief/regime components against calm versus erratic snapshots under regression coverage.【F:src/understanding/belief_regime_calibrator.py†L1-L175】【F:tests/operations/test_belief_regime_calibrator.py†L34-L136】
  - Progress: Guardrail suites now replay volatility spikes through the full sensory-to-belief pipeline, asserting the `RealSensoryOrgan` emits drift summaries, anomaly payloads, and event-bus telemetry while the `BeliefEmitter` produces enriched posterior features, proving the integrated rehearsal path stays wired to the canonical understanding surface.【F:tests/understanding/test_sensory_belief_pipeline.py†L84-L139】【F:src/sensory/real_sensory_organ.py†L190-L235】【F:src/understanding/belief.py†L649-L706】
  - Progress: Live belief manager now applies hysteresis when scaling regime thresholds, emits belief/regime telemetry for cached ingest snapshots, and guardrail suites replay volatility spikes to confirm thresholds climb monotonically while routing stays stable.【F:src/understanding/live_belief_manager.py†L18-L168】【F:tests/understanding/test_live_belief_manager.py†L210-L296】【F:tests/integration/test_operational_backbone_pipeline.py†L198-L295】
  - Progress: Belief buffer now tolerates dynamic feature sets by backfilling missing dimensions, extending the canonical feature order, and aligning prior covariance shapes so live ingest can add or remove sensory fields without crashing; guardrail coverage replays expansion and contraction to prove metadata zeros and posterior matrices stay consistent.【F:src/understanding/belief.py†L310-L449】【F:tests/understanding/test_belief_updates.py†L264-L309】
  - Progress: Belief assimilation now drops NaN/∞ telemetry before Hebbian updates, clamps volatility scaling to finite values, and a guardrail suite floods snapshots with non-finite strengths/z-scores to prove emitted observations, covariance eigenvalues, and regime diagnostics stay bounded.【F:src/understanding/belief.py†L167-L333】【F:src/understanding/belief_real_data_utils.py†L72-L93】【F:tests/understanding/test_belief_real_data_integration.py†L179-L224】
  - Completion: Belief buffer hyperparameters can now be reapplied in-flight with validation, volatility window resizing, and state/volatility resets so calibration workflows retune decay and variance bounds without reinstantiating buffers; guardrail tests confirm the API zeroes history and persists the updated configuration.【F:src/understanding/belief.py†L314-L363】【F:tests/understanding/test_belief_updates.py†L312-L354】
  - Completion: Live belief manager recalibration processes calm vs. storm frames, clears historical snapshots, reapplies freshly derived thresholds, and republishes belief/regime telemetry while preserving PSD covariance; guardrail coverage asserts bus emissions, threshold shifts, and bounded eigenvalues after the recalibration sweep.【F:src/understanding/live_belief_manager.py†L200-L257】【F:tests/understanding/test_live_belief_manager.py†L204-L260】
**Acceptance tests:** After ingesting a month of historical data, the distribution of regime states should make sense (e.g. some % calm, some % normal, few % storm, without oscillating every tick). A test replays a known volatile period (market crash) and asserts that the regime state enters “storm” and drift alerts fire, aligning with expectations.

- [x] Fast-Weight Adaptation Tuning – Leverage BDH principles to refine fast-weight updates. Introduce configuration to enforce sparse positive activations in adaptation: for instance, apply ReLU or thresholding so that fast-weight adjustments never produce negative connection strengths, and perhaps prune (set to zero) the smallest weights periodically to maintain sparsity[44][45]. DoD: A new fast_weights.py module (or extension of the router) implements constrained Hebbian updates: only additive (excitatory) weight changes or segregated inhibitory signals if needed. A metric for “% of strategy weights active” is tracked per decision – aiming for only a small fraction of strategies to get significant weight boosts at any time (sparsity). Unit tests verify that with fast-weights enabled, all weights in the router’s internal matrices remain ≥ 0 (non-negative) and that toggling the fast-weight feature flag reverts to baseline behavior[12][46].
  - Completion: Fast-weight constraints now parse from `SystemConfig` extras, enforcing excitatory-only mode, max-active fractions, and activation thresholds while tracking inhibitory versus suppressed multipliers in the metrics payload; router decisions surface the telemetry and guardrail tests prove inhibitory suppression, excitatory clamping, and feature-flag fallbacks.【src/thinking/adaptation/fast_weights.py:21】【src/thinking/adaptation/fast_weights.py:217】【src/understanding/router.py:205】【tests/thinking/test_fast_weights.py:13】【tests/understanding/test_understanding_router.py:216】【tests/thinking/test_policy_router.py:169】
**Acceptance tests:** Run a simulation of 100 decisions with fast-weights on; ensure that at most (say) 20% of strategies got weight updates in that period (sparsity threshold) and none of the weight adjustments were negative. Also verify that disabling fast-weights yields identical decisions for a given input sequence (proving determinism when the feature is off[47]).

- [x] Seed the Evolution Engine (Basic) – Implement a simple strategy mutation or catalog integration. As a first step toward the Evolution Engine, allow the system to tweak an existing strategy’s parameter or choose from a library of strategies when performance lags. For example, introduce a configuration where after N iterations, if a strategy’s win-rate is below X, an alternate strategy from the catalogue is introduced for trial (or a parameter is perturbed). DoD: A rudimentary strategy catalogue is created (even if just a list of two strategies for now). The PolicyRouter or a new EvolutionManager monitors performance stats from diaries and can swap or spawn a strategy in the routing table under certain conditions[13]. Document this behavior and ensure it’s feature-flagged (only active in paper-trade mode). At least one test simulates a consistently losing strategy and confirms the system either adjusts its weight down or replaces it with a variant.
  - Completion: Catalogue-backed variants, parameter mutations, and paper-stage gating now flow through `EvolutionManager`, which emits an adaptation summary consumed by the AlphaTrade loop to register variants, degrade base weights, and persist the actions into decision diaries under feature-flag control, with regression coverage across manager and orchestration tests.【src/thinking/adaptation/evolution_manager.py:70】【src/thinking/adaptation/evolution_manager.py:104】【src/orchestration/alpha_trade_loop.py:299】【tests/thinking/test_evolution_manager.py:72】【tests/orchestration/test_alpha_trade_loop.py:485】
**Acceptance tests:** Use the decision diary data to verify evolutionary behavior: e.g. after 50 iterations with poor outcomes for Strategy A, the logs/diary should show Strategy B (new or mutated) being tried. Confirm that governance rules still apply (the new strategy should start in an experimental stage with force_paper true until promoted).

- [x] Documentation & Example Notebook – Document the Understanding Loop and provide a usage example. Update docs/ with a description of the Phase I features: data flow from ingest to decision, how fast-weights work, how to interpret the decision diary. Create a simple Jupyter notebook (or markdown in docs/examples) demonstrating a small end-to-end run: ingest sample data, run a loop iteration, inspect outputs (belief state, chosen strategy, diary entry). DoD: Documentation pages updated (include the new config options, flags, and any new modules). The example notebook runs without errors and produces a clear visualization or printout of one loop cycle on real data. This serves as both a validation and a training artifact for new contributors.
  - Completion: Understanding Loop Phase I guide and the runnable notebook now walk through ingest→belief→decision flow, fast-weight adapters, and diary exports with troubleshooting notes and demo outputs for the EURUSD slice.【docs/UNDERSTANDING_LOOP_GUIDE.md†L1-L67】【docs/examples/understanding_loop_demo.ipynb†L1-L420】
**Acceptance tests:** (Manual) A reviewer can follow the docs to set up data ingest and reproduce the notebook’s results, confirming the system behaves as documented.

## Phase II (Days 31–60): Governance, Risk & Policy Hardening

**Goal:** Solidify the Governance and Risk management aspects so that the system can safely handle increasingly autonomous decisions. In this phase, we focus on enforcing risk limits deterministically, integrating governance checks throughout the loop, and improving overall system resilience (task supervision, compliance logging). By the end of Phase II, AlphaTrade should be robust enough to run in a closed-loop paper trading environment with minimal human intervention, while ensuring no strategy can bypass defined policies. Governance processes will be streamlined and fully tested, and the groundwork laid for eventual live trading under strict control.

- [ ] Task Supervision & Runtime Builder Completion – Finalize the asynchronous task management. Ensure that all background tasks (data ingest, signal processing, trade execution) are launched via the unified runtime supervisor. Remove any remaining asyncio.create_task calls that aren’t tracked[26]. DoD: The bootstrap_runtime code uses a supervised task registry for every service (ingest loops, drift monitoring, etc.). If a task fails or hangs, the supervisor logs it and optionally restarts it. Add a torture test: deliberately cause an ingest task to fail and verify the system logs the failure and continues running other tasks (no global crash). The runtime builder configuration should now treat data backbone, understanding loop, and drift monitors as first-class citizens (no manual steps to enable them).
  - Progress: Timescale ingest scheduler now returns its managed task handle, exposes `wait_until_stopped`, and is shut down under shielded cleanup in the runtime builder, with institutional services tracking the supervised task so ingest loops drain deterministically; guardrails cover wait semantics and cancellation handling alongside the existing TaskSupervisor enforcement.【F:src/data_foundation/ingest/scheduler.py†L221-L282】【F:src/data_foundation/ingest/institutional_vertical.py†L394-L441】【F:src/runtime/runtime_builder.py†L3934-L3998】【F:tests/data_foundation/test_ingest_scheduler.py†L177-L236】
  - Progress: TaskSupervisor now supports restart policies with bounded backoff while the runtime builder wires ingestion/trading workloads through those restart hooks, ensuring failing ingest loops relaunch without tearing down trading under new regression coverage.【F:src/runtime/task_supervisor.py†L83-L203】【F:src/runtime/runtime_builder.py†L793-L964】【F:tests/runtime/test_task_supervisor.py†L107-L138】【F:tests/runtime/test_runtime_builder.py†L1447-L1499】
  - Progress: Timeout diagnostics now surface the hung task name, metadata, and asyncio state when cancellation exceeds the configured budget so operators can pinpoint stuck ingestion or trading loops during shutdown rehearsals, with regression coverage asserting the enriched log payloads.【F:src/runtime/task_supervisor.py†L205-L248】【F:tests/runtime/test_task_supervisor.py†L141-L167】
  - Progress: Runtime shutdown now asks the shared `TaskSupervisor` to cancel active workloads before running teardown callbacks, logging cancelled workloads instead of surfacing spurious exceptions and leaving no dangling snapshots; regression coverage asserts long-running ingest/trade coroutines are cancelled and summarised cleanly.【F:src/runtime/runtime_builder.py†L946-L1083】【F:tests/runtime/test_runtime_builder.py†L167-L224】
  - Progress: Event bus supervision now provisions a scoped `TaskSupervisor` by default, cancels lingering handler tasks when factories swap, and exposes fallback supervision for ad-hoc background jobs so runtime telemetry no longer leaves orphaned tasks during lifecycle changes.【F:src/core/_event_bus_impl.py†L44-L520】【F:tests/runtime/test_runtime_builder.py†L60-L129】
  - Progress: Event bus diagnostics now expose `AsyncEventBus.task_snapshots()` (with `TopicBus` delegation) so supervised worker/handler metadata is queryable, and `ProfessionalPredatorApp.summary()` records the snapshots as `event_bus_tasks`, giving operators a live roster of supervised workloads; regressions cover the metadata contract and summary wiring while keeping diagnostics best-effort on failure.【F:src/core/_event_bus_impl.py†L291-L708】【F:src/runtime/predator_app.py†L1491-L1512】【F:tests/current/test_event_bus_task_supervision.py†L103-L156】【F:tests/current/test_runtime_professional_app.py†L433-L461】
  - Progress: TaskSupervisor snapshots now publish each task’s age in seconds under regression coverage, letting operators spot hung workloads directly from runtime summaries before cancellation kicks in.【F:src/runtime/task_supervisor.py†L15-L33】【F:src/runtime/task_supervisor.py†L203-L224】【F:tests/runtime/test_task_supervisor.py†L25-L47】
  - Progress: Bootstrap runtime now instantiates a scoped supervisor when none is supplied, registers the loop with restart metadata, and exposes the background task through the ProfessionalPredatorApp summary so bootstrap runs share the supervised lifecycle contract, with regression ensuring the shared supervisor wiring stays intact.【F:src/runtime/bootstrap_runtime.py†L572-L607】【F:src/runtime/predator_app.py†L260-L285】【F:tests/runtime/test_runtime_builder.py†L1540-L1571】
  - Progress: ProfessionalPredatorApp now detects supervisor-aware components, injects the shared TaskSupervisor via setter or keyword, registers component-spawned tasks, and merges metadata when workloads re-register; the supervisor adds `is_tracked` and metadata updates while regression coverage confirms supervised components surface their tasks in readiness snapshots.【F:src/runtime/predator_app.py†L360-L928】【F:src/runtime/task_supervisor.py†L170-L200】【F:tests/current/test_runtime_professional_app.py†L99-L199】
  - Progress: RuntimeApplication now records per-workload lifecycle state, keeps trading loops alive when ingest fails, and exposes supervisor namespace, active task counts, restart policies, and workload states through `summary()` so operators can confirm supervised recovery in-flight, with regression covering failure-during-trading scenarios.【F:src/runtime/runtime_builder.py†L1007-L1070】【F:tests/runtime/test_runtime_builder.py†L1683-L1767】
  - Progress: Ingest “torture” regression now forces repeated ingestion crashes, asserts supervised retries, verifies restart counters in live snapshots, and checks summaries after orderly shutdown so runtime builders prove ingest instability no longer topples trading loops.【F:tests/runtime/test_runtime_builder.py†L215-L280】
  - Progress: Professional runtime summaries now attach the live `RuntimeApplication` snapshot, surfacing ingestion/trading workload metadata whenever the builder stitches the app together, with guardrails asserting the ProfessionalPredatorApp forwards the runtime workloads verbatim for operators and dashboards.【F:src/runtime/predator_app.py†L328-L391】【F:src/runtime/predator_app.py†L1498-L1509】【F:src/runtime/runtime_builder.py†L3940-L3949】【F:tests/runtime/test_runtime_builder.py†L309-L348】
  - Progress: Workload metadata now labels ingestion/trading tasks with canonical `workload_kind` and `supervised_components`, including skip paths, and regression coverage asserts summaries expose the labels while supervised retries keep trading live after ingest failures.【F:src/runtime/runtime_builder.py†L2589-L2609】【F:src/runtime/runtime_builder.py†L4155-L4160】【F:tests/runtime/test_runtime_builder.py†L470-L561】
  - Progress: Failure-recovery regressions now crash the ingest workload on first run, assert the supervisor logs the restart, and verify trading stays live while summaries expose a failed-ingest state so roadmap uptime criteria have direct evidence of resilient restarts.【F:tests/runtime/test_runtime_builder.py†L1600-L1704】
  - Progress: Runtime builder now spins up a `RealDataManager` with institutional extras, registers cleanup hooks, feeds cache metrics into ingest telemetry, and hands the manager to the supervised ingest execution so pipelines can fall back to the Timescale orchestrator when synthetic publishing fails without losing observability.【F:src/runtime/runtime_builder.py†L3172-L3424】【F:src/runtime/runtime_builder.py†L1671-L1770】【F:src/data_integration/real_data_integration.py†L121-L418】
  - Progress: RealDataManager now normalises weekly and monthly interval tokens onto the daily dimension so provider-backed fetches reuse cached bars instead of returning empty frames, with regression coverage proving monthly and weekly queries resolve the expected closing prices from the daily slice.【F:src/data_integration/real_data_integration.py†L50-L133】【F:tests/data_integration/test_real_data_manager.py†L302-L335】
**Acceptance tests:** Run the full system for an hour in a test (accelerated or with dummy fast-forward) – all tasks should remain running. Introduce a fault (e.g. make the Kafka consumer throw an exception) and verify via logs or events that the supervisor caught it and attempted recovery.

- [x] Deterministic Risk Enforcement – Enforce all risk limits and ensure any breach is caught before trade execution. Expand the risk checks beyond drift: e.g., incorporate position sizing rules, max drawdown limits, and exposure limits from the Policy Ledger into the execution gating. DoD: The trading execution module (or a new RiskManager component) will intercept proposed trades and validate: (a) position size does not exceed a percentage of portfolio based on confidence, (b) leverage constraints are respected, (c) if adding this position would breach max sector or asset exposure, it’s blocked. Any violation should result in the trade being either converted to paper or cancelled, and an event recorded. These checks must run in O(1) time per trade (simple arithmetic) for determinism. Add tests simulating a trade that violates each rule – confirm the system sets decision.decision outcome to “REJECTED” or similar, and that the diary notes the reason (e.g. “Blocked by risk policy: exposure limit”).
  - Completion: RiskGateway now enforces confidence-notional, leverage, and sector caps derived from `RiskConfig`, rejects breaches with reason-coded telemetry, and ships guardrail tests for each violation path alongside refreshed limit snapshots for diaries and dashboards.【src/trading/risk/risk_gateway.py:336】【tests/current/test_risk_gateway_validation.py:452】【tests/current/test_risk_gateway_validation.py:479】【tests/current/test_risk_gateway_validation.py:528】
  - Progress: Liquidity probes now reuse resolved portfolio pricing inside the gateway, the trading-manager regression asserts the stricter signature, and the Phase II mini-audit documents the repair so governance inherits priced liquidity evidence.【F:src/trading/risk/risk_gateway.py†L796-L832】【F:tests/current/test_risk_gateway_validation.py†L154-L199】【F:tests/trading/test_trading_manager_execution.py†L1255-L1266】【F:docs/reports/risk_governance_phase2_mini_audit.md†L1-L34】
  - Progress: The `RiskManager` facade ships regression coverage for misconfiguration guards and sector budget enforcement, alongside a follow-up audit capturing 100 % trace evidence so roadmap coverage targets remain demonstrably satisfied.【F:tests/risk/test_risk_manager_facade.py†L1-L87】【F:docs/reports/governance_risk_phase2_followup_audit.md†L1-L24】
  - Progress: Portfolio risk aggregation now measures absolute position size when computing exposure so short trades consume risk budget instead of falling to zero, with regression coverage proving negative sizes still drive non-zero `risk_amount` and the Phase II audit recording the fix for governance reviewers.【F:src/risk/risk_manager_impl.py†L228-L236】【F:tests/current/test_risk_manager_impl.py†L234-L242】【F:docs/audits/policy_code_audit_phase2.md†L8-L18】
  - Progress: Phase II completion refresh audit now documents the stop-loss pip conversion regressions that prove percentage conversion and floor clamping, closing the last coverage gap called out by reviewers and keeping the evidence pack current.【F:docs/reports/governance_risk_phase2_code_audit_refresh.md†L1-L52】【F:tests/current/test_risk_gateway_validation.py†L210-L276】
  - Progress: Safety manager confirmation parsing now normalises environment strings, rejects unknown payloads, and documents the regression fix in the refreshed Phase II audit so live boots cannot bypass confirmation gates, with coverage exercising kill-switch warnings and boolean coercion.【F:src/governance/safety_manager.py†L1-L120】【F:tests/governance/test_safety_manager.py†L1-L76】【F:docs/reports/risk_governance_phase2_completion_audit.md†L1-L31】
  - Progress: The Phase II closeout audit now locks in telemetry guardrails for the trading risk interface, pairing a new regression suite with Markdown/event-bus checks so governance dashboards receive deterministic risk snapshots and alert payloads.【F:docs/reports/risk_governance_phase2_code_audit_closeout.md†L1-L32】【F:tests/trading/test_risk_interface_telemetry.py†L1-L124】
**Acceptance tests:** Using a controlled scenario, attempt to execute a trade that would take the account over a 5:1 leverage – verify it is forced to paper (if in dev mode) or not executed at all, with a log entry. Also confirm normal trades (well within limits) pass through. This ensures no silent breach of risk happens[4].

- [x] Governance Checkpoints in Loop – Integrate governance stage checks at every decision point. Currently, the ledger stage is consulted in the loop orchestrator to resolve thresholds[37]; expand this so that at the moment of decision routing, the system double-checks the strategy’s allowed actions. DoD: If a strategy is in “Shadow” (paper) stage, the orchestrator (or release router) ensures force_paper=True for that strategy regardless of drift (even if drift is normal). If a strategy is unapproved (not in ledger or in “Concept” stage), the system should not execute it live under any circumstance. Essentially, tie the strategy’s release stage directly to execution mode: Concept/Experiment -> can only simulate; Paper -> can execute on paper; Live -> allowed to send real orders (provided other checks pass)[32]. Tests: try to trick the system by manually calling an execution with a concept-stage strategy – it should refuse or convert to paper. Also verify that when a strategy is promoted to Live (in the ledger), the next run no longer forces paper (assuming drift is fine).
  - Completion: The AlphaTrade loop now rewrites every decision bundle with governance guardrails derived from the most conservative ledger stage, forcing paper execution for experiment/paper tactics, attaching the exact stage-gate reason, and persisting provenance into the compliance events under comprehensive regression coverage.【F:src/orchestration/alpha_trade_loop.py†L175-L287】【F:tests/orchestration/test_alpha_trade_loop.py†L167-L409】
  - Completion: The release-aware execution router continues to enforce ledger stages deterministically, forcing paper routes for experiment and paper tactics, merging drift and audit reasons, and surfacing overrides in metadata while orchestration and drift sentry tests assert that paper-stage policies stay in simulation even when drift is normal.【F:src/trading/execution/release_router.py†L80-L141】【F:tests/trading/test_release_execution_router.py†L27-L234】【F:tests/trading/test_drift_sentry_gate.py†L190-L212】
**Acceptance tests:** Simulate the promotion process: Start with a new strategy (stage=experiment) – it should only paper trade (check diary/log). Update ledger to stage=live for that strategy (simulate governance approval), run again – now the system allows it to go live (perhaps by printing “Live trade allowed” in logs). This demonstrates closed-loop governance enforcement.

- [x] Policy Promotion Workflow & Tests – Fully test and document the governance CLI tools and their effects. During this phase, as strategies evolve, use the promote_policy.py and alpha_trade_graduation.py tools in dry-runs to ensure they behave as expected. Possibly improve them: e.g. add a prompt or config for multi-approver requirement or automatic evidence attachment. DoD: The promotion CLI and graduation CLI have 100% scenario coverage – tests for promoting with valid evidence (should succeed) and without evidence (should fail)[38] are in place. If any gaps are found (e.g. no test for threshold override parsing), add them. Ensure the CLI outputs are clear (they produce a markdown or summary of what was changed). Definition of Done for governance could include “All promotions produce a log entry in a governance log file and update the ledger JSON.” Documentation is updated to instruct how a user promotes a strategy from paper to live.
  - Completion: `promote_policy.py` now enforces distinct-approver thresholds for limited live, streams JSONL logs, and writes Markdown summaries via shared helpers, with the runbook updated and CLI tests covering evidence validation, logging, and waiver flows end to end.【tools/governance/promote_policy.py:122】【tools/governance/_promotion_helpers.py:13】【docs/operations/runbooks/policy_promotion_governance.md:15】【tests/tools/test_promote_policy_cli.py:1】
**Acceptance tests:** Perform an end-to-end promotion in a staging environment: mark a strategy as ready (perhaps by having diaries showing X successful paper trades), run the promotion CLI with that strategy and dummy approval inputs – confirm the ledger file updates the stage, and the next loop run recognizes the new stage. This test should mimic a real review board decision to ensure the tools support the process.

- [x] Compliance & Observability Enhancements – Integrate risk and governance events into observability dashboards. Expand the existing observability dashboard (which currently has panels for drift, regime, etc.) to include compliance status – for example, a panel that shows any recent risk limit warnings or governance actions (like “Strategy Y demoted to paper due to breach”). DoD: The observability_dashboard module now consumes events from risk enforcement and governance: each loop iteration’s result (AlphaTradeLoopResult) includes a summary of any risk interventions and this is reflected on the dashboard[48]. Additionally, define Service Level Objectives (SLOs) for risk/compliance (e.g. “0 trades executed that violate policy” as a target) and surface these on the dashboard. Set up Prometheus gauges or log counters for: number of policy breaches caught, number of governance promotions done, etc.[49][50]. Write a small test to simulate a breach and ensure the dashboard data structure contains an entry for it. Completed in 2fc70b5 by introducing compliance/governance Prometheus gauges, wiring the loop’s compliance events into a dedicated dashboard panel, and adding regression coverage for risk breach scenarios.
**Acceptance tests:** Trigger a known compliance event (e.g. push a fake trade that violates a rule) and refresh the observability data – confirm that a WARN flag or specific entry is present in the output (could be as simple as checking a JSON from the dashboard component). Also ensure normal operation (no breaches) results in a green status for compliance. This keeps operators informed in real time.

- [x] Graph Diagnostics & Health Metrics – Augment the understanding loop graph diagnostics with “health” metrics. Since Phase I introduced sparse activations, now compute metrics like graph sparsity, degree distribution, and synapse utilization from the fast-weight adaptation graph. DoD: The graph_diagnostics CLI or a new graph_health.py tool calculates: (a) percentage of active fast-weight connections (non-zero weights) in the understanding graph, (b) distribution of strategy activation counts (how many times each strategy was chosen in a window), and (c) any nodes with disproportionate influence (e.g. a strategy that is always chosen or never chosen). These metrics should be output alongside the graph structure (e.g. as a section in the Markdown report)[51]. Tests: construct a scenario with a known activation pattern (maybe stub the router to activate the same strategy every time) and verify the metrics catch this (e.g. one node with 100% activation).
  - Completion: Diagnostics builder now synthesises decision windows, computes fast-weight utilisation, sparsity, and dominance summaries, and surfaces them through the CLI JSON/Markdown exports under regression coverage.【src/understanding/diagnostics.py†L621-L958】【tests/understanding/test_understanding_diagnostics.py†L15-L39】【tests/tools/test_understanding_graph_cli.py†L8-L19】
**Acceptance tests:** After running the system for some period, generate the diagnostics report. It should show, for example, “Graph sparsity: 85% of possible fast-weight connections are zero” and “No single strategy exceeds 30% usage” (or if it does, highlight it). Reviewers can use this to decide if the adaptation is balanced or needs tweaking. These graph health metrics ensure the adaptation mechanism stays interpretable and controlled, aligning with BDH’s emphasis on modularity and sparse activation[52][53].

- [x] Policy & Code Audit (Phase II Completion) – Conduct a mini-audit focusing on governance and risk code. Before moving to Phase III, spend ~2 days reviewing the code for any weaknesses in enforcement logic, race conditions in async tasks, or gaps in test coverage identified in prior phases. DoD: A short report (added to docs or as an issue ticket) listing any findings and quick fixes. Address critical fixes immediately (e.g. if a race condition is found in risk gating, fix it now). Ensure test coverage for governance/risk has improved (target, say, 85%+ for those modules).
  - Progress: The Risk & Governance Phase II mini-audit now captures the liquidity probe repair, associated regressions, and follow-up recommendations so the roadmap checkpoint has traceable evidence in docs.【F:docs/reports/risk_governance_phase2_mini_audit.md†L1-L34】
  - Progress: A Phase II completion audit now logs the run-mode normalisation fix, refreshed regression coverage, and residual recommendations so the checkpoint shows closed findings and ongoing governance follow-ups.【F:docs/reports/risk_governance_phase2_completion_audit.md†L1-L31】
  - Progress: A Phase II completion refresh audit now documents stop-loss pip conversion coverage, pairing new regression tests that assert the percentage conversion and floor clamp so the roadmap checkpoint tracks the closed coverage gap and follow-up telemetry recommendation.【F:docs/reports/governance_risk_phase2_code_audit_refresh.md†L1-L34】【F:tests/current/test_risk_gateway_validation.py†L210-L276】
  - Completion: Phase II completion evidence now ships as `docs/reports/policy_code_audit_phase2.md`, documenting corrupted-audit-line filtering, timestamp validation, and concurrent risk book safeguards with explicit remediation notes so reviewers inherit the full finding→fix trace.【F:docs/reports/policy_code_audit_phase2.md†L1-L19】
  - Completion: Audit logger defenses skip malformed JSON/timestamps and surface telemetry warnings, with the new guardrail suite replaying corrupt entries to prove dashboards retain valid history and statistics.【F:src/governance/audit_logger.py†L203-L338】【F:tests/governance/test_audit_logger.py†L15-L72】
  - Completion: Risk manager mutations now share a re-entrant lock across add/update/summary flows, preventing async workloads from racing the portfolio book; regression coverage exercises the locked helpers while calculating portfolio and per-position risk snapshots.【F:src/risk/risk_manager_impl.py†L738-L929】【F:tests/risk/test_risk_manager_impl_additional.py†L648-L694】
  - Progress: The Phase II fail-closed refresh now documents the hardened portfolio risk path, covering the new `evaluate_portfolio_risk` guards, regression evidence, and governance follow-ups so reviewers inherit a single remediation packet.【docs/reports/policy_code_audit_phase_ii_fail_closed_refresh.md:1】【src/risk/risk_manager_impl.py:845】【tests/risk/test_risk_manager_impl_additional.py:531】【tests/current/test_risk_manager_impl.py:95】
  - Progress: Market-regime detection now fails closed when the detector raises, logging the fault, zeroing multipliers, surfacing blocked telemetry, and preserving the error reason so governance reviews inherit deterministic posture, with the refreshed Phase II audit summarising the guardrail and regressions covering the recovery path.【docs/audits/policy_code_audit_phase2.md:1】【src/risk/risk_manager_impl.py:639】【tests/risk/test_risk_manager_impl_additional.py:449】
  - Progress: Risk manager limit updates now normalise heterogeneous boolean overrides through a dedicated coercion helper so remote configs can disable mandatory stop losses or enable research mode without being reinterpreted as truthy, with regression coverage and the Phase II follow-up audit documenting the fix for governance reviewers.【F:src/risk/risk_manager_impl.py†L50-L68】【F:src/risk/risk_manager_impl.py†L972-L980】【F:tests/current/test_risk_manager_impl.py†L141-L166】【F:docs/audits/policy_code_audit_phase2_followup.md†L10-L19】
  - Progress: Policy ledger record parsing continues to reject payloads missing policy or tactic identifiers and now de-duplicates persisted approval tuples and history entries, with the Phase II follow-up audit capturing the remediation so quorum counts stay truthful under regression coverage.【F:src/governance/policy_ledger.py†L123-L174】【F:src/governance/policy_ledger.py†L212-L229】【F:tests/governance/test_policy_ledger.py†L202-L233】【F:docs/audits/policy_code_audit_phase2_followup.md†L10-L18】
**Acceptance tests:** All Phase II features should have passing tests. CI pipeline shows improved coverage numbers for src/governance and src/trading modules. Any remaining TODO or FIXME notes in those modules are resolved or ticketed for later with a plan.

## Phase III (Days 61–90): Full Integration & Paper Trading Readiness

**Goal:** Tie everything together and demonstrate the AlphaTrade system operating as a cohesive whole. In Phase III, we focus on end-to-end runs, system optimization, and documentation/preparation for a “paper trading launch.” This includes finalizing any features needed for a controlled public demo or an internal pilot running on a paper trading account. We also incorporate the remaining BDH-inspired ideas: making sure activations remain interpretable and that the system’s behavior can be explained (key for writing a paper or report on AlphaTrade). By the end of this phase, the system should be ready for either a formal write-up (if academic bent) and/or a closed beta with paper trading, with confidence in its stability and clarity.

- [ ] End-to-End Paper Trade Simulation – Integrate with a paper trading API and simulate real trading. Connect the execution module to a broker’s paper trading endpoint (e.g. Interactive Brokers paper account or a sandbox exchange API). DoD: The system can place paper trades in real-time based on its decisions. This requires implementing a minimal BrokerAdapter class to translate AlphaTrade’s internal order (symbol, quantity, etc.) into API calls. All trades should go through this adapter when stage=Live but using paper credentials. Conduct a full-day simulation on a live market (in paper mode) with no manual intervention. The system should ingest live data, make decisions, and “execute” them via the paper API, while logging outcomes to the decision diary. Any error (API failure, etc.) should be caught by the supervisor with the system continuing (failover to no-trade rather than crash).
  - Progress: PaperBrokerExecutionAdapter now installs as the trading manager’s live engine, captures the last submitted order *and* the most recent broker error snapshot for diaries, and when paper-trading extras are supplied bootstrap attaches the REST-based `PaperTradingApiAdapter`. Integration tests cover adapter lifecycle, runtime cleanup callbacks, HTTP order placement, and API failure retries that persist the diagnostic payload into the decision diary while the loop keeps trading.【src/trading/execution/paper_broker_adapter.py†L1-L310】【src/orchestration/bootstrap_stack.py†L418-L462】【src/runtime/predator_app.py†L2087-L2149】【tests/integration/test_paper_trading_simulation.py†L1-L192】【tests/runtime/test_bootstrap_paper_broker.py†L1-L147】
  - Progress: Bootstrap diaries now capture `paper_metrics` snapshots from the live execution engine, and integration coverage asserts paper simulations persist order counts plus success/failure rollups for governance evidence packs.【F:src/orchestration/bootstrap_stack.py†L428-L441】【F:tests/integration/test_paper_trading_simulation.py†L99-L120】
  - Progress: A dedicated simulation helper and CLI now run the bootstrap runtime against a paper broker end-to-end, capturing orders, errors, and broker telemetry so roadmap evidence packs can execute reproducible paper drills under regression coverage.【F:src/runtime/paper_simulation.py†L1-L156】【F:tools/trading/run_paper_trading_simulation.py†L1-L198】【F:tests/runtime/test_paper_trading_simulation_runner.py†L39-L94】
  - Progress: Paper simulation CLI now exposes `--paper-retry-*` overrides and threads them into runtime extras so operators can tune adapter resilience without editing config files, with regression coverage over retry promotion and exhaustion paths.【F:tools/trading/run_paper_trading_simulation.py†L74-L206】【F:tests/trading/test_paper_trading_api_adapter.py†L93-L163】
  - Progress: Bootstrap trade intents now ship an `understanding_snapshot` alongside stop-loss defaults, dropping the legacy `intelligence_snapshot` alias and normalising timestamps so downstream governance, dashboards, and diaries all consume understanding-first metadata without stale keys.【F:src/orchestration/bootstrap_stack.py†L218-L240】【F:src/trading/trading_manager.py†L1223-L1250】【F:tests/current/test_bootstrap_stack.py†L159-L166】
  - Progress: Trading manager now releases reserved paper positions whenever execution fails, logging diagnostics if the release backstop misbehaves so simulated inventory cannot remain artificially locked after broker errors under regression coverage.【F:src/trading/trading_manager.py†L623-L689】
  - Progress: Paper simulation runs can now persist structured JSON reports, keep polling after hitting the order quota, and reuse the runtime’s serializer so operators capture diary counts, throttle evidence, and broker telemetry in a single artifact straight from the CLI, with regression coverage exercising the new flags and file outputs.【F:src/runtime/paper_simulation.py†L163-L333】【F:tools/trading/run_paper_trading_simulation.py†L117-L216】【F:tests/runtime/test_paper_trading_simulation_runner.py†L112-L191】
  - Progress: Paper simulation runner now carries regression coverage for broker 503 failures, asserting the CLI surfaces structured error telemetry and keeps the diary/tick loop alive when submissions are rejected so failover behaviour stays guarded.【F:tests/runtime/test_paper_trading_simulation_runner.py†L194-L234】
  - Progress: Simulation loop now honours an external `stop_event` and the CLI installs temporary SIGINT/SIGTERM handlers so long drills stop cleanly without orphaning runtime tasks; guardrail coverage asserts the stop event short-circuits the loop and prior signal handlers are restored.【src/runtime/paper_simulation.py:83】【tools/trading/run_paper_trading_simulation.py:25】【tests/runtime/test_paper_trading_simulation_runner.py:177】
  - Progress: Persistent broker-failure guardrail exercises a 500-response path end to end, proving the simulation records PaperTradingApiError telemetry, emits 0% success ratios, and journals the failure snapshot into the decision diary when retries exhaust.【tests/integration/test_paper_trading_simulation.py:337】【src/runtime/paper_simulation.py:193】【tests/integration/test_paper_trading_simulation.py:395】
  - Progress: Trading manager now tracks per-strategy execution stats, ROI/risk telemetry, and release routing summaries while the paper simulation report persists broker orders, error snapshots, strategy summaries, and derived throughput posture (`execution_stats` plus `performance_health`) to JSON so paper drills embed backlog/resource evidence without bespoke scripts.【src/trading/trading_manager.py:139】【src/trading/trading_manager.py:2681】【src/runtime/paper_simulation.py:49】【src/runtime/paper_simulation.py:179】【tests/runtime/test_paper_trading_simulation_runner.py:90】【tests/runtime/test_paper_trading_simulation_runner.py:167】【docs/performance/trading_throughput_monitoring.md:114】
  - Progress: Paper broker adapter now accrues latency/health metrics per submission, bootstrap status surfaces the live engine’s aggregated metrics, and the paper simulation report embeds the same telemetry so governance reviewers can audit fill rates and latency without scraping logs; the adapter also timestamps first/last orders, records the most recent broker error and risk context for bootstrap summaries, and the simulation falls back to `describe_last_error()` when history buffers are empty, with regression coverage asserting diaries capture the timestamps and ratios.【F:src/trading/execution/paper_broker_adapter.py†L242-L318】【F:src/runtime/bootstrap_runtime.py†L461-L476】【F:src/runtime/paper_simulation.py†L156-L219】【F:tests/integration/test_paper_trading_simulation.py†L94-L117】【F:tests/runtime/test_paper_trading_simulation_runner.py†L96-L140】 Latest snapshot refactor clones the metrics payload before returning and guarantees `first_order_at`/`last_order_at`/`last_error_at` ISO timestamps surface under guardrail tests so downstream telemetry cannot mutate adapter state while dashboards receive stable timing fields.【F:src/trading/execution/paper_broker_adapter.py†L291-L318】【F:tests/trading/test_paper_broker_adapter.py†L109-L150】
**Acceptance tests:** (Integration test) Hook up to a testnet or simulated exchange environment for a short run – e.g. let the system trade a single asset for 1 hour. Verify that at least one trade is attempted and logged in the diary. Check that all risk and governance checks still applied (e.g. if a trade was blocked by risk, it did not reach the API). This proves the system is “paper-ready” in that it can run continuously with real data and trading actions, but without risking real money.

- [ ] Performance Tuning & Throttle – Optimize system performance and add throttling to avoid overtrading. Analyze any bottlenecks revealed in the end-to-end run (Phase II/III tests). If the adaptation or ingest loop is slow, optimize critical sections (vectorize computations, use asyncio properly, etc.). Also, implement a configurable Trade Throttle – e.g. limit the system to at most N trades per minute or require a minimum time between trades, as a safety governance measure. DoD: No backlog buildup in event loop (system processes data in real-time without lag). CPU and memory usage are within acceptable bounds on a test machine (document baseline resource usage). The throttle mechanism is in place: if the system is in a rapid oscillation, it will log “Throttled: too many trades in short time” and skip or delay some decisions. Tests for throttle: simulate a scenario where the strategy would trade on every tick; with throttle set to, say, 1 trade/minute, ensure it only executes the first trade and defers others, and diary notes the throttle activation[54][48].
  - Progress: Execution pipeline now evaluates trades through the shared `TradeThrottle`, exposes throttle posture via the trading manager, and blocks bursts in both async manager regressions and dedicated unit tests so governance can cap order frequency without bespoke wiring. Snapshot metadata now carries `max_trades`, remaining trade credits, bounded utilisation, retry/reset timers, and the resolved scope/context dictionaries (plus `scope_key`) so diaries and dashboards spell out which tactic is paused and when capacity returns, with regression coverage and refreshed docs codifying the contract.【F:src/trading/execution/trade_throttle.py†L202-L344】【F:src/trading/trading_manager.py†L1991-L2031】【F:tests/trading/test_trade_throttle.py†L25-L160】【F:tests/trading/test_trading_manager_execution.py†L524-L550】【F:docs/performance/trading_throughput_monitoring.md†L60-L74】
  - Progress: Trade throttle reason parsing now preserves fractional window sizes, formats human-readable countdowns (e.g. `2.5 seconds`), and regression coverage exercises sub-second windows so diaries and dashboards keep precise retry timers for tight governance controls.【F:src/trading/execution/trade_throttle.py†L123-L195】【F:src/trading/execution/trade_throttle.py†L680-L712】【F:tests/trading/test_trade_throttle.py†L113-L148】
  - Progress: Per-scope throttle snapshots now flow directly into execution stats and performance baselines via the new `trade_throttle_scopes` list and `get_trade_throttle_scope_snapshots()` helper, so evidence packs and dashboards inherit per-strategy guardrail posture without custom plumbing; guardrail suites assert the manager, baseline helper, and CLI evidence include the new snapshots and clear them when throttling is disabled.【F:src/trading/trading_manager.py†L621-L725】【F:src/trading/execution/performance_baseline.py†L52-L74】【F:tests/trading/test_trade_throttle.py†L171-L210】【F:tests/trading/test_trading_manager_execution.py†L2092-L2138】【F:tests/trading/test_trading_manager_execution.py†L2648-L2666】
  - Progress: Scoped throttle states now purge automatically once cooldowns expire, dropping idle strategy scopes and recalculating window state on demand so long-running sessions avoid unbounded `_states` growth; new regression coverage asserts stale strategy namespaces collapse after the purge.【F:src/trading/execution/trade_throttle.py†L418-L478】【F:tests/trading/test_trade_throttle.py†L214-L242】
  - Progress: Rolled-back throttles now restore capacity when execution fails by removing the recorded timestamp, clearing retry timers, and pruning empty scopes so failed orders no longer consume the rate window; trading manager hooks call the rollback on broker errors and guardrail suites assert throttle stats reopen immediately after failure.【src/trading/execution/trade_throttle.py:289】【src/trading/trading_manager.py:1240】【tests/trading/test_trade_throttle.py:313】【tests/trading/test_trading_manager_execution.py:2149】
  - Progress: Rate-limit snapshots now bound `window_utilisation`, surface the UTC `window_reset_at`, and publish a clamped `window_reset_in_seconds` countdown so governance dashboards know exactly when each scope regains a trade credit; regression coverage exercises rate, cooldown, and spacing scenarios while docs spell out the telemetry contract.【F:src/trading/execution/trade_throttle.py†L204-L312】【F:tests/trading/test_trade_throttle.py†L15-L205】【F:docs/performance/trading_throughput_monitoring.md†L165-L173】
  - Progress: Scoped throttles now maintain independent rolling windows per strategy (or other metadata scopes), emit the resolved `scope_key`/`scope` in snapshots, and document operator guidance for configuring scope fields so bursts from a single tactic can’t exhaust the global quota. Regression coverage exercises scope parsing, cooldown handling, and missing metadata fallbacks.【F:src/trading/execution/trade_throttle.py†L41-L260】【F:tests/trading/test_trade_throttle.py†L1-L232】【F:docs/performance/trading_throughput_monitoring.md†L87-L115】
  - Progress: Minimum-spacing enforcement now blocks rapid-fire orders even when the rate window has headroom, logging `min_interval` states with retry timestamps, Markdown guidance, and regression coverage across throttle helpers, docs, and trading-manager snapshots so oscillating tactics cool down predictably.【F:src/trading/execution/trade_throttle.py†L18-L344】【F:tests/trading/test_trade_throttle.py†L1-L147】【F:docs/performance/trading_throughput_monitoring.md†L102-L116】【F:tests/trading/test_trading_manager_execution.py†L1730-L1899】
  - Progress: Bootstrap runtime now resolves trade throttle config from `SystemConfig.extras`, normalises JSON blobs, file hints, and individual knobs (name, rate, cooldown, scope), and threads the result into `TradingManager` so deployments can enforce governance limits directly from environment configuration, with regression coverage confirming scope-aware snapshots flow through the professional predator app.【F:src/runtime/predator_app.py†L1723-L2470】【F:src/runtime/bootstrap_runtime.py†L123-L201】【F:tests/runtime/test_trade_throttle_configuration.py†L1-L55】
  - Progress: Throughput instrumentation now returns structured backlog observations, increments global breach counters, emits `backlog_breach` experiment events, and documents the new metadata so ops can trace lag incidents end-to-end, with regression coverage locking the observation semantics and event telemetry.【F:src/trading/execution/performance_monitor.py†L1-L143】【F:src/trading/execution/backlog_tracker.py†L20-L123】【F:src/trading/trading_manager.py†L982-L1020】【F:docs/performance/trading_throughput_monitoring.md†L28-L72】【F:tests/trading/execution/test_backlog_tracker.py†L8-L30】【F:tests/trading/test_trading_manager_execution.py†L2003-L2062】
  - Progress: Trading manager now offers an `assess_throughput_health()` helper that evaluates recent latency and ingest lag against configurable budgets, enabling operational runbooks to assert health from a single snapshot during high-frequency replays under regression coverage.【F:src/trading/trading_manager.py†L1296-L1335】【F:tests/trading/test_trading_manager_execution.py†L1831-L1887】
  - Progress: Execution manager now exposes `generate_execution_report()` backed by a Markdown renderer that summarises throttle posture and throughput metrics so operations can export evidence snapshots, with regression coverage over the renderer and manager surfaces guarding the contract.【F:src/trading/execution/performance_report.py†L1-L78】【F:src/trading/trading_manager.py†L1248-L1294】【F:tests/trading/execution/test_performance_report.py†L10-L53】【F:tests/trading/test_trading_manager_execution.py†L1898-L1983】
  - Progress: Event backlog tracker now maintains sliding-window statistics via bounded deques, preserving rolling sums and worst-breach telemetry without list copies, and its snapshot payload adds `latest_lag_ms`, `p95_lag_ms`, `breach_rate`, and `max_breach_streak` so operators can spot creeping lag and breach streaks from a single payload. Performance reports and baseline docs render the new metrics, while regression coverage spans backlog rollover, Markdown evidence, and execution stats so dashboards inherit authoritative retry timers and lag analytics.【F:src/trading/execution/backlog_tracker.py†L31-L170】【F:src/trading/execution/performance_report.py†L104-L193】【F:docs/performance/trading_throughput_monitoring.md†L86-L133】【F:docs/performance/performance_baseline.md†L20-L83】【F:tests/trading/execution/test_backlog_tracker.py†L26-L103】【F:tests/trading/execution/test_performance_report.py†L39-L131】
  - Progress: Performance health reporting now renders Markdown assessments via `build_performance_health_report`, while `TradingManager.generate_performance_health_report()` wraps the helper for operators and runbooks; docs capture the workflow and regressions cover end-to-end rendering for healthy and degraded cases.【F:src/trading/execution/performance_report.py†L129-L225】【F:src/trading/trading_manager.py†L1696-L1757】【F:docs/performance/trading_throughput_monitoring.md†L69-L73】【F:tests/trading/execution/test_performance_report.py†L78-L128】【F:tests/trading/test_trading_manager_execution.py†L359-L458】
  - Progress: Trade throttle decisions now surface a `multiplier` so the trading manager can scale validated quantities, emit `throttle_scaled` experiment events, and track multiplier usage in execution stats while runtime extras, docs, and guardrail suites prove the path end to end.【F:src/trading/execution/trade_throttle.py†L236-L250】【F:src/trading/trading_manager.py†L538-L999】【F:docs/performance/trading_throughput_monitoring.md†L165-L175】【F:tests/trading/test_trade_throttle.py†L245-L263】【F:tests/runtime/test_trade_throttle_configuration.py†L28-L41】【F:tests/trading/test_trading_manager_execution.py†L2127-L2549】
  - Progress: A dedicated `collect_performance_baseline()` helper and CLI now capture execution stats, throughput verdicts, backlog/resource posture, and Markdown evidence packs so Phase III drills can archive reproducible performance baselines, with docs and regression suites anchoring the workflow.【F:src/trading/execution/performance_baseline.py†L21-L72】【F:tools/performance_baseline.py†L1-L108】【F:docs/performance/performance_baseline.md†L1-L76】【F:tests/trading/test_trading_manager_execution.py†L2504-L2549】
  - Progress: AlphaTrade loop now captures structured `TradeIntentOutcome` payloads, merges the execution/throttle snapshot into decision diary metadata, and threads the same outcome back through run results so dashboards and audits inherit a single source of truth for why trades executed, failed, or were throttled under regression coverage.【F:src/orchestration/alpha_trade_loop.py†L124-L129】【F:src/orchestration/alpha_trade_runner.py†L162-L193】【F:src/trading/trading_manager.py†L640-L922】【F:tests/orchestration/test_alpha_trade_runner.py†L118-L197】【F:tests/understanding/test_decision_diary.py†L103-L143】
**Acceptance tests:** Run a high-frequency data replay (e.g. tick data) faster than real-time – confirm that the system doesn’t fall behind processing (throughput is sufficient). Also, check logs for any “throttled” messages when appropriate. This will give confidence that when connected to live markets, AlphaTrade won’t overload itself or the broker with excessive orders.

- [x] Evaluation and KPIs Collection – Define and collect Key Performance Indicators for the understanding loop and trading performance. Even though still in paper mode, decide on metrics such as: win rate of strategy decisions, average trade return, maximum drawdown in the paper account, accuracy of regime detection, and false positive/negative rate of drift alerts. DoD: Implement a StrategyPerformanceTracker (if not already) that computes ROI, P&L, and other stats per strategy[22]. At end of each day (or test run), output a summary (could integrate with the observability dashboard or as a separate report). Metrics on the cognitive loop itself (e.g. “Did the fast-weight adaptation improve decision quality?”) are gathered by comparing periods with it on vs off. Possibly use the existing benchmark harness to log latency and variance impact of fast-weights[55]. Tests: artificially manipulate outcomes to see if tracker correctly computes metrics (e.g. feed 10 trades with known P&L and verify ROI calc).
  - Completion: `StrategyPerformanceTracker` now calculates per-strategy KPIs, ROI snapshots, loop metrics, and Markdown summaries with coverage validating trades, regime/ drift stats, and report surfaces so operational dashboards can rely on a single aggregation surface.【src/operations/strategy_performance_tracker.py:1】【tests/operations/test_strategy_performance_tracker.py:1】
  - Progress: The backtest evidence script now auto-shims optional replay, microstructure, and parquet dependencies so KPI exports stay reproducible even when FAISS-era modules are unavailable, defaulting to JSONL replays and logging the degraded mode rather than aborting operator drills.【F:scripts/backtest_report.py†L1-L200】
**Acceptance tests:** After a multi-day paper trading simulation, produce the performance report. It should list, for example: “Strategy A: 30 trades, 60% win rate, +2.3% ROI; Strategy B: 15 trades, 40% win rate, -1.0% ROI; Overall P&L: +1.5%”. It should also include system metrics like uptime, number of drift alerts, etc. These KPIs will inform whether the system is ready to consider live trading or needs adjustments (and also provide material for a potential paper).

- [x] Documentation: AlphaTrade Whitepaper Draft – Compile the architecture, methods, and results into a draft paper or detailed technical document. By now, we have a wealth of information (design, features, metrics, and possibly unique findings like the effect of fast-weights). DoD: A comprehensive document (could be docs/AlphaTrade_Whitepaper.md) is written, including: the high-level architecture diagram of Perception→Adaptation→Reflection→Execution→Governance loop, an explanation of the BDH-inspired features (fast weights, positive sparse activations, graph metrics) and why they matter, and preliminary results from the paper-trading simulation (e.g. charts of performance, illustrations of decision graphs). This serves as the “Paper readiness” proof – demonstrating we can communicate the system’s value to stakeholders or as an academic/industry case study.
  - Completion: The refreshed draft now opens with a stage inventory that maps each roadmap loop to concrete modules, configuration entry points, and guardrail suites, explicitly restating that the milestone criteria are satisfied for reviewers.【docs/AlphaTrade_Whitepaper.md:6】【docs/AlphaTrade_Whitepaper.md:27】【docs/AlphaTrade_Whitepaper.md:30】【docs/AlphaTrade_Whitepaper.md:32】
  - Completion: BDH capability summaries and the accompanying evidence table tie fast-weight adaptation, sparse activations, and governance telemetry to reproducible commands, fulfilling the requirement to document methods and preliminary results for the paper-trading harness.【docs/AlphaTrade_Whitepaper.md:62】【docs/AlphaTrade_Whitepaper.md:65】【docs/AlphaTrade_Whitepaper.md:82】【docs/AlphaTrade_Whitepaper.md:84】【docs/AlphaTrade_Whitepaper.md:88】
  - Completion: Appendices list the relevant context packs and suggested review commands so documentation stays aligned with the encyclopedia briefs and operational workflows when stakeholders regenerate evidence.【docs/AlphaTrade_Whitepaper.md:97】【docs/AlphaTrade_Whitepaper.md:100】【docs/AlphaTrade_Whitepaper.md:104】【docs/AlphaTrade_Whitepaper.md:106】
**Acceptance tests:** Editorial sign-off can now rely on Appendix B’s regenerate commands to validate the packet; treat completion of that review as the publication gate for this milestone.

- [ ] Final Dry Run & Sign-off – Run a final dry-run of the entire system under realistic conditions for several days and fix any last bugs. This is effectively a user acceptance test (UAT) for the platform. DoD: The system runs continuously (in paper mode) for, say, 3 days straight with no crashes or major errors. All logs remain at info/debug (no uncaught exceptions). Any minor issues found are documented and either fixed or added to a backlog for post-90-day work. At the end of this run, gather all evidence (logs, performance metrics, diary samples) and have a wrap-up review meeting.
  **Acceptance criteria:** The review board (or project owner) signs off that the 90-day objectives have been met: data backbone is live, understanding loop is autonomous and interpretable, governance and risk are enforced, and the system is ready for either live pilot or publication. If any objective is not met, we clearly itemize why and possibly schedule that as priority in the next roadmap.
  - Progress: Final dry-run audit tooling now aggregates logs, diary highlights, readiness posture, and KPI telemetry into a Markdown report with severity roll-ups, while flagging log continuity gaps with configurable warn/fail thresholds, uptime ratios, and CLI controls so reviewers can detect evidence drop-outs during sign-off. Command-line helpers and pytest fixtures validate the evidence synthesis contract for UAT rehearsals.【F:src/operations/dry_run_audit.py†L18-L789】【F:tests/operations/test_dry_run_audit.py†L1-L220】【F:tools/operations/final_dry_run_audit.py†L1-L118】
  - Progress: Dry-run evaluation now enforces minimum run durations and uptime ratios, emits structured incidents when evidence windows are too short, and ships a sign-off assessment helper plus CLI toggles so review boards can demand diary/performance evidence before approving a run, with regression covering pass, fail, and warn pathways.【F:src/operations/dry_run_audit.py†L760-L835】【F:tools/operations/final_dry_run_audit.py†L21-L150】【F:tests/operations/test_dry_run_audit.py†L189-L259】
  - Progress: Structured log analysis now classifies traceback tokens and stack payloads as content incidents, ensuring the audit fails even info-level exception traces and documents anomalies in the Markdown rollup under guardrail coverage.【F:src/operations/dry_run_audit.py†L700-L755】【F:src/operations/dry_run_audit.py†L1037-L1060】【F:tests/operations/test_dry_run_audit.py†L81-L100】
  - Progress: Structured log ingestion now streams gz/bz2/xz archives and normalises ISO timestamps with trailing `Z`, so compressed evidence bundles and legacy diary exports feed the audit without manual unpacking, under regression coverage for gzip/bz2 fixtures and the diary parser.【F:src/operations/dry_run_audit.py†L31-L671】【F:src/understanding/decision_diary.py†L41-L54】【F:tests/operations/test_dry_run_audit.py†L1-L47】
  - Progress: Evidence packet tooling now composes the dry-run summary, optional sign-off verdicts, raw logs, diary exports, and KPI snapshots into a manifest plus tarball so governance reviewers can archive or distribute the complete sign-off bundle from a single command, with regression exercising raw-artifact toggles and archive emission.【F:src/operations/dry_run_packet.py†L1-L158】【F:tests/operations/test_dry_run_packet.py†L54-L143】
  - Progress: A dedicated workflow now runs the dry run, assembles evidence packets, and emits review briefs; CLI flags surface packet archives, review notes, and formatting controls while the runbook captures the new options and tests assert packets and reviews materialise for governance boards. The wrapper now ships as an EMP CLI subcommand so operators can invoke `emp final-dry-run` with the same switches, with integration tests covering pass/fail paths.【F:src/operations/final_dry_run_workflow.py†L25-L310】【F:tools/operations/final_dry_run.py†L236-L412】【F:emp/cli/final_dry_run.py†L1-L450】【F:emp/__main__.py†L1-L33】【F:docs/runbooks/final_dry_run.md†L10-L57】【F:tests/operations/test_final_dry_run_cli.py†L49-L134】【F:tests/operations/test_final_dry_run.py†L92-L201】
  - Progress: Final dry-run workflow and CLIs now accept `--objective` flags, normalise the inputs into review metadata, and surface readiness outcomes in Markdown/JSON briefs so governance reviews capture pass/warn/fail checkpoints alongside incidents under regression coverage.【F:emp/cli/final_dry_run.py†L246-L360】【F:tools/operations/final_dry_run.py†L290-L392】【F:src/operations/final_dry_run_review.py†L220-L307】【F:tests/operations/test_final_dry_run_review.py†L206-L217】【F:docs/runbooks/final_dry_run.md†L80-L90】
  - Progress: Smoke harness now bundles a simulated runtime and CLI wrapper so ops can rehearse end-to-end dry-run workflows in minutes; the runbook documents the shortcut and regression coverage asserts the smoke pipeline produces logs, diary entries, reviews, and packet scaffolding.【F:src/operations/final_dry_run_simulated_runtime.py†L1-L214】【F:src/operations/final_dry_run_smoke.py†L1-L298】【F:tools/operations/final_dry_run_smoke.py†L1-L361】【F:tests/operations/test_final_dry_run_smoke.py†L1-L43】【F:docs/runbooks/final_dry_run.md†L54-L73】
  - Progress: Dry-run entrypoints add `--compress-logs` across EMP and tooling CLIs, writing `.jsonl.gz`/`.log.gz` artefacts and guarding gzip decoding so multi-day rehearsals shrink evidence footprints without breaking audit ingestion.【F:emp/cli/final_dry_run.py†L268-L424】【F:src/operations/final_dry_run.py†L67-L724】【F:tools/operations/final_dry_run.py†L257-L454】【F:tests/operations/test_final_dry_run.py†L339-L411】【F:docs/runbooks/final_dry_run.md†L19-L77】
  - Progress: Dry-run progress telemetry now tracks the highest-severity incident seen so far, flushes snapshots as soon as log monitors emit WARN/FAIL payloads, and guardrail coverage asserts the JSON progress file records incident metadata before the harness completes so governance can tail real-time failures instead of waiting for the final report.【src/operations/final_dry_run.py:209】【src/operations/final_dry_run.py:246】【tests/operations/test_final_dry_run.py:277】
  - Progress: Harness log monitors now classify stdout/stderr stack traces as fail-severity incidents, emit human-friendly summaries, and persist them in progress snapshots so latent exceptions surface even when logs stay at INFO level, with guardrail coverage over detection and reporting.【F:src/operations/final_dry_run.py†L1206-L1321】【F:tests/operations/test_final_dry_run.py†L383-L401】
  - Progress: Diary summarisation enforces coverage windows tied to the log timeline, producing fail or warn issues when entries are missing at the start or end of the run so reviewers can certify diary completeness during sign-off rehearsals.【F:src/operations/dry_run_audit.py†L712-L796】【F:tests/operations/test_dry_run_audit.py†L174-L206】
  - Progress: Daily diary coverage checks now demand configurable minimum entries per calendar day and wire CLI toggles for severity, ensuring dry-run packets surface gaps in operator journaling with regression coverage across audit helpers and the final sign-off script.【F:src/operations/dry_run_audit.py†L738-L947】【F:tests/operations/test_dry_run_audit.py†L188-L304】【F:tools/operations/final_dry_run_audit.py†L21-L206】
  - Progress: Sign-off checks now require Sharpe ratios whenever a threshold is configured, fail runs that fall short, and expose a `--sign-off-min-sharpe` CLI option so review boards can demand risk-adjusted performance evidence with regression coverage across pass, fail, and missing-metric paths.【F:src/operations/dry_run_audit.py†L323-L1072】【F:tools/operations/final_dry_run_audit.py†L100-L178】【F:tests/operations/test_dry_run_audit.py†L226-L360】
  - Progress: Final dry-run harness now launches pumps, timeouts, and waiters under a shared `TaskSupervisor`, cancelling owned workloads on exit, with regression coverage asserting supervised task registration and cleanup so sign-off runs inherit the runtime guardrails.【F:src/operations/final_dry_run.py†L203-L268】【F:tests/operations/test_final_dry_run.py†L138-L171】
  - Progress: Evidence freshness monitors now watch decision diaries and performance telemetry in-flight, emitting WARN/FAIL incidents when artefacts stall; CLI switches (`--diary-stale-*`, `--performance-stale-*`, `--evidence-*`) span the EMP entrypoint and operations helper, the runbook documents the cadence knobs, and regression coverage asserts stalled diaries surface incidents and land in progress snapshots.【F:emp/cli/final_dry_run.py†L250-L420】【F:tools/operations/final_dry_run.py†L194-L452】【F:docs/runbooks/final_dry_run.md†L36-L42】【F:src/operations/final_dry_run.py†L776-L823】【F:tests/operations/test_final_dry_run.py†L382-L411】
  - Progress: Harness progress telemetry now snapshots status, phase, log stream and level counts, exit codes, incidents, and final sign-off verdicts on a configurable interval, then seals the JSON timeline at shutdown so governance can monitor multi-day rehearsals without tailing raw logs.【F:src/operations/final_dry_run.py†L167-L575】

## Directory Layout & Code Practices

Throughout the roadmap execution, maintain a clean project structure and code quality:
Organize new code by domain layer for clarity. For example, perception-related additions go under src/sensory or src/understanding (ensuring sensory organs and belief logic stay together), adaptation and learning components under src/thinking (e.g. adaptation/fast_weights.py, evolution/ subpackage), execution and risk under src/trading or src/risk, and governance tools under src/governance. This preserves the core → sensory → thinking → trading → orchestration layering[4] and helps new contributors map features to architecture. If certain cross-cutting concerns (like operations/incident_response.py or operations/event_bus_failover.py) exist, document their role clearly rather than moving them, to avoid confusion.
Continue using dataclasses and Pydantic models for config and state where appropriate to enforce schema (e.g. a Pydantic model for a strategy config ensures required fields are present). All public interfaces of modules should be well-defined – consider adding .pyi stub files for any dynamically generated interfaces, following the project’s stub guidelines (model minimal API surface, avoid Any types)[40][56]. This acts as an explicit contract for each component.
Each new major feature should come with corresponding tests (unit and integration) and documentation. Adhere to the existing guardrail testing concept: critical paths (like ingest, risk enforcement, fast-weight toggling) should have tests marked with guardrail so they run in CI gatekeeping[57]. Likewise, update the pytest markers/manifest to include new domains if needed so nothing slips through CI.
Maintain and improve the CI coverage: aim to raise coverage from ~76% into the 80s or 90s by adding tests as you implement features[58]. Particularly, ensure new code in previously untested areas (sensory organs, evolution, broker adapter) has thorough test cases. Leverage the existing style of golden files and replay tests for deterministic behavior (e.g. record a golden belief snapshot JSON from Phase I and use it to test backward compatibility after later changes).
Keep documentation up-to-date with code. The roadmap’s context briefs and sprint briefs should be revised when assumptions change. For example, once real data ingest is done, the alignment brief for the data backbone can be marked as achieved or updated with remaining gaps. This ensures the “concept promises” in the docs match reality, maintaining alignment between narrative and implementation[59].
By following this roadmap, in 90 days we will have transformed the AlphaTrade prototype from a scaffold into a paper-trading-ready, well-governed AI trading loop. We will have demonstrated the system’s ability to perceive real market data, adapt using fast weights and evolutionary tweaks, reflect on its decisions with interpretable metrics, execute trades under strict risk controls, and govern itself through policy stages. Quick wins are captured early to build momentum, and risky, research-heavy tasks are deferred or made optional behind feature flags. Each phase concludes with concrete acceptance tests and definitions of done, giving a clear target for completion. With this structured approach, the team and stakeholders can track progress and gain confidence as AlphaTrade moves toward both practical deployment and the potential to contribute novel insights (via the BDH-inspired design) to the algorithmic trading community.

## Automation updates — 2025-10-13T00:28:22Z

### Last 4 commits
- 39faa668 feat(intelligence): add 2 files (2025-10-13)
- 7a2807a7 refactor(docs): tune 5 files (2025-10-13)
- e9e770b2 feat(docs): add 5 files (2025-10-13)
- e2ff4011 docs(docs): tune 4 files (2025-10-13)
