# Observability and alerting plan

With the Kilocode bridge retired, the modernization roadmap called for a
lightweight alternative that still surfaces failures quickly. The guardrails
below are now in place, along with the remaining follow-up work needed to keep
telemetry actionable.

## Current capabilities

* **CI failure alerts** – `.github/workflows/ci-failure-alerts.yml` opens (or
  updates) a `CI failure alerts` issue whenever the main pipeline finishes in a
  failing, cancelled, or timed-out state. The workflow also auto-closes the
  issue after the next successful run so the backlog reflects only outstanding
  problems. A mirror notification posts to the `#ci-alerts` Slack channel so the
  on-call engineer receives prompt visibility.
* **CI step summaries** – The `tests` job in `.github/workflows/ci.yml` tees
  pytest output into `pytest.log`, appends the final chunk to the GitHub Step
  Summary, and uploads the full log as an artifact even on failure. This
  preserves visibility without the fragile Kilocode relay.
* **Pytest flake metadata** – `conftest.py` captures failure metadata in
  `build/pytest_failures.json`. CI appends a Markdown leaderboard generated by
  `scripts/flake_metadata_report.py` to the run summary and publishes the JSON
  as an artifact for longitudinal analysis.
* **Health snapshot** – [`docs/status/ci_health.md`](../status/ci_health.md)
  tracks the latest pipeline status, coverage baseline, formatter rollout
  progress, and where to look first when jobs fail.
* **Baseline hygiene reports** –
  [`docs/ci_baseline_report.md`](../ci_baseline_report.md) and
  [`docs/reports/dead_code_audit.md`](../reports/dead_code_audit.md) capture
  periodic snapshots of pipeline health so regressions are easy to spot.
* **Runtime logging** – The platform leans on Python's structured logging
  (module loggers + UTC timestamps). Configuration and policy failures abort
  startup with actionable errors.

## Alerting channel decision

*Primary*: GitHub issue automation backed by the `#ci-alerts` Slack mirror.

* Why: keeps all actionable history in GitHub while still notifying engineers in
  real time via Slack.
* Expectations: acknowledge the alert issue within one business hour and note
  whether the fix is in progress, blocked, or requires pairing.

## On-call expectations

* **Primary rotation** – The trading-platform team maintains a weekly rotation.
  The current order lives in the shared on-call calendar; each handoff occurs
  during Monday stand-up.
* **Alert intake** – When the CI failure issue opens (and the Slack mirror
  pings), acknowledge it within one business hour and either drive the fix or
  pair the change with the triggering contributor.
* **Resolution** – Once the blocking run passes, close the alert issue. Add a
  comment summarizing the root cause and any follow-up tickets that were filed.
* **Escalation path** – If the failure blocks production hotfixes or persists
  past the business day, page the engineering manager and broadcast in the
  `#ci-alerts` channel.

## Immediate next steps

1. **Runtime healthchecks** – For long-running deployments, expose a `/health`
   endpoint that checks FIX connectivity, market-data freshness, and telemetry
   exporter status.
2. **Optional webhook** – If stakeholders need out-of-band notifications, wire
   GitHub's native workflow notification integration to email or PagerDuty. This
   avoids custom code while restoring proactive alerts during off-hours.

## Long-term instrumentation ideas

* Promote Prometheus metrics beyond the existing counters so we can alert on
  order execution latency, event bus backlog, and ingestion freshness.
* Introduce OpenTelemetry tracing when strategic refactors land; the event bus
  provides a natural place to propagate trace context.
* Adopt GitHub's dependency review and code scanning alerts once the formatting
  backlog is addressed to avoid noisy signal during active cleanup.

Owners should revisit this plan quarterly and adjust the roadmap as new
observability gaps surface.
